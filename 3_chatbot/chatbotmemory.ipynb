{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "os.environ['GROQ_API_KEY']=os.getenv('GROQ_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=ChatGroq(model=\"gemma2-9b-it\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x0000022EBD87BFB0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x0000022EBD8AD0D0>, model_name='gemma2-9b-it', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! üëã  How can I help you today?\\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 10, 'total_tokens': 24, 'completion_time': 0.025454545, 'prompt_time': 4.3e-07, 'queue_time': 0.020612576, 'total_time': 0.025454975}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-a2179645-94f9-4c4a-96e1-176b65157b2f-0', usage_metadata={'input_tokens': 10, 'output_tokens': 14, 'total_tokens': 24})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\"hello\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### exploring message history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"You are HV, a data scientist who specializes in generative AI, NLP (Natural Language Processing), and forecasting!  \\n\\nThat's a really exciting field to be in.  Do you have a favorite project you've worked on that combines any of those areas?  \\n\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 59, 'prompt_tokens': 101, 'total_tokens': 160, 'completion_time': 0.107272727, 'prompt_time': 0.003323459, 'queue_time': 0.025445916, 'total_time': 0.110596186}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-455e5931-f8d1-4cc5-8801-963cd5c53e3f-0', usage_metadata={'input_tokens': 101, 'output_tokens': 59, 'total_tokens': 160})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\n",
    "    [\n",
    "        HumanMessage(\"my name is HV and i am data scientist\"),\n",
    "        AIMessage(content=\"Nice to meet you, HV!  \\n\\nIt's great to know you're a data scientist. What kind of work do you do? \\n\\nI'm always interested in learning more about how people use data to solve problems and make discoveries.\\n\"),\n",
    "        HumanMessage(\"i work on generative ai, nlp and forecasting. Whats my name and what do i do?\"),\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### implementing message history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableWithMessageHistory(bound=RunnableBinding(bound=RunnableBinding(bound=RunnableLambda(_enter_history), kwargs={}, config={'run_name': 'load_history'}, config_factories=[])\n",
       "| RunnableBinding(bound=RunnableLambda(_call_runnable_sync), kwargs={}, config={'run_name': 'check_sync_or_async'}, config_factories=[]), kwargs={}, config={'run_name': 'RunnableWithMessageHistory'}, config_factories=[]), kwargs={}, config={}, config_factories=[], get_session_history=<function get_session_history at 0x0000022ECDC5ED40>, history_factory_config=[ConfigurableFieldSpec(id='session_id', annotation=<class 'str'>, name='Session ID', description='Unique identifier for a session.', default='', is_shared=True, dependencies=None)])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cache \n",
    "store={}\n",
    "\n",
    "def get_session_history(session_id:str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id]=ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "with_message_history=RunnableWithMessageHistory(model, get_session_history)\n",
    "with_message_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"As an AI, I have no memory of past conversations and do not know your name. If you'd like to tell me, I'm happy to learn it! üòä\\n\" additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 39, 'prompt_tokens': 14, 'total_tokens': 53, 'completion_time': 0.070909091, 'prompt_time': 7.498e-05, 'queue_time': 0.022772846000000003, 'total_time': 0.070984071}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None} id='run-72fb7208-8495-4207-bd8c-d6127741c7f2-0' usage_metadata={'input_tokens': 14, 'output_tokens': 39, 'total_tokens': 53}\n"
     ]
    }
   ],
   "source": [
    "config={\"configurable\":{\"session_id\":'HV'}}\n",
    "\n",
    "response=with_message_history.invoke(\n",
    "    [HumanMessage(content=\"what is my name?\")],\n",
    "    config=config\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InMemoryChatMessageHistory(messages=[HumanMessage(content='what is my name?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"As an AI, I have no memory of past conversations and do not know your name. If you'd like to tell me, I'm happy to learn it! üòä\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 39, 'prompt_tokens': 14, 'total_tokens': 53, 'completion_time': 0.070909091, 'prompt_time': 7.498e-05, 'queue_time': 0.022772846000000003, 'total_time': 0.070984071}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-72fb7208-8495-4207-bd8c-d6127741c7f2-0', usage_metadata={'input_tokens': 14, 'output_tokens': 39, 'total_tokens': 53})])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_session_history(\"HV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### retrieve your history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Well, hello there, John Cena! It's an honor to meet you.  \\n\\n üòâ  \\n\\nYou and me both on liking wrestling! It's an awesome sport full of action, drama, and larger-than-life characters.  Who are some of your favorite wrestlers?  \\n\\nMaybe we can even talk about some epic matches you've seen!\\n\" additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 78, 'prompt_tokens': 417, 'total_tokens': 495, 'completion_time': 0.141818182, 'prompt_time': 0.022047216, 'queue_time': 0.02321231, 'total_time': 0.163865398}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None} id='run-9e6405dd-f7cd-4815-ba4e-6109af4b3652-0' usage_metadata={'input_tokens': 417, 'output_tokens': 78, 'total_tokens': 495}\n"
     ]
    }
   ],
   "source": [
    "config2={\"configurable\":{\"session_id\":'JC'}}\n",
    "\n",
    "response=with_message_history.invoke(\n",
    "    [HumanMessage(content=\"my name is John Cena and i like wrestling\")],\n",
    "    config=config2\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"You told me your name is John Cena!  \\n\\n Are you ready to discuss some wrestling?  I'm excited to hear your thoughts on the world of sports entertainment!  \\n\\n\\n\\n\" additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 39, 'prompt_tokens': 507, 'total_tokens': 546, 'completion_time': 0.070909091, 'prompt_time': 0.016177003, 'queue_time': 0.023612479000000002, 'total_time': 0.087086094}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None} id='run-b6e95157-5939-4ad6-b701-40a9b4627278-0' usage_metadata={'input_tokens': 507, 'output_tokens': 39, 'total_tokens': 546}\n"
     ]
    }
   ],
   "source": [
    "response=with_message_history.invoke(\n",
    "    [HumanMessage(content=\"what is my name\")],\n",
    "    config=config2\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant. Answer all the questions to the best of your ability\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain=prompt|model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hi HV, it's nice to meet you!  \\n\\nI'm ready to help with any questions you have.  What can I do for you today? üòä \\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 39, 'prompt_tokens': 30, 'total_tokens': 69, 'completion_time': 0.070909091, 'prompt_time': 0.00013747, 'queue_time': 0.021839108, 'total_time': 0.071046561}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-9bbcba01-80cd-4d34-8f91-30185ab8e569-0', usage_metadata={'input_tokens': 30, 'output_tokens': 39, 'total_tokens': 69})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"messages\":[HumanMessage(content=\"hi my name is HV\")]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history=RunnableWithMessageHistory(chain, get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Hello John Wick! It's nice to meet you.  \\n\\nAs a helpful assistant, I'm here to answer your questions and chat about anything you like.  \\n\\nSince you enjoy action movies, I can:\\n\\n* **Recommend some action movies you might like**, based on your favorites. Just tell me what you've enjoyed in the past!\\n* **Discuss specific action movie tropes or genres** you're interested in. Do you prefer martial arts films, gun-fu, explosions, or something else entirely?\\n* **Help you find information about your favorite action movies**, like release dates, cast lists, or trivia.\\n\\nWhat can I do for you first, John Wick? \\n\\n\" additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 148, 'prompt_tokens': 35, 'total_tokens': 183, 'completion_time': 0.269090909, 'prompt_time': 0.000305959, 'queue_time': 0.069165928, 'total_time': 0.269396868}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None} id='run-c0aa5b69-2b64-4f14-b03e-6e48b86dd6c1-0' usage_metadata={'input_tokens': 35, 'output_tokens': 148, 'total_tokens': 183}\n"
     ]
    }
   ],
   "source": [
    "config3={\"configurable\":{\"session_id\":'JW'}}\n",
    "\n",
    "response=with_message_history.invoke(\n",
    "    [HumanMessage(content=\"my name is John Wick and i like action movies\")],\n",
    "    config=config3\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"You told me your name is John Wick and you like action movies!  \\n\\nIs there anything else you'd like to talk about, John Wick? Perhaps a particular action movie you've been wanting to discuss? üòÑ  \\n\" additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 49, 'prompt_tokens': 199, 'total_tokens': 248, 'completion_time': 0.089090909, 'prompt_time': 0.007222341, 'queue_time': 0.021712174, 'total_time': 0.09631325}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None} id='run-00a086ce-8b50-49c3-bd3e-a62eff7f5fe9-0' usage_metadata={'input_tokens': 199, 'output_tokens': 49, 'total_tokens': 248}\n"
     ]
    }
   ],
   "source": [
    "response=with_message_history.invoke(\n",
    "    [HumanMessage(content=\"what is my name and what do i like\")],\n",
    "    config=config3\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### adding languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant. Answer all the questions to the best of your ability in the language {language}\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\")\n",
    "    ]\n",
    ")\n",
    "chain=prompt|model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='‡§Æ‡•Å‡§ù‡•á ‡§Ü‡§™‡§ï‡•á ‡§®‡§æ‡§Æ ‡§î‡§∞ ‡§™‡§∏‡§Ç‡§¶ ‡§ï‡•á ‡§¨‡§æ‡§∞‡•á ‡§Æ‡•á‡§Ç ‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä ‡§®‡§π‡•Ä‡§Ç ‡§π‡•à ‡§ï‡•ç‡§Ø‡•ã‡§Ç‡§ï‡§ø ‡§Æ‡•à‡§Ç ‡§è‡§ï ‡§≠‡§æ‡§∑‡§æ ‡§Æ‡•â‡§°‡§≤ ‡§π‡•Ç‡§Å ‡§î‡§∞ ‡§Æ‡•á‡§∞‡•á ‡§™‡§æ‡§∏ ‡§ï‡•ã‡§à ‡§Ø‡§æ‡§¶‡§¶‡§æ‡§∂‡•ç‡§§ ‡§Ø‡§æ ‡§µ‡•ç‡§Ø‡§ï‡•ç‡§§‡§ø‡§ó‡§§ ‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä ‡§®‡§π‡•Ä‡§Ç ‡§π‡•ã‡§§‡•Ä‡•§ \\n\\n‡§Ü‡§™‡§ï‡§æ ‡§®‡§æ‡§Æ ‡§ï‡•ç‡§Ø‡§æ ‡§π‡•à ‡§î‡§∞ ‡§Ü‡§™‡§ï‡•ã ‡§ï‡•ç‡§Ø‡§æ ‡§™‡§∏‡§Ç‡§¶ ‡§π‡•à, ‡§Ø‡§π ‡§¨‡§§‡§æ‡§è‡§Ç ‡§§‡•ã ‡§Æ‡•à‡§Ç ‡§Ü‡§™‡§∏‡•á ‡§¨‡•á‡§π‡§§‡§∞ ‡§§‡§∞‡•Ä‡§ï‡•á ‡§∏‡•á ‡§¨‡§æ‡§§ ‡§ï‡§∞ ‡§∏‡§ï‡•Ç‡§Å‡§ó‡§æ‡•§ \\n\\n' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 77, 'prompt_tokens': 38, 'total_tokens': 115, 'completion_time': 0.14, 'prompt_time': 0.000301199, 'queue_time': 0.022759879, 'total_time': 0.140301199}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None} id='run-36944d31-31aa-412e-bc4b-f533c84ce5bf-0' usage_metadata={'input_tokens': 38, 'output_tokens': 77, 'total_tokens': 115}\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke({\"messages\":[HumanMessage(content=\"what is my name and what do i like\")], \"language\":\"Hindi\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### trimming chat history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage, trim_messages\n",
    "\n",
    "trimmer=trim_messages(\n",
    "    max_tokens=60,\n",
    "    strategy=\"last\",\n",
    "    token_counter=model,\n",
    "    include_system=True,\n",
    "    allow_partial=False,\n",
    "    start_on=\"human\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Harsh Vora\\OneDrive\\Desktop\\PROJECTS\\agenticai\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\Harsh Vora\\OneDrive\\Desktop\\PROJECTS\\agenticai\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Harsh Vora\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"you're a good assistant\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content=\"hi! I'm bob\", additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='hi!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='I like vanilla ice cream', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='nice', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='whats 2 + 2', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='4', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='thanks', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='no problem!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='having fun?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='yes!', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"you're a good assistant\"),\n",
    "    HumanMessage(content=\"hi! I'm bob\"),\n",
    "    AIMessage(content=\"hi!\"),\n",
    "    HumanMessage(content=\"I like vanilla ice cream\"),\n",
    "    AIMessage(content=\"nice\"),\n",
    "    HumanMessage(content=\"whats 2 + 2\"),\n",
    "    AIMessage(content=\"4\"),\n",
    "    HumanMessage(content=\"thanks\"),\n",
    "    AIMessage(content=\"no problem!\"),\n",
    "    HumanMessage(content=\"having fun?\"),\n",
    "    AIMessage(content=\"yes!\"),\n",
    "]\n",
    "trimmer.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "chain = RunnablePassthrough.assign(#####fwefwedwqqwdqwqwdq=itemgetter('messages')| trimmer)|prompt|model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableAssign(mapper={\n",
       "  messages: RunnableLambda(itemgetter('messages'))\n",
       "            | RunnableLambda(...)\n",
       "})\n",
       "| ChatPromptTemplate(input_variables=['language', 'messages'], input_types={'messages': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x0000022EBB948540>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['language'], input_types={}, partial_variables={}, template='You are a helpful assistant. Answer all the questions to the best of your ability in the language {language}'), additional_kwargs={}), MessagesPlaceholder(variable_name='messages')])\n",
       "| ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x0000022EBD87BFB0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x0000022EBD8AD0D0>, model_name='gemma2-9b-it', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "response=chain.invoke(\n",
    "    {\n",
    "        \"messages\":messages+[HumanMessage(content=\"what icecream do i like?\")],\n",
    "        \"language\":\"English\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='You said you like vanilla ice cream!  üç¶ üòä  Is that still your favorite? \\n' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 105, 'total_tokens': 127, 'completion_time': 0.04, 'prompt_time': 0.003698001, 'queue_time': 0.021340646, 'total_time': 0.043698001}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None} id='run-b3bf4d3f-951a-43cf-a948-50c89894c666-0' usage_metadata={'input_tokens': 105, 'output_tokens': 22, 'total_tokens': 127}\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

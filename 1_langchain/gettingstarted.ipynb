{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enable langchain tracing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPEN_API_KEY\"]=os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"LANGCHAIN_API_KEY\"]=os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"]=os.getenv(\"LANGCHAIN_PROJECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.chat.completions.Completions object at 0x000001A01412F170> async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000001A01414CE60> root_client=<openai.OpenAI object at 0x000001A011726630> root_async_client=<openai.AsyncOpenAI object at 0x000001A0140DD580> model_name='gpt-4o-mini' model_kwargs={} openai_api_key=SecretStr('**********')\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm=ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Agentic AI refers to artificial intelligence systems that possess a degree of autonomy and can make decisions, take actions, or exhibit behavior that appears self-directed. In this context, \"agentic\" implies the ability to act with some level of independence and decision-making capability, rather than merely following direct instructions or scripts.\\n\\nKey characteristics that can define agentic AI include:\\n\\n1. **Autonomy**: The ability to operate and make choices without human intervention. Agentic AI systems can analyze situations, weigh options, and select actions based on their programming and the data available to them.\\n\\n2. **Goal-Oriented Behavior**: These systems can be designed to pursue specific objectives, adapting their actions based on changing circumstances to achieve those goals.\\n\\n3. **Learning and Adaptation**: Some agentic AI systems can learn from their experiences, enhancing their decision-making abilities over time. This can involve reinforcement learning, where the system receives feedback based on the outcomes of its actions.\\n\\n4. **Complex Decision-Making**: Agentic AI systems may address complex, dynamic environments where many variables must be considered simultaneously.\\n\\n5. **Interactivity**: They can interact with other systems, environments, or humans in a way that involves negotiation, collaboration, or even competition.\\n\\nExamples of agentic AI can be seen in autonomous vehicles, advanced robotics, and certain game AI systems, which exhibit behaviors that suggest a level of decision-making autonomy. However, the degree of agency can vary significantly, and many AI systems still operate within parameters set by human designers. As AI technology advances, discussions around the implications of agentic AIâ€”especially regarding ethics, control, and accountabilityâ€”become increasingly important.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 336, 'prompt_tokens': 13, 'total_tokens': 349, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_bd83329f63', 'finish_reason': 'stop', 'logprobs': None} id='run-de536d2c-07cf-4a42-ab28-fcaa1dd564d3-0' usage_metadata={'input_tokens': 13, 'output_tokens': 336, 'total_tokens': 349, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "result=llm.invoke(\"What is agentic AI?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agentic AI refers to artificial intelligence systems that possess a degree of autonomy and can make decisions, take actions, or exhibit behavior that appears self-directed. In this context, \"agentic\" implies the ability to act with some level of independence and decision-making capability, rather than merely following direct instructions or scripts.\n",
      "\n",
      "Key characteristics that can define agentic AI include:\n",
      "\n",
      "1. **Autonomy**: The ability to operate and make choices without human intervention. Agentic AI systems can analyze situations, weigh options, and select actions based on their programming and the data available to them.\n",
      "\n",
      "2. **Goal-Oriented Behavior**: These systems can be designed to pursue specific objectives, adapting their actions based on changing circumstances to achieve those goals.\n",
      "\n",
      "3. **Learning and Adaptation**: Some agentic AI systems can learn from their experiences, enhancing their decision-making abilities over time. This can involve reinforcement learning, where the system receives feedback based on the outcomes of its actions.\n",
      "\n",
      "4. **Complex Decision-Making**: Agentic AI systems may address complex, dynamic environments where many variables must be considered simultaneously.\n",
      "\n",
      "5. **Interactivity**: They can interact with other systems, environments, or humans in a way that involves negotiation, collaboration, or even competition.\n",
      "\n",
      "Examples of agentic AI can be seen in autonomous vehicles, advanced robotics, and certain game AI systems, which exhibit behaviors that suggest a level of decision-making autonomy. However, the degree of agency can vary significantly, and many AI systems still operate within parameters set by human designers. As AI technology advances, discussions around the implications of agentic AIâ€”especially regarding ethics, control, and accountabilityâ€”become increasingly important.\n"
     ]
    }
   ],
   "source": [
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CREATING chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt=ChatPromptTemplate(\n",
    "    [\n",
    "        (\"system\", \"You are an expert AI Engineer. Provide me answer based on the question\"),\n",
    "        (\"user\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI Engineer. Provide me answer based on the question'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain=prompt|llm\n",
    "response=chain.invoke({\"input\":\"can you tell me about langsmith\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Langsmith, as of my last knowledge update in October 2023, refers to a tool or platform that revolves around AI and machine learning for natural language processing (NLP) tasks. It may offer services for building, training, or deploying language models and applications that utilize language understanding, generation, or other NLP capabilities.\\n\\nWhile specific features and offerings might vary, platforms like Langsmith typically aim to simplify the development process for developers and businesses looking to integrate AI-driven language solutions. They may provide support for various programming languages and frameworks, along with APIs for easy integration into existing applications.\\n\\nIf Langsmith has evolved or changed significantly after October 2023, I would recommend checking their official website or recent news for the most accurate and up-to-date information.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 152, 'prompt_tokens': 32, 'total_tokens': 184, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c', 'finish_reason': 'stop', 'logprobs': None} id='run-b93e08fd-783b-4ecd-ac3f-bbd74bfc4698-0' usage_metadata={'input_tokens': 32, 'output_tokens': 152, 'total_tokens': 184, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Langsmith, as of my last knowledge update in October 2023, refers to a tool or platform that revolves around AI and machine learning for natural language processing (NLP) tasks. It may offer services for building, training, or deploying language models and applications that utilize language understanding, generation, or other NLP capabilities.\n",
      "\n",
      "While specific features and offerings might vary, platforms like Langsmith typically aim to simplify the development process for developers and businesses looking to integrate AI-driven language solutions. They may provide support for various programming languages and frameworks, along with APIs for easy integration into existing applications.\n",
      "\n",
      "If Langsmith has evolved or changed significantly after October 2023, I would recommend checking their official website or recent news for the most accurate and up-to-date information.\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "output_parser=StrOutputParser()\n",
    "\n",
    "chain=prompt|llm|output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith is a platform designed for creating and managing AI applications, particularly those that utilize large language models (LLMs). It provides tools for developers and data scientists to build, train, and deploy language-based AI solutions efficiently. LangSmith often emphasizes collaboration, allowing teams to work together on AI projects, integrate various data sources, and automate workflows.\n",
      "\n",
      "The platform may include features for fine-tuning models, testing, and deploying applications, as well as monitoring performance and collecting user feedback. This can enable users to refine their applications over time based on real-world interactions.\n",
      "\n",
      "For the most specific and current details, I recommend visiting the official LangSmith website or checking their most recent announcements and documentation, as platforms like these may evolve rapidly.\n"
     ]
    }
   ],
   "source": [
    "response=chain.invoke({\"input\":\"can you tell me about langsmith\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testing with JSONPARSER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "parser = JsonOutputParser()\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'definition': 'Agentic AI refers to artificial intelligence systems that possess the ability to act autonomously and make decisions in a manner that reflects an understanding of their environment and goals. These systems can operate independently, adapting their behavior based on interactions and feedback.', 'key_features': ['Autonomy', 'Decision-making capabilities', 'Adaptability to environments', 'Goal-oriented behavior'], 'examples': ['Self-driving cars', 'Autonomous drones', 'Robotic process automation'], 'potential_implications': ['Ethical considerations regarding decision-making', 'Impact on employment and labor markets', 'Regulatory challenges for oversight']}\n"
     ]
    }
   ],
   "source": [
    "chain=prompt|llm|parser\n",
    "response=chain.invoke({\"query\":\"What is agentic AI?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader=WebBaseLoader(\"https://python.langchain.com/api_reference/langchain/chains/langchain.chains.llm.LLMChain.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents=loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://python.langchain.com/api_reference/langchain/chains/langchain.chains.llm.LLMChain.html', 'title': 'LLMChain â€” ðŸ¦œðŸ”— LangChain  documentation', 'language': 'en'}, page_content='\\n\\n\\n\\n\\n\\n\\n\\n\\nLLMChain â€” ðŸ¦œðŸ”— LangChain  documentation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to main content\\n\\n\\nBack to top\\n\\n\\n\\n\\nCtrl+K\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Reference\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCtrl+K\\n\\n\\n\\n\\n\\n\\n\\nDocs\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nGitHub\\n\\n\\n\\nX / Twitter\\n\\n\\n\\n\\n\\n\\n\\n\\nCtrl+K\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Reference\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDocs\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nGitHub\\n\\n\\n\\nX / Twitter\\n\\n\\n\\n\\n\\n\\n\\nSection Navigation\\nBase packages\\n\\nCore\\nLangchain\\nagents\\ncallbacks\\nchains\\nChain\\nBaseCombineDocumentsChain\\nAsyncCombineDocsProtocol\\nCombineDocsProtocol\\nConstitutionalPrinciple\\nBaseConversationalRetrievalChain\\nChatVectorDBChain\\nInputType\\nElasticsearchDatabaseChain\\nFlareChain\\nQuestionGeneratorChain\\nFinishedOutputParser\\nHypotheticalDocumentEmbedder\\nOpenAIModerationChain\\nCrawler\\nElementInViewPort\\nFactWithEvidence\\nQuestionAnswer\\nSimpleRequestChain\\nAnswerWithSources\\nBasePromptSelector\\nConditionalPromptSelector\\nLoadingCallable\\nRetrievalQAWithSourcesChain\\nVectorDBQAWithSourcesChain\\nStructuredQueryOutputParser\\nISO8601Date\\nISO8601DateTime\\nAttributeInfo\\nLoadingCallable\\nMultiRouteChain\\nRoute\\nRouterChain\\nEmbeddingRouterChain\\nRouterOutputParser\\nMultiRetrievalQAChain\\nSequentialChain\\nSimpleSequentialChain\\nSQLInput\\nSQLInputWithTables\\nLoadingCallable\\nTransformChain\\nacollapse_docs\\ncollapse_docs\\nsplit_list_of_docs\\ncreate_stuff_documents_chain\\ngenerate_example\\ncreate_history_aware_retriever\\ncreate_citation_fuzzy_match_runnable\\nopenapi_spec_to_openai_fn\\nget_llm_kwargs\\nis_chat_model\\nis_llm\\nconstruct_examples\\nfix_filter_directive\\nget_query_constructor_prompt\\nload_query_constructor_runnable\\nget_parser\\nv_args\\ncreate_retrieval_chain\\ncreate_sql_query_chain\\nget_openai_output_parser\\nload_summarize_chain\\nAPIChain\\nAnalyzeDocumentChain\\nMapReduceDocumentsChain\\nMapRerankDocumentsChain\\nReduceDocumentsChain\\nRefineDocumentsChain\\nStuffDocumentsChain\\nConstitutionalChain\\nConversationChain\\nConversationalRetrievalChain\\nLLMChain\\nLLMCheckerChain\\nLLMMathChain\\nLLMSummarizationCheckerChain\\nMapReduceChain\\nNatBotChain\\nQAGenerationChain\\nBaseQAWithSourcesChain\\nQAWithSourcesChain\\nBaseRetrievalQA\\nRetrievalQA\\nVectorDBQA\\nLLMRouterChain\\nMultiPromptChain\\nload_chain\\nload_chain_from_config\\ncreate_openai_fn_chain\\ncreate_structured_output_chain\\ncreate_citation_fuzzy_match_chain\\ncreate_extraction_chain\\ncreate_extraction_chain_pydantic\\nget_openapi_chain\\ncreate_qa_with_sources_chain\\ncreate_qa_with_structure_chain\\ncreate_tagging_chain\\ncreate_tagging_chain_pydantic\\ncreate_extraction_chain_pydantic\\nload_qa_with_sources_chain\\nload_query_constructor_chain\\nload_qa_chain\\ncreate_openai_fn_runnable\\ncreate_structured_output_runnable\\n\\n\\nchat_models\\nembeddings\\nevaluation\\nglobals\\nhub\\nindexes\\nmemory\\nmodel_laboratory\\noutput_parsers\\nretrievers\\nrunnables\\nsmith\\nstorage\\n\\n\\nText Splitters\\nCommunity\\nExperimental\\n\\nIntegrations\\n\\nAI21\\nAnthropic\\nAstraDB\\nAWS\\nAzure Dynamic Sessions\\nCerebras\\nChroma\\nCohere\\nDeepseek\\nElasticsearch\\nExa\\nFireworks\\nGoogle Community\\nGoogle GenAI\\nGoogle VertexAI\\nGroq\\nHuggingface\\nIBM\\nMilvus\\nMistralAI\\nNeo4J\\nNomic\\nNvidia Ai Endpoints\\nOllama\\nOpenAI\\nPinecone\\nPostgres\\nPrompty\\nQdrant\\nRedis\\nSema4\\nSnowflake\\nSqlserver\\nStandard Tests\\nTogether\\nUnstructured\\nUpstage\\nVoyageAI\\nWeaviate\\nXAI\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLangChain Python API Reference\\nlangchain: 0.3.18\\nchains\\nLLMChain\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLLMChain#\\n\\n\\nclass langchain.chains.llm.LLMChain[source]#\\nBases: Chain\\n\\nDeprecated since version 0.1.17: Use , `prompt | llm`() instead. It will not be removed until langchain==1.0.\\n\\nChain to run queries against LLMs.\\nThis class is deprecated. See below for an example implementation using\\nLangChain runnables:\\n\\nfrom langchain_core.output_parsers import StrOutputParser\\nfrom langchain_core.prompts import PromptTemplate\\nfrom langchain_openai import OpenAI\\n\\nprompt_template = \"Tell me a {adjective} joke\"\\nprompt = PromptTemplate(\\n    input_variables=[\"adjective\"], template=prompt_template\\n)\\nllm = OpenAI()\\nchain = prompt | llm | StrOutputParser()\\n\\nchain.invoke(\"your adjective here\")\\n\\n\\n\\nExample\\nfrom langchain.chains import LLMChain\\nfrom langchain_community.llms import OpenAI\\nfrom langchain_core.prompts import PromptTemplate\\nprompt_template = \"Tell me a {adjective} joke\"\\nprompt = PromptTemplate(\\n    input_variables=[\"adjective\"], template=prompt_template\\n)\\nllm = LLMChain(llm=OpenAI(), prompt=prompt)\\n\\n\\n\\nNote\\nLLMChain implements the standard Runnable Interface. ðŸƒ\\nThe Runnable Interface has additional methods that are available on runnables, such as with_types, with_retry, assign, bind, get_graph, and more.\\n\\n\\n\\nparam callback_manager: BaseCallbackManager | None = None#\\n[DEPRECATED] Use callbacks instead.\\n\\n\\n\\nparam callbacks: Callbacks = None#\\nOptional list of callback handlers (or callback manager). Defaults to None.\\nCallback handlers are called throughout the lifecycle of a call to a chain,\\nstarting with on_chain_start, ending with on_chain_end or on_chain_error.\\nEach custom chain can optionally call additional callback methods, see Callback docs\\nfor full details.\\n\\n\\n\\nparam llm: Runnable[LanguageModelInput, str] | Runnable[LanguageModelInput, BaseMessage] [Required]#\\nLanguage model to call.\\n\\n\\n\\nparam llm_kwargs: dict [Optional]#\\n\\n\\n\\nparam memory: BaseMemory | None = None#\\nOptional memory object. Defaults to None.\\nMemory is a class that gets called at the start\\nand at the end of every chain. At the start, memory loads variables and passes\\nthem along in the chain. At the end, it saves any returned variables.\\nThere are many different types of memory - please see memory docs\\nfor the full catalog.\\n\\n\\n\\nparam metadata: Dict[str, Any] | None = None#\\nOptional metadata associated with the chain. Defaults to None.\\nThis metadata will be associated with each call to this chain,\\nand passed as arguments to the handlers defined in callbacks.\\nYou can use these to eg identify a specific instance of a chain with its use case.\\n\\n\\n\\nparam output_parser: BaseLLMOutputParser [Optional]#\\nOutput parser to use.\\nDefaults to one that takes the most likely string but does not change it\\notherwise.\\n\\n\\n\\nparam prompt: BasePromptTemplate [Required]#\\nPrompt object to use.\\n\\n\\n\\nparam return_final_only: bool = True#\\nWhether to return only the final parsed result. Defaults to True.\\nIf false, will return a bunch of extra information about the generation.\\n\\n\\n\\nparam tags: List[str] | None = None#\\nOptional list of tags associated with the chain. Defaults to None.\\nThese tags will be associated with each call to this chain,\\nand passed as arguments to the handlers defined in callbacks.\\nYou can use these to eg identify a specific instance of a chain with its use case.\\n\\n\\n\\nparam verbose: bool [Optional]#\\nWhether or not run in verbose mode. In verbose mode, some intermediate logs\\nwill be printed to the console. Defaults to the global verbose value,\\naccessible via langchain.globals.get_verbose().\\n\\n\\n\\n__call__(inputs: Dict[str, Any] | Any, return_only_outputs: bool = False, callbacks: list[BaseCallbackHandler] | BaseCallbackManager | None = None, *, tags: List[str] | None = None, metadata: Dict[str, Any] | None = None, run_name: str | None = None, include_run_info: bool = False) â†’ Dict[str, Any]#\\n\\nDeprecated since version 0.1.0: Use invoke() instead. It will not be removed until langchain==1.0.\\n\\nExecute the chain.\\n\\nParameters:\\n\\ninputs (Dict[str, Any] | Any) â€“ Dictionary of inputs, or single input if chain expects\\nonly one param. Should contain all inputs specified in\\nChain.input_keys except for inputs that will be set by the chainâ€™s\\nmemory.\\nreturn_only_outputs (bool) â€“ Whether to return only outputs in the\\nresponse. If True, only new keys generated by this chain will be\\nreturned. If False, both input keys and new keys generated by this\\nchain will be returned. Defaults to False.\\ncallbacks (list[BaseCallbackHandler] | BaseCallbackManager | None) â€“ Callbacks to use for this chain run. These will be called in\\naddition to callbacks passed to the chain during construction, but only\\nthese runtime callbacks will propagate to calls to other objects.\\ntags (List[str] | None) â€“ List of string tags to pass to all callbacks. These will be passed in\\naddition to tags passed to the chain during construction, but only\\nthese runtime tags will propagate to calls to other objects.\\nmetadata (Dict[str, Any] | None) â€“ Optional metadata associated with the chain. Defaults to None\\ninclude_run_info (bool) â€“ Whether to include run info in the response. Defaults\\nto False.\\nrun_name (str | None)\\n\\n\\nReturns:\\n\\nA dict of named outputs. Should contain all outputs specified inChain.output_keys.\\n\\n\\n\\n\\nReturn type:\\nDict[str, Any]\\n\\n\\n\\n\\n\\nasync aapply(input_list: List[Dict[str, Any]], callbacks: list[BaseCallbackHandler] | BaseCallbackManager | None = None) â†’ List[Dict[str, str]][source]#\\nUtilize the LLM generate method for speed gains.\\n\\nParameters:\\n\\ninput_list (List[Dict[str, Any]])\\ncallbacks (list[BaseCallbackHandler] | BaseCallbackManager | None)\\n\\n\\nReturn type:\\nList[Dict[str, str]]\\n\\n\\n\\n\\n\\nasync aapply_and_parse(input_list: List[Dict[str, Any]], callbacks: list[BaseCallbackHandler] | BaseCallbackManager | None = None) â†’ Sequence[str | List[str] | Dict[str, str]][source]#\\nCall apply and then parse the results.\\n\\nParameters:\\n\\ninput_list (List[Dict[str, Any]])\\ncallbacks (list[BaseCallbackHandler] | BaseCallbackManager | None)\\n\\n\\nReturn type:\\nSequence[str | List[str] | Dict[str, str]]\\n\\n\\n\\n\\n\\nasync abatch(inputs: list[Input], config: RunnableConfig | list[RunnableConfig] | None = None, *, return_exceptions: bool = False, **kwargs: Any | None) â†’ list[Output]#\\nDefault implementation runs ainvoke in parallel using asyncio.gather.\\nThe default implementation of batch works well for IO bound runnables.\\nSubclasses should override this method if they can batch more efficiently;\\ne.g., if the underlying Runnable uses an API which supports a batch mode.\\n\\nParameters:\\n\\ninputs (list[Input]) â€“ A list of inputs to the Runnable.\\nconfig (RunnableConfig | list[RunnableConfig] | None) â€“ A config to use when invoking the Runnable.\\nThe config supports standard keys like â€˜tagsâ€™, â€˜metadataâ€™ for tracing\\npurposes, â€˜max_concurrencyâ€™ for controlling how much work to do\\nin parallel, and other keys. Please refer to the RunnableConfig\\nfor more details. Defaults to None.\\nreturn_exceptions (bool) â€“ Whether to return exceptions instead of raising them.\\nDefaults to False.\\nkwargs (Any | None) â€“ Additional keyword arguments to pass to the Runnable.\\n\\n\\nReturns:\\nA list of outputs from the Runnable.\\n\\nReturn type:\\nlist[Output]\\n\\n\\n\\n\\n\\nasync abatch_as_completed(inputs: Sequence[Input], config: RunnableConfig | Sequence[RunnableConfig] | None = None, *, return_exceptions: bool = False, **kwargs: Any | None) â†’ AsyncIterator[tuple[int, Output | Exception]]#\\nRun ainvoke in parallel on a list of inputs,\\nyielding results as they complete.\\n\\nParameters:\\n\\ninputs (Sequence[Input]) â€“ A list of inputs to the Runnable.\\nconfig (RunnableConfig | Sequence[RunnableConfig] | None) â€“ A config to use when invoking the Runnable.\\nThe config supports standard keys like â€˜tagsâ€™, â€˜metadataâ€™ for tracing\\npurposes, â€˜max_concurrencyâ€™ for controlling how much work to do\\nin parallel, and other keys. Please refer to the RunnableConfig\\nfor more details. Defaults to None. Defaults to None.\\nreturn_exceptions (bool) â€“ Whether to return exceptions instead of raising them.\\nDefaults to False.\\nkwargs (Any | None) â€“ Additional keyword arguments to pass to the Runnable.\\n\\n\\nYields:\\nA tuple of the index of the input and the output from the Runnable.\\n\\nReturn type:\\nAsyncIterator[tuple[int, Output | Exception]]\\n\\n\\n\\n\\n\\nasync acall(inputs: Dict[str, Any] | Any, return_only_outputs: bool = False, callbacks: list[BaseCallbackHandler] | BaseCallbackManager | None = None, *, tags: List[str] | None = None, metadata: Dict[str, Any] | None = None, run_name: str | None = None, include_run_info: bool = False) â†’ Dict[str, Any]#\\n\\nDeprecated since version 0.1.0: Use ainvoke() instead. It will not be removed until langchain==1.0.\\n\\nAsynchronously execute the chain.\\n\\nParameters:\\n\\ninputs (Dict[str, Any] | Any) â€“ Dictionary of inputs, or single input if chain expects\\nonly one param. Should contain all inputs specified in\\nChain.input_keys except for inputs that will be set by the chainâ€™s\\nmemory.\\nreturn_only_outputs (bool) â€“ Whether to return only outputs in the\\nresponse. If True, only new keys generated by this chain will be\\nreturned. If False, both input keys and new keys generated by this\\nchain will be returned. Defaults to False.\\ncallbacks (list[BaseCallbackHandler] | BaseCallbackManager | None) â€“ Callbacks to use for this chain run. These will be called in\\naddition to callbacks passed to the chain during construction, but only\\nthese runtime callbacks will propagate to calls to other objects.\\ntags (List[str] | None) â€“ List of string tags to pass to all callbacks. These will be passed in\\naddition to tags passed to the chain during construction, but only\\nthese runtime tags will propagate to calls to other objects.\\nmetadata (Dict[str, Any] | None) â€“ Optional metadata associated with the chain. Defaults to None\\ninclude_run_info (bool) â€“ Whether to include run info in the response. Defaults\\nto False.\\nrun_name (str | None)\\n\\n\\nReturns:\\n\\nA dict of named outputs. Should contain all outputs specified inChain.output_keys.\\n\\n\\n\\n\\nReturn type:\\nDict[str, Any]\\n\\n\\n\\n\\n\\nasync ainvoke(input: Dict[str, Any], config: RunnableConfig | None = None, **kwargs: Any) â†’ Dict[str, Any]#\\nDefault implementation of ainvoke, calls invoke from a thread.\\nThe default implementation allows usage of async code even if\\nthe Runnable did not implement a native async version of invoke.\\nSubclasses should override this method if they can run asynchronously.\\n\\nParameters:\\n\\ninput (Dict[str, Any])\\nconfig (RunnableConfig | None)\\nkwargs (Any)\\n\\n\\nReturn type:\\nDict[str, Any]\\n\\n\\n\\n\\n\\napply(input_list: List[Dict[str, Any]], callbacks: list[BaseCallbackHandler] | BaseCallbackManager | None = None) â†’ List[Dict[str, str]][source]#\\nUtilize the LLM generate method for speed gains.\\n\\nParameters:\\n\\ninput_list (List[Dict[str, Any]])\\ncallbacks (list[BaseCallbackHandler] | BaseCallbackManager | None)\\n\\n\\nReturn type:\\nList[Dict[str, str]]\\n\\n\\n\\n\\n\\napply_and_parse(input_list: List[Dict[str, Any]], callbacks: list[BaseCallbackHandler] | BaseCallbackManager | None = None) â†’ Sequence[str | List[str] | Dict[str, str]][source]#\\nCall apply and then parse the results.\\n\\nParameters:\\n\\ninput_list (List[Dict[str, Any]])\\ncallbacks (list[BaseCallbackHandler] | BaseCallbackManager | None)\\n\\n\\nReturn type:\\nSequence[str | List[str] | Dict[str, str]]\\n\\n\\n\\n\\n\\nasync apredict_and_parse(callbacks: list[BaseCallbackHandler] | BaseCallbackManager | None = None, **kwargs: Any) â†’ str | List[str] | Dict[str, str][source]#\\nCall apredict and then parse the results.\\n\\nParameters:\\n\\ncallbacks (list[BaseCallbackHandler] | BaseCallbackManager | None)\\nkwargs (Any)\\n\\n\\nReturn type:\\nstr | List[str] | Dict[str, str]\\n\\n\\n\\n\\n\\nasync aprep_inputs(inputs: Dict[str, Any] | Any) â†’ Dict[str, str]#\\nPrepare chain inputs, including adding inputs from memory.\\n\\nParameters:\\ninputs (Dict[str, Any] | Any) â€“ Dictionary of raw inputs, or single input if chain expects\\nonly one param. Should contain all inputs specified in\\nChain.input_keys except for inputs that will be set by the chainâ€™s\\nmemory.\\n\\nReturns:\\nA dictionary of all inputs, including those added by the chainâ€™s memory.\\n\\nReturn type:\\nDict[str, str]\\n\\n\\n\\n\\n\\nasync aprep_outputs(inputs: Dict[str, str], outputs: Dict[str, str], return_only_outputs: bool = False) â†’ Dict[str, str]#\\nValidate and prepare chain outputs, and save info about this run to memory.\\n\\nParameters:\\n\\ninputs (Dict[str, str]) â€“ Dictionary of chain inputs, including any inputs added by chain\\nmemory.\\noutputs (Dict[str, str]) â€“ Dictionary of initial chain outputs.\\nreturn_only_outputs (bool) â€“ Whether to only return the chain outputs. If False,\\ninputs are also added to the final outputs.\\n\\n\\nReturns:\\nA dict of the final chain outputs.\\n\\nReturn type:\\nDict[str, str]\\n\\n\\n\\n\\n\\nasync aprep_prompts(input_list: List[Dict[str, Any]], run_manager: AsyncCallbackManagerForChainRun | None = None) â†’ Tuple[List[PromptValue], List[str] | None][source]#\\nPrepare prompts from inputs.\\n\\nParameters:\\n\\ninput_list (List[Dict[str, Any]])\\nrun_manager (AsyncCallbackManagerForChainRun | None)\\n\\n\\nReturn type:\\nTuple[List[PromptValue], List[str] | None]\\n\\n\\n\\n\\n\\nasync arun(*args: Any, callbacks: list[BaseCallbackHandler] | BaseCallbackManager | None = None, tags: List[str] | None = None, metadata: Dict[str, Any] | None = None, **kwargs: Any) â†’ Any#\\n\\nDeprecated since version 0.1.0: Use ainvoke() instead. It will not be removed until langchain==1.0.\\n\\nConvenience method for executing chain.\\nThe main difference between this method and Chain.__call__ is that this\\nmethod expects inputs to be passed directly in as positional arguments or\\nkeyword arguments, whereas Chain.__call__ expects a single input dictionary\\nwith all the inputs\\n\\nParameters:\\n\\n*args (Any) â€“ If the chain expects a single input, it can be passed in as the\\nsole positional argument.\\ncallbacks (list[BaseCallbackHandler] | BaseCallbackManager | None) â€“ Callbacks to use for this chain run. These will be called in\\naddition to callbacks passed to the chain during construction, but only\\nthese runtime callbacks will propagate to calls to other objects.\\ntags (List[str] | None) â€“ List of string tags to pass to all callbacks. These will be passed in\\naddition to tags passed to the chain during construction, but only\\nthese runtime tags will propagate to calls to other objects.\\n**kwargs (Any) â€“ If the chain expects multiple inputs, they can be passed in\\ndirectly as keyword arguments.\\nmetadata (Dict[str, Any] | None)\\n**kwargs\\n\\n\\nReturns:\\nThe chain output.\\n\\nReturn type:\\nAny\\n\\n\\nExample\\n# Suppose we have a single-input chain that takes a \\'question\\' string:\\nawait chain.arun(\"What\\'s the temperature in Boise, Idaho?\")\\n# -> \"The temperature in Boise is...\"\\n\\n# Suppose we have a multi-input chain that takes a \\'question\\' string\\n# and \\'context\\' string:\\nquestion = \"What\\'s the temperature in Boise, Idaho?\"\\ncontext = \"Weather report for Boise, Idaho on 07/03/23...\"\\nawait chain.arun(question=question, context=context)\\n# -> \"The temperature in Boise is...\"\\n\\n\\n\\n\\n\\nasync astream(input: Input, config: RunnableConfig | None = None, **kwargs: Any | None) â†’ AsyncIterator[Output]#\\nDefault implementation of astream, which calls ainvoke.\\nSubclasses should override this method if they support streaming output.\\n\\nParameters:\\n\\ninput (Input) â€“ The input to the Runnable.\\nconfig (RunnableConfig | None) â€“ The config to use for the Runnable. Defaults to None.\\nkwargs (Any | None) â€“ Additional keyword arguments to pass to the Runnable.\\n\\n\\nYields:\\nThe output of the Runnable.\\n\\nReturn type:\\nAsyncIterator[Output]\\n\\n\\n\\n\\n\\nasync astream_events(input: Any, config: RunnableConfig | None = None, *, version: Literal[\\'v1\\', \\'v2\\'], include_names: Sequence[str] | None = None, include_types: Sequence[str] | None = None, include_tags: Sequence[str] | None = None, exclude_names: Sequence[str] | None = None, exclude_types: Sequence[str] | None = None, exclude_tags: Sequence[str] | None = None, **kwargs: Any) â†’ AsyncIterator[StandardStreamEvent | CustomStreamEvent]#\\nGenerate a stream of events.\\nUse to create an iterator over StreamEvents that provide real-time information\\nabout the progress of the Runnable, including StreamEvents from intermediate\\nresults.\\nA StreamEvent is a dictionary with the following schema:\\n\\n\\nevent: str - Event names are of theformat: on_[runnable_type]_(start|stream|end).\\n\\n\\n\\nname: str - The name of the Runnable that generated the event.\\n\\nrun_id: str - randomly generated ID associated with the given execution ofthe Runnable that emitted the event.\\nA child Runnable that gets invoked as part of the execution of a\\nparent Runnable is assigned its own unique ID.\\n\\n\\n\\n\\nparent_ids: List[str] - The IDs of the parent runnables thatgenerated the event. The root Runnable will have an empty list.\\nThe order of the parent IDs is from the root to the immediate parent.\\nOnly available for v2 version of the API. The v1 version of the API\\nwill return an empty list.\\n\\n\\n\\n\\ntags: Optional[List[str]] - The tags of the Runnable that generatedthe event.\\n\\n\\n\\n\\nmetadata: Optional[Dict[str, Any]] - The metadata of the Runnablethat generated the event.\\n\\n\\n\\ndata: Dict[str, Any]\\n\\nBelow is a table that illustrates some events that might be emitted by various\\nchains. Metadata fields have been omitted from the table for brevity.\\nChain definitions have been included after the table.\\nATTENTION This reference table is for the V2 version of the schema.\\n\\n\\nevent\\nname\\nchunk\\ninput\\noutput\\n\\n\\n\\non_chat_model_start\\n[model name]\\n\\n{â€œmessagesâ€: [[SystemMessage, HumanMessage]]}\\n\\n\\non_chat_model_stream\\n[model name]\\nAIMessageChunk(content=â€helloâ€)\\n\\n\\n\\non_chat_model_end\\n[model name]\\n\\n{â€œmessagesâ€: [[SystemMessage, HumanMessage]]}\\nAIMessageChunk(content=â€hello worldâ€)\\n\\non_llm_start\\n[model name]\\n\\n{â€˜inputâ€™: â€˜helloâ€™}\\n\\n\\non_llm_stream\\n[model name]\\nâ€˜Helloâ€™\\n\\n\\n\\non_llm_end\\n[model name]\\n\\nâ€˜Hello human!â€™\\n\\n\\non_chain_start\\nformat_docs\\n\\n\\n\\n\\non_chain_stream\\nformat_docs\\nâ€œhello world!, goodbye world!â€\\n\\n\\n\\non_chain_end\\nformat_docs\\n\\n[Document(â€¦)]\\nâ€œhello world!, goodbye world!â€\\n\\non_tool_start\\nsome_tool\\n\\n{â€œxâ€: 1, â€œyâ€: â€œ2â€}\\n\\n\\non_tool_end\\nsome_tool\\n\\n\\n{â€œxâ€: 1, â€œyâ€: â€œ2â€}\\n\\non_retriever_start\\n[retriever name]\\n\\n{â€œqueryâ€: â€œhelloâ€}\\n\\n\\non_retriever_end\\n[retriever name]\\n\\n{â€œqueryâ€: â€œhelloâ€}\\n[Document(â€¦), ..]\\n\\non_prompt_start\\n[template_name]\\n\\n{â€œquestionâ€: â€œhelloâ€}\\n\\n\\non_prompt_end\\n[template_name]\\n\\n{â€œquestionâ€: â€œhelloâ€}\\nChatPromptValue(messages: [SystemMessage, â€¦])\\n\\n\\n\\n\\nIn addition to the standard events, users can also dispatch custom events (see example below).\\nCustom events will be only be surfaced with in the v2 version of the API!\\nA custom event has following format:\\n\\n\\nAttribute\\nType\\nDescription\\n\\n\\n\\nname\\nstr\\nA user defined name for the event.\\n\\ndata\\nAny\\nThe data associated with the event. This can be anything, though we suggest making it JSON serializable.\\n\\n\\n\\n\\nHere are declarations associated with the standard events shown above:\\nformat_docs:\\ndef format_docs(docs: List[Document]) -> str:\\n    \\'\\'\\'Format the docs.\\'\\'\\'\\n    return \", \".join([doc.page_content for doc in docs])\\n\\nformat_docs = RunnableLambda(format_docs)\\n\\n\\nsome_tool:\\n@tool\\ndef some_tool(x: int, y: str) -> dict:\\n    \\'\\'\\'Some_tool.\\'\\'\\'\\n    return {\"x\": x, \"y\": y}\\n\\n\\nprompt:\\ntemplate = ChatPromptTemplate.from_messages(\\n    [(\"system\", \"You are Cat Agent 007\"), (\"human\", \"{question}\")]\\n).with_config({\"run_name\": \"my_template\", \"tags\": [\"my_template\"]})\\n\\n\\nExample:\\nfrom langchain_core.runnables import RunnableLambda\\n\\nasync def reverse(s: str) -> str:\\n    return s[::-1]\\n\\nchain = RunnableLambda(func=reverse)\\n\\nevents = [\\n    event async for event in chain.astream_events(\"hello\", version=\"v2\")\\n]\\n\\n# will produce the following events (run_id, and parent_ids\\n# has been omitted for brevity):\\n[\\n    {\\n        \"data\": {\"input\": \"hello\"},\\n        \"event\": \"on_chain_start\",\\n        \"metadata\": {},\\n        \"name\": \"reverse\",\\n        \"tags\": [],\\n    },\\n    {\\n        \"data\": {\"chunk\": \"olleh\"},\\n        \"event\": \"on_chain_stream\",\\n        \"metadata\": {},\\n        \"name\": \"reverse\",\\n        \"tags\": [],\\n    },\\n    {\\n        \"data\": {\"output\": \"olleh\"},\\n        \"event\": \"on_chain_end\",\\n        \"metadata\": {},\\n        \"name\": \"reverse\",\\n        \"tags\": [],\\n    },\\n]\\n\\n\\nExample: Dispatch Custom Event\\nfrom langchain_core.callbacks.manager import (\\n    adispatch_custom_event,\\n)\\nfrom langchain_core.runnables import RunnableLambda, RunnableConfig\\nimport asyncio\\n\\n\\nasync def slow_thing(some_input: str, config: RunnableConfig) -> str:\\n    \"\"\"Do something that takes a long time.\"\"\"\\n    await asyncio.sleep(1) # Placeholder for some slow operation\\n    await adispatch_custom_event(\\n        \"progress_event\",\\n        {\"message\": \"Finished step 1 of 3\"},\\n        config=config # Must be included for python < 3.10\\n    )\\n    await asyncio.sleep(1) # Placeholder for some slow operation\\n    await adispatch_custom_event(\\n        \"progress_event\",\\n        {\"message\": \"Finished step 2 of 3\"},\\n        config=config # Must be included for python < 3.10\\n    )\\n    await asyncio.sleep(1) # Placeholder for some slow operation\\n    return \"Done\"\\n\\nslow_thing = RunnableLambda(slow_thing)\\n\\nasync for event in slow_thing.astream_events(\"some_input\", version=\"v2\"):\\n    print(event)\\n\\n\\n\\nParameters:\\n\\ninput (Any) â€“ The input to the Runnable.\\nconfig (RunnableConfig | None) â€“ The config to use for the Runnable.\\nversion (Literal[\\'v1\\', \\'v2\\']) â€“ The version of the schema to use either v2 or v1.\\nUsers should use v2.\\nv1 is for backwards compatibility and will be deprecated\\nin 0.4.0.\\nNo default will be assigned until the API is stabilized.\\ncustom events will only be surfaced in v2.\\ninclude_names (Sequence[str] | None) â€“ Only include events from runnables with matching names.\\ninclude_types (Sequence[str] | None) â€“ Only include events from runnables with matching types.\\ninclude_tags (Sequence[str] | None) â€“ Only include events from runnables with matching tags.\\nexclude_names (Sequence[str] | None) â€“ Exclude events from runnables with matching names.\\nexclude_types (Sequence[str] | None) â€“ Exclude events from runnables with matching types.\\nexclude_tags (Sequence[str] | None) â€“ Exclude events from runnables with matching tags.\\nkwargs (Any) â€“ Additional keyword arguments to pass to the Runnable.\\nThese will be passed to astream_log as this implementation\\nof astream_events is built on top of astream_log.\\n\\n\\nYields:\\nAn async stream of StreamEvents.\\n\\nRaises:\\nNotImplementedError â€“ If the version is not v1 or v2.\\n\\nReturn type:\\nAsyncIterator[StandardStreamEvent | CustomStreamEvent]\\n\\n\\n\\n\\n\\nbatch(inputs: list[Input], config: RunnableConfig | list[RunnableConfig] | None = None, *, return_exceptions: bool = False, **kwargs: Any | None) â†’ list[Output]#\\nDefault implementation runs invoke in parallel using a thread pool executor.\\nThe default implementation of batch works well for IO bound runnables.\\nSubclasses should override this method if they can batch more efficiently;\\ne.g., if the underlying Runnable uses an API which supports a batch mode.\\n\\nParameters:\\n\\ninputs (list[Input])\\nconfig (RunnableConfig | list[RunnableConfig] | None)\\nreturn_exceptions (bool)\\nkwargs (Any | None)\\n\\n\\nReturn type:\\nlist[Output]\\n\\n\\n\\n\\n\\nbatch_as_completed(inputs: Sequence[Input], config: RunnableConfig | Sequence[RunnableConfig] | None = None, *, return_exceptions: bool = False, **kwargs: Any | None) â†’ Iterator[tuple[int, Output | Exception]]#\\nRun invoke in parallel on a list of inputs,\\nyielding results as they complete.\\n\\nParameters:\\n\\ninputs (Sequence[Input])\\nconfig (RunnableConfig | Sequence[RunnableConfig] | None)\\nreturn_exceptions (bool)\\nkwargs (Any | None)\\n\\n\\nReturn type:\\nIterator[tuple[int, Output | Exception]]\\n\\n\\n\\n\\n\\nbind(**kwargs: Any) â†’ Runnable[Input, Output]#\\nBind arguments to a Runnable, returning a new Runnable.\\nUseful when a Runnable in a chain requires an argument that is not\\nin the output of the previous Runnable or included in the user input.\\n\\nParameters:\\nkwargs (Any) â€“ The arguments to bind to the Runnable.\\n\\nReturns:\\nA new Runnable with the arguments bound.\\n\\nReturn type:\\nRunnable[Input, Output]\\n\\n\\nExample:\\nfrom langchain_community.chat_models import ChatOllama\\nfrom langchain_core.output_parsers import StrOutputParser\\n\\nllm = ChatOllama(model=\\'llama2\\')\\n\\n# Without bind.\\nchain = (\\n    llm\\n    | StrOutputParser()\\n)\\n\\nchain.invoke(\"Repeat quoted words exactly: \\'One two three four five.\\'\")\\n# Output is \\'One two three four five.\\'\\n\\n# With bind.\\nchain = (\\n    llm.bind(stop=[\"three\"])\\n    | StrOutputParser()\\n)\\n\\nchain.invoke(\"Repeat quoted words exactly: \\'One two three four five.\\'\")\\n# Output is \\'One two\\'\\n\\n\\n\\n\\n\\nconfigurable_alternatives(which: ConfigurableField, *, default_key: str = \\'default\\', prefix_keys: bool = False, **kwargs: Runnable[Input, Output] | Callable[[], Runnable[Input, Output]]) â†’ RunnableSerializable#\\nConfigure alternatives for Runnables that can be set at runtime.\\n\\nParameters:\\n\\nwhich (ConfigurableField) â€“ The ConfigurableField instance that will be used to select the\\nalternative.\\ndefault_key (str) â€“ The default key to use if no alternative is selected.\\nDefaults to â€œdefaultâ€.\\nprefix_keys (bool) â€“ Whether to prefix the keys with the ConfigurableField id.\\nDefaults to False.\\n**kwargs (Runnable[Input, Output] | Callable[[], Runnable[Input, Output]]) â€“ A dictionary of keys to Runnable instances or callables that\\nreturn Runnable instances.\\n\\n\\nReturns:\\nA new Runnable with the alternatives configured.\\n\\nReturn type:\\nRunnableSerializable\\n\\n\\nfrom langchain_anthropic import ChatAnthropic\\nfrom langchain_core.runnables.utils import ConfigurableField\\nfrom langchain_openai import ChatOpenAI\\n\\nmodel = ChatAnthropic(\\n    model_name=\"claude-3-sonnet-20240229\"\\n).configurable_alternatives(\\n    ConfigurableField(id=\"llm\"),\\n    default_key=\"anthropic\",\\n    openai=ChatOpenAI()\\n)\\n\\n# uses the default model ChatAnthropic\\nprint(model.invoke(\"which organization created you?\").content)\\n\\n# uses ChatOpenAI\\nprint(\\n    model.with_config(\\n        configurable={\"llm\": \"openai\"}\\n    ).invoke(\"which organization created you?\").content\\n)\\n\\n\\n\\n\\n\\nconfigurable_fields(**kwargs: ConfigurableField | ConfigurableFieldSingleOption | ConfigurableFieldMultiOption) â†’ RunnableSerializable#\\nConfigure particular Runnable fields at runtime.\\n\\nParameters:\\n**kwargs (ConfigurableField | ConfigurableFieldSingleOption | ConfigurableFieldMultiOption) â€“ A dictionary of ConfigurableField instances to configure.\\n\\nReturns:\\nA new Runnable with the fields configured.\\n\\nReturn type:\\nRunnableSerializable\\n\\n\\nfrom langchain_core.runnables import ConfigurableField\\nfrom langchain_openai import ChatOpenAI\\n\\nmodel = ChatOpenAI(max_tokens=20).configurable_fields(\\n    max_tokens=ConfigurableField(\\n        id=\"output_token_number\",\\n        name=\"Max tokens in the output\",\\n        description=\"The maximum number of tokens in the output\",\\n    )\\n)\\n\\n# max_tokens = 20\\nprint(\\n    \"max_tokens_20: \",\\n    model.invoke(\"tell me something about chess\").content\\n)\\n\\n# max_tokens = 200\\nprint(\"max_tokens_200: \", model.with_config(\\n    configurable={\"output_token_number\": 200}\\n    ).invoke(\"tell me something about chess\").content\\n)\\n\\n\\n\\n\\n\\ncreate_outputs(llm_result: LLMResult) â†’ List[Dict[str, Any]][source]#\\nCreate outputs from response.\\n\\nParameters:\\nllm_result (LLMResult)\\n\\nReturn type:\\nList[Dict[str, Any]]\\n\\n\\n\\n\\n\\nclassmethod from_string(llm: BaseLanguageModel, template: str) â†’ LLMChain[source]#\\nCreate LLMChain from LLM and template.\\n\\nParameters:\\n\\nllm (BaseLanguageModel)\\ntemplate (str)\\n\\n\\nReturn type:\\nLLMChain\\n\\n\\n\\n\\n\\ninvoke(input: Dict[str, Any], config: RunnableConfig | None = None, **kwargs: Any) â†’ Dict[str, Any]#\\nTransform a single input into an output. Override to implement.\\n\\nParameters:\\n\\ninput (Dict[str, Any]) â€“ The input to the Runnable.\\nconfig (RunnableConfig | None) â€“ A config to use when invoking the Runnable.\\nThe config supports standard keys like â€˜tagsâ€™, â€˜metadataâ€™ for tracing\\npurposes, â€˜max_concurrencyâ€™ for controlling how much work to do\\nin parallel, and other keys. Please refer to the RunnableConfig\\nfor more details.\\nkwargs (Any)\\n\\n\\nReturns:\\nThe output of the Runnable.\\n\\nReturn type:\\nDict[str, Any]\\n\\n\\n\\n\\n\\npredict_and_parse(callbacks: list[BaseCallbackHandler] | BaseCallbackManager | None = None, **kwargs: Any) â†’ str | List[str] | Dict[str, Any][source]#\\nCall predict and then parse the results.\\n\\nParameters:\\n\\ncallbacks (list[BaseCallbackHandler] | BaseCallbackManager | None)\\nkwargs (Any)\\n\\n\\nReturn type:\\nstr | List[str] | Dict[str, Any]\\n\\n\\n\\n\\n\\nprep_inputs(inputs: Dict[str, Any] | Any) â†’ Dict[str, str]#\\nPrepare chain inputs, including adding inputs from memory.\\n\\nParameters:\\ninputs (Dict[str, Any] | Any) â€“ Dictionary of raw inputs, or single input if chain expects\\nonly one param. Should contain all inputs specified in\\nChain.input_keys except for inputs that will be set by the chainâ€™s\\nmemory.\\n\\nReturns:\\nA dictionary of all inputs, including those added by the chainâ€™s memory.\\n\\nReturn type:\\nDict[str, str]\\n\\n\\n\\n\\n\\nprep_outputs(inputs: Dict[str, str], outputs: Dict[str, str], return_only_outputs: bool = False) â†’ Dict[str, str]#\\nValidate and prepare chain outputs, and save info about this run to memory.\\n\\nParameters:\\n\\ninputs (Dict[str, str]) â€“ Dictionary of chain inputs, including any inputs added by chain\\nmemory.\\noutputs (Dict[str, str]) â€“ Dictionary of initial chain outputs.\\nreturn_only_outputs (bool) â€“ Whether to only return the chain outputs. If False,\\ninputs are also added to the final outputs.\\n\\n\\nReturns:\\nA dict of the final chain outputs.\\n\\nReturn type:\\nDict[str, str]\\n\\n\\n\\n\\n\\nprep_prompts(input_list: List[Dict[str, Any]], run_manager: CallbackManagerForChainRun | None = None) â†’ Tuple[List[PromptValue], List[str] | None][source]#\\nPrepare prompts from inputs.\\n\\nParameters:\\n\\ninput_list (List[Dict[str, Any]])\\nrun_manager (CallbackManagerForChainRun | None)\\n\\n\\nReturn type:\\nTuple[List[PromptValue], List[str] | None]\\n\\n\\n\\n\\n\\nrun(*args: Any, callbacks: list[BaseCallbackHandler] | BaseCallbackManager | None = None, tags: List[str] | None = None, metadata: Dict[str, Any] | None = None, **kwargs: Any) â†’ Any#\\n\\nDeprecated since version 0.1.0: Use invoke() instead. It will not be removed until langchain==1.0.\\n\\nConvenience method for executing chain.\\nThe main difference between this method and Chain.__call__ is that this\\nmethod expects inputs to be passed directly in as positional arguments or\\nkeyword arguments, whereas Chain.__call__ expects a single input dictionary\\nwith all the inputs\\n\\nParameters:\\n\\n*args (Any) â€“ If the chain expects a single input, it can be passed in as the\\nsole positional argument.\\ncallbacks (list[BaseCallbackHandler] | BaseCallbackManager | None) â€“ Callbacks to use for this chain run. These will be called in\\naddition to callbacks passed to the chain during construction, but only\\nthese runtime callbacks will propagate to calls to other objects.\\ntags (List[str] | None) â€“ List of string tags to pass to all callbacks. These will be passed in\\naddition to tags passed to the chain during construction, but only\\nthese runtime tags will propagate to calls to other objects.\\n**kwargs (Any) â€“ If the chain expects multiple inputs, they can be passed in\\ndirectly as keyword arguments.\\nmetadata (Dict[str, Any] | None)\\n**kwargs\\n\\n\\nReturns:\\nThe chain output.\\n\\nReturn type:\\nAny\\n\\n\\nExample\\n# Suppose we have a single-input chain that takes a \\'question\\' string:\\nchain.run(\"What\\'s the temperature in Boise, Idaho?\")\\n# -> \"The temperature in Boise is...\"\\n\\n# Suppose we have a multi-input chain that takes a \\'question\\' string\\n# and \\'context\\' string:\\nquestion = \"What\\'s the temperature in Boise, Idaho?\"\\ncontext = \"Weather report for Boise, Idaho on 07/03/23...\"\\nchain.run(question=question, context=context)\\n# -> \"The temperature in Boise is...\"\\n\\n\\n\\n\\n\\nsave(file_path: Path | str) â†’ None#\\nSave the chain.\\n\\nExpects Chain._chain_type property to be implemented and for memory to benull.\\n\\n\\n\\nParameters:\\nfile_path (Path | str) â€“ Path to file to save the chain to.\\n\\nReturn type:\\nNone\\n\\n\\nExample\\nchain.save(file_path=\"path/chain.yaml\")\\n\\n\\n\\n\\n\\nstream(input: Input, config: RunnableConfig | None = None, **kwargs: Any | None) â†’ Iterator[Output]#\\nDefault implementation of stream, which calls invoke.\\nSubclasses should override this method if they support streaming output.\\n\\nParameters:\\n\\ninput (Input) â€“ The input to the Runnable.\\nconfig (RunnableConfig | None) â€“ The config to use for the Runnable. Defaults to None.\\nkwargs (Any | None) â€“ Additional keyword arguments to pass to the Runnable.\\n\\n\\nYields:\\nThe output of the Runnable.\\n\\nReturn type:\\nIterator[Output]\\n\\n\\n\\n\\n\\nwith_alisteners(*, on_start: AsyncListener | None = None, on_end: AsyncListener | None = None, on_error: AsyncListener | None = None) â†’ Runnable[Input, Output]#\\nBind async lifecycle listeners to a Runnable, returning a new Runnable.\\non_start: Asynchronously called before the Runnable starts running.\\non_end: Asynchronously called after the Runnable finishes running.\\non_error: Asynchronously called if the Runnable throws an error.\\nThe Run object contains information about the run, including its id,\\ntype, input, output, error, start_time, end_time, and any tags or metadata\\nadded to the run.\\n\\nParameters:\\n\\non_start (Optional[AsyncListener]) â€“ Asynchronously called before the Runnable starts running.\\nDefaults to None.\\non_end (Optional[AsyncListener]) â€“ Asynchronously called after the Runnable finishes running.\\nDefaults to None.\\non_error (Optional[AsyncListener]) â€“ Asynchronously called if the Runnable throws an error.\\nDefaults to None.\\n\\n\\nReturns:\\nA new Runnable with the listeners bound.\\n\\nReturn type:\\nRunnable[Input, Output]\\n\\n\\nExample:\\nfrom langchain_core.runnables import RunnableLambda\\nimport time\\n\\nasync def test_runnable(time_to_sleep : int):\\n    print(f\"Runnable[{time_to_sleep}s]: starts at {format_t(time.time())}\")\\n    await asyncio.sleep(time_to_sleep)\\n    print(f\"Runnable[{time_to_sleep}s]: ends at {format_t(time.time())}\")\\n\\nasync def fn_start(run_obj : Runnable):\\n    print(f\"on start callback starts at {format_t(time.time())}\\n    await asyncio.sleep(3)\\n    print(f\"on start callback ends at {format_t(time.time())}\")\\n\\nasync def fn_end(run_obj : Runnable):\\n    print(f\"on end callback starts at {format_t(time.time())}\\n    await asyncio.sleep(2)\\n    print(f\"on end callback ends at {format_t(time.time())}\")\\n\\nrunnable = RunnableLambda(test_runnable).with_alisteners(\\n    on_start=fn_start,\\n    on_end=fn_end\\n)\\nasync def concurrent_runs():\\n    await asyncio.gather(runnable.ainvoke(2), runnable.ainvoke(3))\\n\\nasyncio.run(concurrent_runs())\\nResult:\\non start callback starts at 2024-05-16T14:20:29.637053+00:00\\non start callback starts at 2024-05-16T14:20:29.637150+00:00\\non start callback ends at 2024-05-16T14:20:32.638305+00:00\\non start callback ends at 2024-05-16T14:20:32.638383+00:00\\nRunnable[3s]: starts at 2024-05-16T14:20:32.638849+00:00\\nRunnable[5s]: starts at 2024-05-16T14:20:32.638999+00:00\\nRunnable[3s]: ends at 2024-05-16T14:20:35.640016+00:00\\non end callback starts at 2024-05-16T14:20:35.640534+00:00\\nRunnable[5s]: ends at 2024-05-16T14:20:37.640169+00:00\\non end callback starts at 2024-05-16T14:20:37.640574+00:00\\non end callback ends at 2024-05-16T14:20:37.640654+00:00\\non end callback ends at 2024-05-16T14:20:39.641751+00:00\\n\\n\\n\\n\\n\\nwith_config(config: RunnableConfig | None = None, **kwargs: Any) â†’ Runnable[Input, Output]#\\nBind config to a Runnable, returning a new Runnable.\\n\\nParameters:\\n\\nconfig (RunnableConfig | None) â€“ The config to bind to the Runnable.\\nkwargs (Any) â€“ Additional keyword arguments to pass to the Runnable.\\n\\n\\nReturns:\\nA new Runnable with the config bound.\\n\\nReturn type:\\nRunnable[Input, Output]\\n\\n\\n\\n\\n\\nwith_fallbacks(fallbacks: Sequence[Runnable[Input, Output]], *, exceptions_to_handle: tuple[type[BaseException], ...] = (<class \\'Exception\\'>,), exception_key: Optional[str] = None) â†’ RunnableWithFallbacksT[Input, Output]#\\nAdd fallbacks to a Runnable, returning a new Runnable.\\nThe new Runnable will try the original Runnable, and then each fallback\\nin order, upon failures.\\n\\nParameters:\\n\\nfallbacks (Sequence[Runnable[Input, Output]]) â€“ A sequence of runnables to try if the original Runnable fails.\\nexceptions_to_handle (tuple[type[BaseException], ...]) â€“ A tuple of exception types to handle.\\nDefaults to (Exception,).\\nexception_key (Optional[str]) â€“ If string is specified then handled exceptions will be passed\\nto fallbacks as part of the input under the specified key. If None,\\nexceptions will not be passed to fallbacks. If used, the base Runnable\\nand its fallbacks must accept a dictionary as input. Defaults to None.\\n\\n\\nReturns:\\nA new Runnable that will try the original Runnable, and then each\\nfallback in order, upon failures.\\n\\nReturn type:\\nRunnableWithFallbacksT[Input, Output]\\n\\n\\nExample\\nfrom typing import Iterator\\n\\nfrom langchain_core.runnables import RunnableGenerator\\n\\n\\ndef _generate_immediate_error(input: Iterator) -> Iterator[str]:\\n    raise ValueError()\\n    yield \"\"\\n\\n\\ndef _generate(input: Iterator) -> Iterator[str]:\\n    yield from \"foo bar\"\\n\\n\\nrunnable = RunnableGenerator(_generate_immediate_error).with_fallbacks(\\n    [RunnableGenerator(_generate)]\\n    )\\nprint(\\'\\'.join(runnable.stream({}))) #foo bar\\n\\n\\n\\nParameters:\\n\\nfallbacks (Sequence[Runnable[Input, Output]]) â€“ A sequence of runnables to try if the original Runnable fails.\\nexceptions_to_handle (tuple[type[BaseException], ...]) â€“ A tuple of exception types to handle.\\nexception_key (Optional[str]) â€“ If string is specified then handled exceptions will be passed\\nto fallbacks as part of the input under the specified key. If None,\\nexceptions will not be passed to fallbacks. If used, the base Runnable\\nand its fallbacks must accept a dictionary as input.\\n\\n\\nReturns:\\nA new Runnable that will try the original Runnable, and then each\\nfallback in order, upon failures.\\n\\nReturn type:\\nRunnableWithFallbacksT[Input, Output]\\n\\n\\n\\n\\n\\nwith_listeners(*, on_start: Callable[[Run], None] | Callable[[Run, RunnableConfig], None] | None = None, on_end: Callable[[Run], None] | Callable[[Run, RunnableConfig], None] | None = None, on_error: Callable[[Run], None] | Callable[[Run, RunnableConfig], None] | None = None) â†’ Runnable[Input, Output]#\\nBind lifecycle listeners to a Runnable, returning a new Runnable.\\non_start: Called before the Runnable starts running, with the Run object.\\non_end: Called after the Runnable finishes running, with the Run object.\\non_error: Called if the Runnable throws an error, with the Run object.\\nThe Run object contains information about the run, including its id,\\ntype, input, output, error, start_time, end_time, and any tags or metadata\\nadded to the run.\\n\\nParameters:\\n\\non_start (Optional[Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]]) â€“ Called before the Runnable starts running. Defaults to None.\\non_end (Optional[Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]]) â€“ Called after the Runnable finishes running. Defaults to None.\\non_error (Optional[Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]]) â€“ Called if the Runnable throws an error. Defaults to None.\\n\\n\\nReturns:\\nA new Runnable with the listeners bound.\\n\\nReturn type:\\nRunnable[Input, Output]\\n\\n\\nExample:\\nfrom langchain_core.runnables import RunnableLambda\\nfrom langchain_core.tracers.schemas import Run\\n\\nimport time\\n\\ndef test_runnable(time_to_sleep : int):\\n    time.sleep(time_to_sleep)\\n\\ndef fn_start(run_obj: Run):\\n    print(\"start_time:\", run_obj.start_time)\\n\\ndef fn_end(run_obj: Run):\\n    print(\"end_time:\", run_obj.end_time)\\n\\nchain = RunnableLambda(test_runnable).with_listeners(\\n    on_start=fn_start,\\n    on_end=fn_end\\n)\\nchain.invoke(2)\\n\\n\\n\\n\\n\\nwith_retry(*, retry_if_exception_type: tuple[type[BaseException], ...] = (<class \\'Exception\\'>,), wait_exponential_jitter: bool = True, stop_after_attempt: int = 3) â†’ Runnable[Input, Output]#\\nCreate a new Runnable that retries the original Runnable on exceptions.\\n\\nParameters:\\n\\nretry_if_exception_type (tuple[type[BaseException], ...]) â€“ A tuple of exception types to retry on.\\nDefaults to (Exception,).\\nwait_exponential_jitter (bool) â€“ Whether to add jitter to the wait\\ntime between retries. Defaults to True.\\nstop_after_attempt (int) â€“ The maximum number of attempts to make before\\ngiving up. Defaults to 3.\\n\\n\\nReturns:\\nA new Runnable that retries the original Runnable on exceptions.\\n\\nReturn type:\\nRunnable[Input, Output]\\n\\n\\nExample:\\nfrom langchain_core.runnables import RunnableLambda\\n\\ncount = 0\\n\\n\\ndef _lambda(x: int) -> None:\\n    global count\\n    count = count + 1\\n    if x == 1:\\n        raise ValueError(\"x is 1\")\\n    else:\\n         pass\\n\\n\\nrunnable = RunnableLambda(_lambda)\\ntry:\\n    runnable.with_retry(\\n        stop_after_attempt=2,\\n        retry_if_exception_type=(ValueError,),\\n    ).invoke(1)\\nexcept ValueError:\\n    pass\\n\\nassert (count == 2)\\n\\n\\n\\nParameters:\\n\\nretry_if_exception_type (tuple[type[BaseException], ...]) â€“ A tuple of exception types to retry on\\nwait_exponential_jitter (bool) â€“ Whether to add jitter to the wait time\\nbetween retries\\nstop_after_attempt (int) â€“ The maximum number of attempts to make before giving up\\n\\n\\nReturns:\\nA new Runnable that retries the original Runnable on exceptions.\\n\\nReturn type:\\nRunnable[Input, Output]\\n\\n\\n\\n\\n\\nwith_types(*, input_type: type[Input] | None = None, output_type: type[Output] | None = None) â†’ Runnable[Input, Output]#\\nBind input and output types to a Runnable, returning a new Runnable.\\n\\nParameters:\\n\\ninput_type (type[Input] | None) â€“ The input type to bind to the Runnable. Defaults to None.\\noutput_type (type[Output] | None) â€“ The output type to bind to the Runnable. Defaults to None.\\n\\n\\nReturns:\\nA new Runnable with the types bound.\\n\\nReturn type:\\nRunnable[Input, Output]\\n\\n\\n\\n\\nExamples using LLMChain\\n\\n# Basic example (short documents)\\n# Example\\n# Legacy\\nAim\\nAlibaba Cloud PAI EAS\\nAnyscale\\nAphrodite Engine\\nArgilla\\nAzure ML\\nBanana\\nBaseten\\nBittensor\\nC Transformers\\nCTranslate2\\nCerebriumAI\\nChatGLM\\nClarifai\\nCloudflare Workers AI\\nComet\\nContext\\nDall-E Image Generator\\nDeepInfra\\nEden AI\\nFlyte\\nForefrontAI\\nGigaChat\\nGoogle Cloud Vertex AI Reranker\\nGooseAI\\nGradient\\nHuggingface Endpoints\\nIPEX-LLM\\nJavelin AI Gateway\\nJavelin AI Gateway Tutorial\\nLlama2Chat\\nMLflow AI Gateway\\nMLflow Deployments for LLMs\\nMemorize\\nMinimax\\nModal\\nMosaicML\\nMotÃ¶rhead\\nNLP Cloud\\nNebula (Symbl.ai)\\nOctoAI\\nOpaquePrompts\\nOpenLLM\\nOpenLM\\nPetals\\nPredibase\\nPrediction Guard\\nRay Serve\\nRePhraseQuery\\nRebuff\\nReddit Search \\nReplicate\\nRunhouse\\nSageMaker Tracking\\nSolar\\nStochasticAI\\nSummarize Text\\nTextGen\\nWeights & Biases\\nWriter\\nXorbits Inference (Xinference)\\nYandexGPT\\nYellowbrick\\nYuan2.0\\nZapier Natural Language Actions\\nvLLM\\n\\n\\n\\n\\n\\n\\n\\n\\n On this page\\n  \\n\\n\\nLLMChain\\ncallback_manager\\ncallbacks\\nllm\\nllm_kwargs\\nmemory\\nmetadata\\noutput_parser\\nprompt\\nreturn_final_only\\ntags\\nverbose\\n__call__()\\naapply()\\naapply_and_parse()\\nabatch()\\nabatch_as_completed()\\nacall()\\nainvoke()\\napply()\\napply_and_parse()\\napredict_and_parse()\\naprep_inputs()\\naprep_outputs()\\naprep_prompts()\\narun()\\nastream()\\nastream_events()\\nbatch()\\nbatch_as_completed()\\nbind()\\nconfigurable_alternatives()\\nconfigurable_fields()\\ncreate_outputs()\\nfrom_string()\\ninvoke()\\npredict_and_parse()\\nprep_inputs()\\nprep_outputs()\\nprep_prompts()\\nrun()\\nsave()\\nstream()\\nwith_alisteners()\\nwith_config()\\nwith_fallbacks()\\nwith_listeners()\\nwith_retry()\\nwith_types()\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    \\n      Â© Copyright 2023, LangChain Inc.\\n      \\n\\n\\n\\n\\n\\n\\n')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://python.langchain.com/api_reference/langchain/chains/langchain.chains.llm.LLMChain.html', 'title': 'LLMChain â€” ðŸ¦œðŸ”— LangChain  documentation', 'language': 'en'}, page_content='LLMChain â€” ðŸ¦œðŸ”— LangChain  documentation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to main content\\n\\n\\nBack to top\\n\\n\\n\\n\\nCtrl+K\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Reference\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCtrl+K\\n\\n\\n\\n\\n\\n\\n\\nDocs\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nGitHub\\n\\n\\n\\nX / Twitter\\n\\n\\n\\n\\n\\n\\n\\n\\nCtrl+K\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Reference\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDocs\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nGitHub\\n\\n\\n\\nX / Twitter\\n\\n\\n\\n\\n\\n\\n\\nSection Navigation\\nBase packages'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/langchain/chains/langchain.chains.llm.LLMChain.html', 'title': 'LLMChain â€” ðŸ¦œðŸ”— LangChain  documentation', 'language': 'en'}, page_content='Core\\nLangchain\\nagents\\ncallbacks\\nchains\\nChain\\nBaseCombineDocumentsChain\\nAsyncCombineDocsProtocol\\nCombineDocsProtocol\\nConstitutionalPrinciple\\nBaseConversationalRetrievalChain\\nChatVectorDBChain\\nInputType\\nElasticsearchDatabaseChain\\nFlareChain\\nQuestionGeneratorChain\\nFinishedOutputParser\\nHypotheticalDocumentEmbedder\\nOpenAIModerationChain\\nCrawler\\nElementInViewPort\\nFactWithEvidence\\nQuestionAnswer\\nSimpleRequestChain\\nAnswerWithSources\\nBasePromptSelector\\nConditionalPromptSelector\\nLoadingCallable\\nRetrievalQAWithSourcesChain\\nVectorDBQAWithSourcesChain\\nStructuredQueryOutputParser\\nISO8601Date\\nISO8601DateTime\\nAttributeInfo\\nLoadingCallable\\nMultiRouteChain\\nRoute\\nRouterChain\\nEmbeddingRouterChain\\nRouterOutputParser\\nMultiRetrievalQAChain\\nSequentialChain\\nSimpleSequentialChain\\nSQLInput\\nSQLInputWithTables\\nLoadingCallable\\nTransformChain\\nacollapse_docs\\ncollapse_docs\\nsplit_list_of_docs\\ncreate_stuff_documents_chain\\ngenerate_example\\ncreate_history_aware_retriever\\ncreate_citation_fuzzy_match_runnable'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/langchain/chains/langchain.chains.llm.LLMChain.html', 'title': 'LLMChain â€” ðŸ¦œðŸ”— LangChain  documentation', 'language': 'en'}, page_content='LoadingCallable\\nTransformChain\\nacollapse_docs\\ncollapse_docs\\nsplit_list_of_docs\\ncreate_stuff_documents_chain\\ngenerate_example\\ncreate_history_aware_retriever\\ncreate_citation_fuzzy_match_runnable\\nopenapi_spec_to_openai_fn\\nget_llm_kwargs\\nis_chat_model\\nis_llm\\nconstruct_examples\\nfix_filter_directive\\nget_query_constructor_prompt\\nload_query_constructor_runnable\\nget_parser\\nv_args\\ncreate_retrieval_chain\\ncreate_sql_query_chain\\nget_openai_output_parser\\nload_summarize_chain\\nAPIChain\\nAnalyzeDocumentChain\\nMapReduceDocumentsChain\\nMapRerankDocumentsChain\\nReduceDocumentsChain\\nRefineDocumentsChain\\nStuffDocumentsChain\\nConstitutionalChain\\nConversationChain\\nConversationalRetrievalChain\\nLLMChain\\nLLMCheckerChain\\nLLMMathChain\\nLLMSummarizationCheckerChain\\nMapReduceChain\\nNatBotChain\\nQAGenerationChain\\nBaseQAWithSourcesChain\\nQAWithSourcesChain\\nBaseRetrievalQA\\nRetrievalQA\\nVectorDBQA\\nLLMRouterChain\\nMultiPromptChain\\nload_chain\\nload_chain_from_config\\ncreate_openai_fn_chain\\ncreate_structured_output_chain'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/langchain/chains/langchain.chains.llm.LLMChain.html', 'title': 'LLMChain â€” ðŸ¦œðŸ”— LangChain  documentation', 'language': 'en'}, page_content='QAWithSourcesChain\\nBaseRetrievalQA\\nRetrievalQA\\nVectorDBQA\\nLLMRouterChain\\nMultiPromptChain\\nload_chain\\nload_chain_from_config\\ncreate_openai_fn_chain\\ncreate_structured_output_chain\\ncreate_citation_fuzzy_match_chain\\ncreate_extraction_chain\\ncreate_extraction_chain_pydantic\\nget_openapi_chain\\ncreate_qa_with_sources_chain\\ncreate_qa_with_structure_chain\\ncreate_tagging_chain\\ncreate_tagging_chain_pydantic\\ncreate_extraction_chain_pydantic\\nload_qa_with_sources_chain\\nload_query_constructor_chain\\nload_qa_chain\\ncreate_openai_fn_runnable\\ncreate_structured_output_runnable'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/langchain/chains/langchain.chains.llm.LLMChain.html', 'title': 'LLMChain â€” ðŸ¦œðŸ”— LangChain  documentation', 'language': 'en'}, page_content='chat_models\\nembeddings\\nevaluation\\nglobals\\nhub\\nindexes\\nmemory\\nmodel_laboratory\\noutput_parsers\\nretrievers\\nrunnables\\nsmith\\nstorage\\n\\n\\nText Splitters\\nCommunity\\nExperimental\\n\\nIntegrations\\n\\nAI21\\nAnthropic\\nAstraDB\\nAWS\\nAzure Dynamic Sessions\\nCerebras\\nChroma\\nCohere\\nDeepseek\\nElasticsearch\\nExa\\nFireworks\\nGoogle Community\\nGoogle GenAI\\nGoogle VertexAI\\nGroq\\nHuggingface\\nIBM\\nMilvus\\nMistralAI\\nNeo4J\\nNomic\\nNvidia Ai Endpoints\\nOllama\\nOpenAI\\nPinecone\\nPostgres\\nPrompty\\nQdrant\\nRedis\\nSema4\\nSnowflake\\nSqlserver\\nStandard Tests\\nTogether\\nUnstructured\\nUpstage\\nVoyageAI\\nWeaviate\\nXAI\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLangChain Python API Reference\\nlangchain: 0.3.18\\nchains\\nLLMChain\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLLMChain#\\n\\n\\nclass langchain.chains.llm.LLMChain[source]#\\nBases: Chain\\n\\nDeprecated since version 0.1.17: Use , `prompt | llm`() instead. It will not be removed until langchain==1.0.\\n\\nChain to run queries against LLMs.\\nThis class is deprecated. See below for an example implementation using\\nLangChain runnables:'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/langchain/chains/langchain.chains.llm.LLMChain.html', 'title': 'LLMChain â€” ðŸ¦œðŸ”— LangChain  documentation', 'language': 'en'}, page_content='Chain to run queries against LLMs.\\nThis class is deprecated. See below for an example implementation using\\nLangChain runnables:\\n\\nfrom langchain_core.output_parsers import StrOutputParser\\nfrom langchain_core.prompts import PromptTemplate\\nfrom langchain_openai import OpenAI\\n\\nprompt_template = \"Tell me a {adjective} joke\"\\nprompt = PromptTemplate(\\n    input_variables=[\"adjective\"], template=prompt_template\\n)\\nllm = OpenAI()\\nchain = prompt | llm | StrOutputParser()\\n\\nchain.invoke(\"your adjective here\")\\n\\n\\n\\nExample\\nfrom langchain.chains import LLMChain\\nfrom langchain_community.llms import OpenAI\\nfrom langchain_core.prompts import PromptTemplate\\nprompt_template = \"Tell me a {adjective} joke\"\\nprompt = PromptTemplate(\\n    input_variables=[\"adjective\"], template=prompt_template\\n)\\nllm = LLMChain(llm=OpenAI(), prompt=prompt)'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/langchain/chains/langchain.chains.llm.LLMChain.html', 'title': 'LLMChain â€” ðŸ¦œðŸ”— LangChain  documentation', 'language': 'en'}, page_content='Note\\nLLMChain implements the standard Runnable Interface. ðŸƒ\\nThe Runnable Interface has additional methods that are available on runnables, such as with_types, with_retry, assign, bind, get_graph, and more.\\n\\n\\n\\nparam callback_manager: BaseCallbackManager | None = None#\\n[DEPRECATED] Use callbacks instead.\\n\\n\\n\\nparam callbacks: Callbacks = None#\\nOptional list of callback handlers (or callback manager). Defaults to None.\\nCallback handlers are called throughout the lifecycle of a call to a chain,\\nstarting with on_chain_start, ending with on_chain_end or on_chain_error.\\nEach custom chain can optionally call additional callback methods, see Callback docs\\nfor full details.\\n\\n\\n\\nparam llm: Runnable[LanguageModelInput, str] | Runnable[LanguageModelInput, BaseMessage] [Required]#\\nLanguage model to call.\\n\\n\\n\\nparam llm_kwargs: dict [Optional]#'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/langchain/chains/langchain.chains.llm.LLMChain.html', 'title': 'LLMChain â€” ðŸ¦œðŸ”— LangChain  documentation', 'language': 'en'}, page_content='param llm: Runnable[LanguageModelInput, str] | Runnable[LanguageModelInput, BaseMessage] [Required]#\\nLanguage model to call.\\n\\n\\n\\nparam llm_kwargs: dict [Optional]#\\n\\n\\n\\nparam memory: BaseMemory | None = None#\\nOptional memory object. Defaults to None.\\nMemory is a class that gets called at the start\\nand at the end of every chain. At the start, memory loads variables and passes\\nthem along in the chain. At the end, it saves any returned variables.\\nThere are many different types of memory - please see memory docs\\nfor the full catalog.\\n\\n\\n\\nparam metadata: Dict[str, Any] | None = None#\\nOptional metadata associated with the chain. Defaults to None.\\nThis metadata will be associated with each call to this chain,\\nand passed as arguments to the handlers defined in callbacks.\\nYou can use these to eg identify a specific instance of a chain with its use case.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/langchain/chains/langchain.chains.llm.LLMChain.html', 'title': 'LLMChain â€” ðŸ¦œðŸ”— LangChain  documentation', 'language': 'en'}, page_content='param output_parser: BaseLLMOutputParser [Optional]#\\nOutput parser to use.\\nDefaults to one that takes the most likely string but does not change it\\notherwise.\\n\\n\\n\\nparam prompt: BasePromptTemplate [Required]#\\nPrompt object to use.\\n\\n\\n\\nparam return_final_only: bool = True#\\nWhether to return only the final parsed result. Defaults to True.\\nIf false, will return a bunch of extra information about the generation.\\n\\n\\n\\nparam tags: List[str] | None = None#\\nOptional list of tags associated with the chain. Defaults to None.\\nThese tags will be associated with each call to this chain,\\nand passed as arguments to the handlers defined in callbacks.\\nYou can use these to eg identify a specific instance of a chain with its use case.\\n\\n\\n\\nparam verbose: bool [Optional]#\\nWhether or not run in verbose mode. In verbose mode, some intermediate logs\\nwill be printed to the console. Defaults to the global verbose value,\\naccessible via langchain.globals.get_verbose().'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/langchain/chains/langchain.chains.llm.LLMChain.html', 'title': 'LLMChain â€” ðŸ¦œðŸ”— LangChain  documentation', 'language': 'en'}, page_content='__call__(inputs: Dict[str, Any] | Any, return_only_outputs: bool = False, callbacks: list[BaseCallbackHandler] | BaseCallbackManager | None = None, *, tags: List[str] | None = None, metadata: Dict[str, Any] | None = None, run_name: str | None = None, include_run_info: bool = False) â†’ Dict[str, Any]#\\n\\nDeprecated since version 0.1.0: Use invoke() instead. It will not be removed until langchain==1.0.\\n\\nExecute the chain.\\n\\nParameters:'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/langchain/chains/langchain.chains.llm.LLMChain.html', 'title': 'LLMChain â€” ðŸ¦œðŸ”— LangChain  documentation', 'language': 'en'}, page_content='inputs (Dict[str, Any] | Any) â€“ Dictionary of inputs, or single input if chain expects\\nonly one param. Should contain all inputs specified in\\nChain.input_keys except for inputs that will be set by the chainâ€™s\\nmemory.\\nreturn_only_outputs (bool) â€“ Whether to return only outputs in the\\nresponse. If True, only new keys generated by this chain will be\\nreturned. If False, both input keys and new keys generated by this\\nchain will be returned. Defaults to False.\\ncallbacks (list[BaseCallbackHandler] | BaseCallbackManager | None) â€“ Callbacks to use for this chain run. These will be called in\\naddition to callbacks passed to the chain during construction, but only\\nthese runtime callbacks will propagate to calls to other objects.\\ntags (List[str] | None) â€“ List of string tags to pass to all callbacks. These will be passed in\\naddition to tags passed to the chain during construction, but only\\nthese runtime tags will propagate to calls to other objects.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/langchain/chains/langchain.chains.llm.LLMChain.html', 'title': 'LLMChain â€” ðŸ¦œðŸ”— LangChain  documentation', 'language': 'en'}, page_content='addition to tags passed to the chain during construction, but only\\nthese runtime tags will propagate to calls to other objects.\\nmetadata (Dict[str, Any] | None) â€“ Optional metadata associated with the chain. Defaults to None\\ninclude_run_info (bool) â€“ Whether to include run info in the response. Defaults\\nto False.\\nrun_name (str | None)'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/langchain/chains/langchain.chains.llm.LLMChain.html', 'title': 'LLMChain â€” ðŸ¦œðŸ”— LangChain  documentation', 'language': 'en'}, page_content='Returns:\\n\\nA dict of named outputs. Should contain all outputs specified inChain.output_keys.\\n\\n\\n\\n\\nReturn type:\\nDict[str, Any]\\n\\n\\n\\n\\n\\nasync aapply(input_list: List[Dict[str, Any]], callbacks: list[BaseCallbackHandler] | BaseCallbackManager | None = None) â†’ List[Dict[str, str]][source]#\\nUtilize the LLM generate method for speed gains.\\n\\nParameters:\\n\\ninput_list (List[Dict[str, Any]])\\ncallbacks (list[BaseCallbackHandler] | BaseCallbackManager | None)\\n\\n\\nReturn type:\\nList[Dict[str, str]]\\n\\n\\n\\n\\n\\nasync aapply_and_parse(input_list: List[Dict[str, Any]], callbacks: list[BaseCallbackHandler] | BaseCallbackManager | None = None) â†’ Sequence[str | List[str] | Dict[str, str]][source]#\\nCall apply and then parse the results.\\n\\nParameters:\\n\\ninput_list (List[Dict[str, Any]])\\ncallbacks (list[BaseCallbackHandler] | BaseCallbackManager | None)\\n\\n\\nReturn type:\\nSequence[str | List[str] | Dict[str, str]]'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/langchain/chains/langchain.chains.llm.LLMChain.html', 'title': 'LLMChain â€” ðŸ¦œðŸ”— LangChain  documentation', 'language': 'en'}, page_content='Parameters:\\n\\ninput_list (List[Dict[str, Any]])\\ncallbacks (list[BaseCallbackHandler] | BaseCallbackManager | None)\\n\\n\\nReturn type:\\nSequence[str | List[str] | Dict[str, str]]\\n\\n\\n\\n\\n\\nasync abatch(inputs: list[Input], config: RunnableConfig | list[RunnableConfig] | None = None, *, return_exceptions: bool = False, **kwargs: Any | None) â†’ list[Output]#\\nDefault implementation runs ainvoke in parallel using asyncio.gather.\\nThe default implementation of batch works well for IO bound runnables.\\nSubclasses should override this method if they can batch more efficiently;\\ne.g., if the underlying Runnable uses an API which supports a batch mode.\\n\\nParameters:'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/langchain/chains/langchain.chains.llm.LLMChain.html', 'title': 'LLMChain â€” ðŸ¦œðŸ”— LangChain  documentation', 'language': 'en'}, page_content='Parameters:\\n\\ninputs (list[Input]) â€“ A list of inputs to the Runnable.\\nconfig (RunnableConfig | list[RunnableConfig] | None) â€“ A config to use when invoking the Runnable.\\nThe config supports standard keys like â€˜tagsâ€™, â€˜metadataâ€™ for tracing\\npurposes, â€˜max_concurrencyâ€™ for controlling how much work to do\\nin parallel, and other keys. Please refer to the RunnableConfig\\nfor more details. Defaults to None.\\nreturn_exceptions (bool) â€“ Whether to return exceptions instead of raising them.\\nDefaults to False.\\nkwargs (Any | None) â€“ Additional keyword arguments to pass to the Runnable.\\n\\n\\nReturns:\\nA list of outputs from the Runnable.\\n\\nReturn type:\\nlist[Output]\\n\\n\\n\\n\\n\\nasync abatch_as_completed(inputs: Sequence[Input], config: RunnableConfig | Sequence[RunnableConfig] | None = None, *, return_exceptions: bool = False, **kwargs: Any | None) â†’ AsyncIterator[tuple[int, Output | Exception]]#\\nRun ainvoke in parallel on a list of inputs,\\nyielding results as they complete.\\n\\nParameters:'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/langchain/chains/langchain.chains.llm.LLMChain.html', 'title': 'LLMChain â€” ðŸ¦œðŸ”— LangChain  documentation', 'language': 'en'}, page_content='Parameters:\\n\\ninputs (Sequence[Input]) â€“ A list of inputs to the Runnable.\\nconfig (RunnableConfig | Sequence[RunnableConfig] | None) â€“ A config to use when invoking the Runnable.\\nThe config supports standard keys like â€˜tagsâ€™, â€˜metadataâ€™ for tracing\\npurposes, â€˜max_concurrencyâ€™ for controlling how much work to do\\nin parallel, and other keys. Please refer to the RunnableConfig\\nfor more details. Defaults to None. Defaults to None.\\nreturn_exceptions (bool) â€“ Whether to return exceptions instead of raising them.\\nDefaults to False.\\nkwargs (Any | None) â€“ Additional keyword arguments to pass to the Runnable.\\n\\n\\nYields:\\nA tuple of the index of the input and the output from the Runnable.\\n\\nReturn type:\\nAsyncIterator[tuple[int, Output | Exception]]'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/langchain/chains/langchain.chains.llm.LLMChain.html', 'title': 'LLMChain â€” ðŸ¦œðŸ”— LangChain  documentation', 'language': 'en'}, page_content='Yields:\\nA tuple of the index of the input and the output from the Runnable.\\n\\nReturn type:\\nAsyncIterator[tuple[int, Output | Exception]]\\n\\n\\n\\n\\n\\nasync acall(inputs: Dict[str, Any] | Any, return_only_outputs: bool = False, callbacks: list[BaseCallbackHandler] | BaseCallbackManager | None = None, *, tags: List[str] | None = None, metadata: Dict[str, Any] | None = None, run_name: str | None = None, include_run_info: bool = False) â†’ Dict[str, Any]#\\n\\nDeprecated since version 0.1.0: Use ainvoke() instead. It will not be removed until langchain==1.0.\\n\\nAsynchronously execute the chain.\\n\\nParameters:'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/langchain/chains/langchain.chains.llm.LLMChain.html', 'title': 'LLMChain â€” ðŸ¦œðŸ”— LangChain  documentation', 'language': 'en'}, page_content='inputs (Dict[str, Any] | Any) â€“ Dictionary of inputs, or single input if chain expects\\nonly one param. Should contain all inputs specified in\\nChain.input_keys except for inputs that will be set by the chainâ€™s\\nmemory.\\nreturn_only_outputs (bool) â€“ Whether to return only outputs in the\\nresponse. If True, only new keys generated by this chain will be\\nreturned. If False, both input keys and new keys generated by this\\nchain will be returned. Defaults to False.\\ncallbacks (list[BaseCallbackHandler] | BaseCallbackManager | None) â€“ Callbacks to use for this chain run. These will be called in\\naddition to callbacks passed to the chain during construction, but only\\nthese runtime callbacks will propagate to calls to other objects.\\ntags (List[str] | None) â€“ List of string tags to pass to all callbacks. These will be passed in\\naddition to tags passed to the chain during construction, but only\\nthese runtime tags will propagate to calls to other objects.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/langchain/chains/langchain.chains.llm.LLMChain.html', 'title': 'LLMChain â€” ðŸ¦œðŸ”— LangChain  documentation', 'language': 'en'}, page_content='addition to tags passed to the chain during construction, but only\\nthese runtime tags will propagate to calls to other objects.\\nmetadata (Dict[str, Any] | None) â€“ Optional metadata associated with the chain. Defaults to None\\ninclude_run_info (bool) â€“ Whether to include run info in the response. Defaults\\nto False.\\nrun_name (str | None)'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/langchain/chains/langchain.chains.llm.LLMChain.html', 'title': 'LLMChain â€” ðŸ¦œðŸ”— LangChain  documentation', 'language': 'en'}, page_content='Returns:\\n\\nA dict of named outputs. Should contain all outputs specified inChain.output_keys.\\n\\n\\n\\n\\nReturn type:\\nDict[str, Any]\\n\\n\\n\\n\\n\\nasync ainvoke(input: Dict[str, Any], config: RunnableConfig | None = None, **kwargs: Any) â†’ Dict[str, Any]#\\nDefault implementation of ainvoke, calls invoke from a thread.\\nThe default implementation allows usage of async code even if\\nthe Runnable did not implement a native async version of invoke.\\nSubclasses should override this method if they can run asynchronously.\\n\\nParameters:\\n\\ninput (Dict[str, Any])\\nconfig (RunnableConfig | None)\\nkwargs (Any)\\n\\n\\nReturn type:\\nDict[str, Any]\\n\\n\\n\\n\\n\\napply(input_list: List[Dict[str, Any]], callbacks: list[BaseCallbackHandler] | BaseCallbackManager | None = None) â†’ List[Dict[str, str]][source]#\\nUtilize the LLM generate method for speed gains.\\n\\nParameters:\\n\\ninput_list (List[Dict[str, Any]])\\ncallbacks (list[BaseCallbackHandler] | BaseCallbackManager | None)\\n\\n\\nReturn type:\\nList[Dict[str, str]]'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/langchain/chains/langchain.chains.llm.LLMChain.html', 'title': 'LLMChain â€” ðŸ¦œðŸ”— LangChain  documentation', 'language': 'en'}, page_content='Parameters:\\n\\ninput_list (List[Dict[str, Any]])\\ncallbacks (list[BaseCallbackHandler] | BaseCallbackManager | None)\\n\\n\\nReturn type:\\nList[Dict[str, str]]\\n\\n\\n\\n\\n\\napply_and_parse(input_list: List[Dict[str, Any]], callbacks: list[BaseCallbackHandler] | BaseCallbackManager | None = None) â†’ Sequence[str | List[str] | Dict[str, str]][source]#\\nCall apply and then parse the results.\\n\\nParameters:\\n\\ninput_list (List[Dict[str, Any]])\\ncallbacks (list[BaseCallbackHandler] | BaseCallbackManager | None)\\n\\n\\nReturn type:\\nSequence[str | List[str] | Dict[str, str]]\\n\\n\\n\\n\\n\\nasync apredict_and_parse(callbacks: list[BaseCallbackHandler] | BaseCallbackManager | None = None, **kwargs: Any) â†’ str | List[str] | Dict[str, str][source]#\\nCall apredict and then parse the results.\\n\\nParameters:\\n\\ncallbacks (list[BaseCallbackHandler] | BaseCallbackManager | None)\\nkwargs (Any)\\n\\n\\nReturn type:\\nstr | List[str] | Dict[str, str]'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/langchain/chains/langchain.chains.llm.LLMChain.html', 'title': 'LLMChain â€” ðŸ¦œðŸ”— LangChain  documentation', 'language': 'en'}, page_content='Parameters:\\n\\ncallbacks (list[BaseCallbackHandler] | BaseCallbackManager | None)\\nkwargs (Any)\\n\\n\\nReturn type:\\nstr | List[str] | Dict[str, str]\\n\\n\\n\\n\\n\\nasync aprep_inputs(inputs: Dict[str, Any] | Any) â†’ Dict[str, str]#\\nPrepare chain inputs, including adding inputs from memory.\\n\\nParameters:\\ninputs (Dict[str, Any] | Any) â€“ Dictionary of raw inputs, or single input if chain expects\\nonly one param. Should contain all inputs specified in\\nChain.input_keys except for inputs that will be set by the chainâ€™s\\nmemory.\\n\\nReturns:\\nA dictionary of all inputs, including those added by the chainâ€™s memory.\\n\\nReturn type:\\nDict[str, str]\\n\\n\\n\\n\\n\\nasync aprep_outputs(inputs: Dict[str, str], outputs: Dict[str, str], return_only_outputs: bool = False) â†’ Dict[str, str]#\\nValidate and prepare chain outputs, and save info about this run to memory.\\n\\nParameters:'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/langchain/chains/langchain.chains.llm.LLMChain.html', 'title': 'LLMChain â€” ðŸ¦œðŸ”— LangChain  documentation', 'language': 'en'}, page_content='Parameters:\\n\\ninputs (Dict[str, str]) â€“ Dictionary of chain inputs, including any inputs added by chain\\nmemory.\\noutputs (Dict[str, str]) â€“ Dictionary of initial chain outputs.\\nreturn_only_outputs (bool) â€“ Whether to only return the chain outputs. If False,\\ninputs are also added to the final outputs.\\n\\n\\nReturns:\\nA dict of the final chain outputs.\\n\\nReturn type:\\nDict[str, str]\\n\\n\\n\\n\\n\\nasync aprep_prompts(input_list: List[Dict[str, Any]], run_manager: AsyncCallbackManagerForChainRun | None = None) â†’ Tuple[List[PromptValue], List[str] | None][source]#\\nPrepare prompts from inputs.\\n\\nParameters:\\n\\ninput_list (List[Dict[str, Any]])\\nrun_manager (AsyncCallbackManagerForChainRun | None)\\n\\n\\nReturn type:\\nTuple[List[PromptValue], List[str] | None]\\n\\n\\n\\n\\n\\nasync arun(*args: Any, callbacks: list[BaseCallbackHandler] | BaseCallbackManager | None = None, tags: List[str] | None = None, metadata: Dict[str, Any] | None = None, **kwargs: Any) â†’ Any#'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/langchain/chains/langchain.chains.llm.LLMChain.html', 'title': 'LLMChain â€” ðŸ¦œðŸ”— LangChain  documentation', 'language': 'en'}, page_content='async arun(*args: Any, callbacks: list[BaseCallbackHandler] | BaseCallbackManager | None = None, tags: List[str] | None = None, metadata: Dict[str, Any] | None = None, **kwargs: Any) â†’ Any#\\n\\nDeprecated since version 0.1.0: Use ainvoke() instead. It will not be removed until langchain==1.0.\\n\\nConvenience method for executing chain.\\nThe main difference between this method and Chain.__call__ is that this\\nmethod expects inputs to be passed directly in as positional arguments or\\nkeyword arguments, whereas Chain.__call__ expects a single input dictionary\\nwith all the inputs\\n\\nParameters:'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/langchain/chains/langchain.chains.llm.LLMChain.html', 'title': 'LLMChain â€” ðŸ¦œðŸ”— LangChain  documentation', 'language': 'en'}, page_content='Parameters:\\n\\n*args (Any) â€“ If the chain expects a single input, it can be passed in as the\\nsole positional argument.\\ncallbacks (list[BaseCallbackHandler] | BaseCallbackManager | None) â€“ Callbacks to use for this chain run. These will be called in\\naddition to callbacks passed to the chain during construction, but only\\nthese runtime callbacks will propagate to calls to other objects.\\ntags (List[str] | None) â€“ List of string tags to pass to all callbacks. These will be passed in\\naddition to tags passed to the chain during construction, but only\\nthese runtime tags will propagate to calls to other objects.\\n**kwargs (Any) â€“ If the chain expects multiple inputs, they can be passed in\\ndirectly as keyword arguments.\\nmetadata (Dict[str, Any] | None)\\n**kwargs\\n\\n\\nReturns:\\nThe chain output.\\n\\nReturn type:\\nAny\\n\\n\\nExample\\n# Suppose we have a single-input chain that takes a \\'question\\' string:\\nawait chain.arun(\"What\\'s the temperature in Boise, Idaho?\")\\n# -> \"The temperature in Boise is...\"'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/langchain/chains/langchain.chains.llm.LLMChain.html', 'title': 'LLMChain â€” ðŸ¦œðŸ”— LangChain  documentation', 'language': 'en'}, page_content='Return type:\\nAny\\n\\n\\nExample\\n# Suppose we have a single-input chain that takes a \\'question\\' string:\\nawait chain.arun(\"What\\'s the temperature in Boise, Idaho?\")\\n# -> \"The temperature in Boise is...\"\\n\\n# Suppose we have a multi-input chain that takes a \\'question\\' string\\n# and \\'context\\' string:\\nquestion = \"What\\'s the temperature in Boise, Idaho?\"\\ncontext = \"Weather report for Boise, Idaho on 07/03/23...\"\\nawait chain.arun(question=question, context=context)\\n# -> \"The temperature in Boise is...\"\\n\\n\\n\\n\\n\\nasync astream(input: Input, config: RunnableConfig | None = None, **kwargs: Any | None) â†’ AsyncIterator[Output]#\\nDefault implementation of astream, which calls ainvoke.\\nSubclasses should override this method if they support streaming output.\\n\\nParameters:\\n\\ninput (Input) â€“ The input to the Runnable.\\nconfig (RunnableConfig | None) â€“ The config to use for the Runnable. Defaults to None.\\nkwargs (Any | None) â€“ Additional keyword arguments to pass to the Runnable.\\n\\n\\nYields:\\nThe output of the Runnable.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/langchain/chains/langchain.chains.llm.LLMChain.html', 'title': 'LLMChain â€” ðŸ¦œðŸ”— LangChain  documentation', 'language': 'en'}, page_content=\"Yields:\\nThe output of the Runnable.\\n\\nReturn type:\\nAsyncIterator[Output]\\n\\n\\n\\n\\n\\nasync astream_events(input: Any, config: RunnableConfig | None = None, *, version: Literal['v1', 'v2'], include_names: Sequence[str] | None = None, include_types: Sequence[str] | None = None, include_tags: Sequence[str] | None = None, exclude_names: Sequence[str] | None = None, exclude_types: Sequence[str] | None = None, exclude_tags: Sequence[str] | None = None, **kwargs: Any) â†’ AsyncIterator[StandardStreamEvent | CustomStreamEvent]#\\nGenerate a stream of events.\\nUse to create an iterator over StreamEvents that provide real-time information\\nabout the progress of the Runnable, including StreamEvents from intermediate\\nresults.\\nA StreamEvent is a dictionary with the following schema:\\n\\n\\nevent: str - Event names are of theformat: on_[runnable_type]_(start|stream|end).\\n\\n\\n\\nname: str - The name of the Runnable that generated the event.\"),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/langchain/chains/langchain.chains.llm.LLMChain.html', 'title': 'LLMChain â€” ðŸ¦œðŸ”— LangChain  documentation', 'language': 'en'}, page_content='event: str - Event names are of theformat: on_[runnable_type]_(start|stream|end).\\n\\n\\n\\nname: str - The name of the Runnable that generated the event.\\n\\nrun_id: str - randomly generated ID associated with the given execution ofthe Runnable that emitted the event.\\nA child Runnable that gets invoked as part of the execution of a\\nparent Runnable is assigned its own unique ID.\\n\\n\\n\\n\\nparent_ids: List[str] - The IDs of the parent runnables thatgenerated the event. The root Runnable will have an empty list.\\nThe order of the parent IDs is from the root to the immediate parent.\\nOnly available for v2 version of the API. The v1 version of the API\\nwill return an empty list.\\n\\n\\n\\n\\ntags: Optional[List[str]] - The tags of the Runnable that generatedthe event.\\n\\n\\n\\n\\nmetadata: Optional[Dict[str, Any]] - The metadata of the Runnablethat generated the event.\\n\\n\\n\\ndata: Dict[str, Any]'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/langchain/chains/langchain.chains.llm.LLMChain.html', 'title': 'LLMChain â€” ðŸ¦œðŸ”— LangChain  documentation', 'language': 'en'}, page_content='tags: Optional[List[str]] - The tags of the Runnable that generatedthe event.\\n\\n\\n\\n\\nmetadata: Optional[Dict[str, Any]] - The metadata of the Runnablethat generated the event.\\n\\n\\n\\ndata: Dict[str, Any]\\n\\nBelow is a table that illustrates some events that might be emitted by various\\nchains. Metadata fields have been omitted from the table for brevity.\\nChain definitions have been included after the table.\\nATTENTION This reference table is for the V2 version of the schema.\\n\\n\\nevent\\nname\\nchunk\\ninput\\noutput\\n\\n\\n\\non_chat_model_start\\n[model name]\\n\\n{â€œmessagesâ€: [[SystemMessage, HumanMessage]]}\\n\\n\\non_chat_model_stream\\n[model name]\\nAIMessageChunk(content=â€helloâ€)\\n\\n\\n\\non_chat_model_end\\n[model name]\\n\\n{â€œmessagesâ€: [[SystemMessage, HumanMessage]]}\\nAIMessageChunk(content=â€hello worldâ€)\\n\\non_llm_start\\n[model name]\\n\\n{â€˜inputâ€™: â€˜helloâ€™}\\n\\n\\non_llm_stream\\n[model name]\\nâ€˜Helloâ€™\\n\\n\\n\\non_llm_end\\n[model name]\\n\\nâ€˜Hello human!â€™\\n\\n\\non_chain_start\\nformat_docs\\n\\n\\n\\n\\non_chain_stream\\nformat_docs\\nâ€œhello world!, goodbye world!â€'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/langchain/chains/langchain.chains.llm.LLMChain.html', 'title': 'LLMChain â€” ðŸ¦œðŸ”— LangChain  documentation', 'language': 'en'}, page_content='{â€˜inputâ€™: â€˜helloâ€™}\\n\\n\\non_llm_stream\\n[model name]\\nâ€˜Helloâ€™\\n\\n\\n\\non_llm_end\\n[model name]\\n\\nâ€˜Hello human!â€™\\n\\n\\non_chain_start\\nformat_docs\\n\\n\\n\\n\\non_chain_stream\\nformat_docs\\nâ€œhello world!, goodbye world!â€\\n\\n\\n\\non_chain_end\\nformat_docs\\n\\n[Document(â€¦)]\\nâ€œhello world!, goodbye world!â€\\n\\non_tool_start\\nsome_tool\\n\\n{â€œxâ€: 1, â€œyâ€: â€œ2â€}\\n\\n\\non_tool_end\\nsome_tool\\n\\n\\n{â€œxâ€: 1, â€œyâ€: â€œ2â€}\\n\\non_retriever_start\\n[retriever name]\\n\\n{â€œqueryâ€: â€œhelloâ€}\\n\\n\\non_retriever_end\\n[retriever name]\\n\\n{â€œqueryâ€: â€œhelloâ€}\\n[Document(â€¦), ..]\\n\\non_prompt_start\\n[template_name]\\n\\n{â€œquestionâ€: â€œhelloâ€}\\n\\n\\non_prompt_end\\n[template_name]\\n\\n{â€œquestionâ€: â€œhelloâ€}\\nChatPromptValue(messages: [SystemMessage, â€¦])\\n\\n\\n\\n\\nIn addition to the standard events, users can also dispatch custom events (see example below).\\nCustom events will be only be surfaced with in the v2 version of the API!\\nA custom event has following format:\\n\\n\\nAttribute\\nType\\nDescription\\n\\n\\n\\nname\\nstr\\nA user defined name for the event.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/langchain/chains/langchain.chains.llm.LLMChain.html', 'title': 'LLMChain â€” ðŸ¦œðŸ”— LangChain  documentation', 'language': 'en'}, page_content='Attribute\\nType\\nDescription\\n\\n\\n\\nname\\nstr\\nA user defined name for the event.\\n\\ndata\\nAny\\nThe data associated with the event. This can be anything, though we suggest making it JSON serializable.\\n\\n\\n\\n\\nHere are declarations associated with the standard events shown above:\\nformat_docs:\\ndef format_docs(docs: List[Document]) -> str:\\n    \\'\\'\\'Format the docs.\\'\\'\\'\\n    return \", \".join([doc.page_content for doc in docs])\\n\\nformat_docs = RunnableLambda(format_docs)\\n\\n\\nsome_tool:\\n@tool\\ndef some_tool(x: int, y: str) -> dict:\\n    \\'\\'\\'Some_tool.\\'\\'\\'\\n    return {\"x\": x, \"y\": y}\\n\\n\\nprompt:\\ntemplate = ChatPromptTemplate.from_messages(\\n    [(\"system\", \"You are Cat Agent 007\"), (\"human\", \"{question}\")]\\n).with_config({\"run_name\": \"my_template\", \"tags\": [\"my_template\"]})\\n\\n\\nExample:\\nfrom langchain_core.runnables import RunnableLambda\\n\\nasync def reverse(s: str) -> str:\\n    return s[::-1]\\n\\nchain = RunnableLambda(func=reverse)\\n\\nevents = [\\n    event async for event in chain.astream_events(\"hello\", version=\"v2\")\\n]'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/langchain/chains/langchain.chains.llm.LLMChain.html', 'title': 'LLMChain â€” ðŸ¦œðŸ”— LangChain  documentation', 'language': 'en'}, page_content='async def reverse(s: str) -> str:\\n    return s[::-1]\\n\\nchain = RunnableLambda(func=reverse)\\n\\nevents = [\\n    event async for event in chain.astream_events(\"hello\", version=\"v2\")\\n]\\n\\n# will produce the following events (run_id, and parent_ids\\n# has been omitted for brevity):\\n[\\n    {\\n        \"data\": {\"input\": \"hello\"},\\n        \"event\": \"on_chain_start\",\\n        \"metadata\": {},\\n        \"name\": \"reverse\",\\n        \"tags\": [],\\n    },\\n    {\\n        \"data\": {\"chunk\": \"olleh\"},\\n        \"event\": \"on_chain_stream\",\\n        \"metadata\": {},\\n        \"name\": \"reverse\",\\n        \"tags\": [],\\n    },\\n    {\\n        \"data\": {\"output\": \"olleh\"},\\n        \"event\": \"on_chain_end\",\\n        \"metadata\": {},\\n        \"name\": \"reverse\",\\n        \"tags\": [],\\n    },\\n]\\n\\n\\nExample: Dispatch Custom Event\\nfrom langchain_core.callbacks.manager import (\\n    adispatch_custom_event,\\n)\\nfrom langchain_core.runnables import RunnableLambda, RunnableConfig\\nimport asyncio'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/langchain/chains/langchain.chains.llm.LLMChain.html', 'title': 'LLMChain â€” ðŸ¦œðŸ”— LangChain  documentation', 'language': 'en'}, page_content='Example: Dispatch Custom Event\\nfrom langchain_core.callbacks.manager import (\\n    adispatch_custom_event,\\n)\\nfrom langchain_core.runnables import RunnableLambda, RunnableConfig\\nimport asyncio\\n\\n\\nasync def slow_thing(some_input: str, config: RunnableConfig) -> str:\\n    \"\"\"Do something that takes a long time.\"\"\"\\n    await asyncio.sleep(1) # Placeholder for some slow operation\\n    await adispatch_custom_event(\\n        \"progress_event\",\\n        {\"message\": \"Finished step 1 of 3\"},\\n        config=config # Must be included for python < 3.10\\n    )\\n    await asyncio.sleep(1) # Placeholder for some slow operation\\n    await adispatch_custom_event(\\n        \"progress_event\",\\n        {\"message\": \"Finished step 2 of 3\"},\\n        config=config # Must be included for python < 3.10\\n    )\\n    await asyncio.sleep(1) # Placeholder for some slow operation\\n    return \"Done\"\\n\\nslow_thing = RunnableLambda(slow_thing)\\n\\nasync for event in slow_thing.astream_events(\"some_input\", version=\"v2\"):\\n    print(event)'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/langchain/chains/langchain.chains.llm.LLMChain.html', 'title': 'LLMChain â€” ðŸ¦œðŸ”— LangChain  documentation', 'language': 'en'}, page_content='slow_thing = RunnableLambda(slow_thing)\\n\\nasync for event in slow_thing.astream_events(\"some_input\", version=\"v2\"):\\n    print(event)\\n\\n\\n\\nParameters:'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/langchain/chains/langchain.chains.llm.LLMChain.html', 'title': 'LLMChain â€” ðŸ¦œðŸ”— LangChain  documentation', 'language': 'en'}, page_content=\"input (Any) â€“ The input to the Runnable.\\nconfig (RunnableConfig | None) â€“ The config to use for the Runnable.\\nversion (Literal['v1', 'v2']) â€“ The version of the schema to use either v2 or v1.\\nUsers should use v2.\\nv1 is for backwards compatibility and will be deprecated\\nin 0.4.0.\\nNo default will be assigned until the API is stabilized.\\ncustom events will only be surfaced in v2.\\ninclude_names (Sequence[str] | None) â€“ Only include events from runnables with matching names.\\ninclude_types (Sequence[str] | None) â€“ Only include events from runnables with matching types.\\ninclude_tags (Sequence[str] | None) â€“ Only include events from runnables with matching tags.\\nexclude_names (Sequence[str] | None) â€“ Exclude events from runnables with matching names.\\nexclude_types (Sequence[str] | None) â€“ Exclude events from runnables with matching types.\\nexclude_tags (Sequence[str] | None) â€“ Exclude events from runnables with matching tags.\"),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/langchain/chains/langchain.chains.llm.LLMChain.html', 'title': 'LLMChain â€” ðŸ¦œðŸ”— LangChain  documentation', 'language': 'en'}, page_content='exclude_types (Sequence[str] | None) â€“ Exclude events from runnables with matching types.\\nexclude_tags (Sequence[str] | None) â€“ Exclude events from runnables with matching tags.\\nkwargs (Any) â€“ Additional keyword arguments to pass to the Runnable.\\nThese will be passed to astream_log as this implementation\\nof astream_events is built on top of astream_log.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/langchain/chains/langchain.chains.llm.LLMChain.html', 'title': 'LLMChain â€” ðŸ¦œðŸ”— LangChain  documentation', 'language': 'en'}, page_content='Yields:\\nAn async stream of StreamEvents.\\n\\nRaises:\\nNotImplementedError â€“ If the version is not v1 or v2.\\n\\nReturn type:\\nAsyncIterator[StandardStreamEvent | CustomStreamEvent]\\n\\n\\n\\n\\n\\nbatch(inputs: list[Input], config: RunnableConfig | list[RunnableConfig] | None = None, *, return_exceptions: bool = False, **kwargs: Any | None) â†’ list[Output]#\\nDefault implementation runs invoke in parallel using a thread pool executor.\\nThe default implementation of batch works well for IO bound runnables.\\nSubclasses should override this method if they can batch more efficiently;\\ne.g., if the underlying Runnable uses an API which supports a batch mode.\\n\\nParameters:\\n\\ninputs (list[Input])\\nconfig (RunnableConfig | list[RunnableConfig] | None)\\nreturn_exceptions (bool)\\nkwargs (Any | None)\\n\\n\\nReturn type:\\nlist[Output]'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/langchain/chains/langchain.chains.llm.LLMChain.html', 'title': 'LLMChain â€” ðŸ¦œðŸ”— LangChain  documentation', 'language': 'en'}, page_content='Parameters:\\n\\ninputs (list[Input])\\nconfig (RunnableConfig | list[RunnableConfig] | None)\\nreturn_exceptions (bool)\\nkwargs (Any | None)\\n\\n\\nReturn type:\\nlist[Output]\\n\\n\\n\\n\\n\\nbatch_as_completed(inputs: Sequence[Input], config: RunnableConfig | Sequence[RunnableConfig] | None = None, *, return_exceptions: bool = False, **kwargs: Any | None) â†’ Iterator[tuple[int, Output | Exception]]#\\nRun invoke in parallel on a list of inputs,\\nyielding results as they complete.\\n\\nParameters:\\n\\ninputs (Sequence[Input])\\nconfig (RunnableConfig | Sequence[RunnableConfig] | None)\\nreturn_exceptions (bool)\\nkwargs (Any | None)\\n\\n\\nReturn type:\\nIterator[tuple[int, Output | Exception]]\\n\\n\\n\\n\\n\\nbind(**kwargs: Any) â†’ Runnable[Input, Output]#\\nBind arguments to a Runnable, returning a new Runnable.\\nUseful when a Runnable in a chain requires an argument that is not\\nin the output of the previous Runnable or included in the user input.\\n\\nParameters:\\nkwargs (Any) â€“ The arguments to bind to the Runnable.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/langchain/chains/langchain.chains.llm.LLMChain.html', 'title': 'LLMChain â€” ðŸ¦œðŸ”— LangChain  documentation', 'language': 'en'}, page_content='Parameters:\\nkwargs (Any) â€“ The arguments to bind to the Runnable.\\n\\nReturns:\\nA new Runnable with the arguments bound.\\n\\nReturn type:\\nRunnable[Input, Output]\\n\\n\\nExample:\\nfrom langchain_community.chat_models import ChatOllama\\nfrom langchain_core.output_parsers import StrOutputParser\\n\\nllm = ChatOllama(model=\\'llama2\\')\\n\\n# Without bind.\\nchain = (\\n    llm\\n    | StrOutputParser()\\n)\\n\\nchain.invoke(\"Repeat quoted words exactly: \\'One two three four five.\\'\")\\n# Output is \\'One two three four five.\\'\\n\\n# With bind.\\nchain = (\\n    llm.bind(stop=[\"three\"])\\n    | StrOutputParser()\\n)\\n\\nchain.invoke(\"Repeat quoted words exactly: \\'One two three four five.\\'\")\\n# Output is \\'One two\\'\\n\\n\\n\\n\\n\\nconfigurable_alternatives(which: ConfigurableField, *, default_key: str = \\'default\\', prefix_keys: bool = False, **kwargs: Runnable[Input, Output] | Callable[[], Runnable[Input, Output]]) â†’ RunnableSerializable#\\nConfigure alternatives for Runnables that can be set at runtime.\\n\\nParameters:'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/langchain/chains/langchain.chains.llm.LLMChain.html', 'title': 'LLMChain â€” ðŸ¦œðŸ”— LangChain  documentation', 'language': 'en'}, page_content='Parameters:\\n\\nwhich (ConfigurableField) â€“ The ConfigurableField instance that will be used to select the\\nalternative.\\ndefault_key (str) â€“ The default key to use if no alternative is selected.\\nDefaults to â€œdefaultâ€.\\nprefix_keys (bool) â€“ Whether to prefix the keys with the ConfigurableField id.\\nDefaults to False.\\n**kwargs (Runnable[Input, Output] | Callable[[], Runnable[Input, Output]]) â€“ A dictionary of keys to Runnable instances or callables that\\nreturn Runnable instances.\\n\\n\\nReturns:\\nA new Runnable with the alternatives configured.\\n\\nReturn type:\\nRunnableSerializable\\n\\n\\nfrom langchain_anthropic import ChatAnthropic\\nfrom langchain_core.runnables.utils import ConfigurableField\\nfrom langchain_openai import ChatOpenAI\\n\\nmodel = ChatAnthropic(\\n    model_name=\"claude-3-sonnet-20240229\"\\n).configurable_alternatives(\\n    ConfigurableField(id=\"llm\"),\\n    default_key=\"anthropic\",\\n    openai=ChatOpenAI()\\n)'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/langchain/chains/langchain.chains.llm.LLMChain.html', 'title': 'LLMChain â€” ðŸ¦œðŸ”— LangChain  documentation', 'language': 'en'}, page_content='model = ChatAnthropic(\\n    model_name=\"claude-3-sonnet-20240229\"\\n).configurable_alternatives(\\n    ConfigurableField(id=\"llm\"),\\n    default_key=\"anthropic\",\\n    openai=ChatOpenAI()\\n)\\n\\n# uses the default model ChatAnthropic\\nprint(model.invoke(\"which organization created you?\").content)\\n\\n# uses ChatOpenAI\\nprint(\\n    model.with_config(\\n        configurable={\"llm\": \"openai\"}\\n    ).invoke(\"which organization created you?\").content\\n)\\n\\n\\n\\n\\n\\nconfigurable_fields(**kwargs: ConfigurableField | ConfigurableFieldSingleOption | ConfigurableFieldMultiOption) â†’ RunnableSerializable#\\nConfigure particular Runnable fields at runtime.\\n\\nParameters:\\n**kwargs (ConfigurableField | ConfigurableFieldSingleOption | ConfigurableFieldMultiOption) â€“ A dictionary of ConfigurableField instances to configure.\\n\\nReturns:\\nA new Runnable with the fields configured.\\n\\nReturn type:\\nRunnableSerializable\\n\\n\\nfrom langchain_core.runnables import ConfigurableField\\nfrom langchain_openai import ChatOpenAI'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/langchain/chains/langchain.chains.llm.LLMChain.html', 'title': 'LLMChain â€” ðŸ¦œðŸ”— LangChain  documentation', 'language': 'en'}, page_content='Returns:\\nA new Runnable with the fields configured.\\n\\nReturn type:\\nRunnableSerializable\\n\\n\\nfrom langchain_core.runnables import ConfigurableField\\nfrom langchain_openai import ChatOpenAI\\n\\nmodel = ChatOpenAI(max_tokens=20).configurable_fields(\\n    max_tokens=ConfigurableField(\\n        id=\"output_token_number\",\\n        name=\"Max tokens in the output\",\\n        description=\"The maximum number of tokens in the output\",\\n    )\\n)\\n\\n# max_tokens = 20\\nprint(\\n    \"max_tokens_20: \",\\n    model.invoke(\"tell me something about chess\").content\\n)\\n\\n# max_tokens = 200\\nprint(\"max_tokens_200: \", model.with_config(\\n    configurable={\"output_token_number\": 200}\\n    ).invoke(\"tell me something about chess\").content\\n)\\n\\n\\n\\n\\n\\ncreate_outputs(llm_result: LLMResult) â†’ List[Dict[str, Any]][source]#\\nCreate outputs from response.\\n\\nParameters:\\nllm_result (LLMResult)\\n\\nReturn type:\\nList[Dict[str, Any]]'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/langchain/chains/langchain.chains.llm.LLMChain.html', 'title': 'LLMChain â€” ðŸ¦œðŸ”— LangChain  documentation', 'language': 'en'}, page_content='create_outputs(llm_result: LLMResult) â†’ List[Dict[str, Any]][source]#\\nCreate outputs from response.\\n\\nParameters:\\nllm_result (LLMResult)\\n\\nReturn type:\\nList[Dict[str, Any]]\\n\\n\\n\\n\\n\\nclassmethod from_string(llm: BaseLanguageModel, template: str) â†’ LLMChain[source]#\\nCreate LLMChain from LLM and template.\\n\\nParameters:\\n\\nllm (BaseLanguageModel)\\ntemplate (str)\\n\\n\\nReturn type:\\nLLMChain\\n\\n\\n\\n\\n\\ninvoke(input: Dict[str, Any], config: RunnableConfig | None = None, **kwargs: Any) â†’ Dict[str, Any]#\\nTransform a single input into an output. Override to implement.\\n\\nParameters:\\n\\ninput (Dict[str, Any]) â€“ The input to the Runnable.\\nconfig (RunnableConfig | None) â€“ A config to use when invoking the Runnable.\\nThe config supports standard keys like â€˜tagsâ€™, â€˜metadataâ€™ for tracing\\npurposes, â€˜max_concurrencyâ€™ for controlling how much work to do\\nin parallel, and other keys. Please refer to the RunnableConfig\\nfor more details.\\nkwargs (Any)\\n\\n\\nReturns:\\nThe output of the Runnable.\\n\\nReturn type:\\nDict[str, Any]'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/langchain/chains/langchain.chains.llm.LLMChain.html', 'title': 'LLMChain â€” ðŸ¦œðŸ”— LangChain  documentation', 'language': 'en'}, page_content='Returns:\\nThe output of the Runnable.\\n\\nReturn type:\\nDict[str, Any]\\n\\n\\n\\n\\n\\npredict_and_parse(callbacks: list[BaseCallbackHandler] | BaseCallbackManager | None = None, **kwargs: Any) â†’ str | List[str] | Dict[str, Any][source]#\\nCall predict and then parse the results.\\n\\nParameters:\\n\\ncallbacks (list[BaseCallbackHandler] | BaseCallbackManager | None)\\nkwargs (Any)\\n\\n\\nReturn type:\\nstr | List[str] | Dict[str, Any]\\n\\n\\n\\n\\n\\nprep_inputs(inputs: Dict[str, Any] | Any) â†’ Dict[str, str]#\\nPrepare chain inputs, including adding inputs from memory.\\n\\nParameters:\\ninputs (Dict[str, Any] | Any) â€“ Dictionary of raw inputs, or single input if chain expects\\nonly one param. Should contain all inputs specified in\\nChain.input_keys except for inputs that will be set by the chainâ€™s\\nmemory.\\n\\nReturns:\\nA dictionary of all inputs, including those added by the chainâ€™s memory.\\n\\nReturn type:\\nDict[str, str]'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/langchain/chains/langchain.chains.llm.LLMChain.html', 'title': 'LLMChain â€” ðŸ¦œðŸ”— LangChain  documentation', 'language': 'en'}, page_content='Returns:\\nA dictionary of all inputs, including those added by the chainâ€™s memory.\\n\\nReturn type:\\nDict[str, str]\\n\\n\\n\\n\\n\\nprep_outputs(inputs: Dict[str, str], outputs: Dict[str, str], return_only_outputs: bool = False) â†’ Dict[str, str]#\\nValidate and prepare chain outputs, and save info about this run to memory.\\n\\nParameters:\\n\\ninputs (Dict[str, str]) â€“ Dictionary of chain inputs, including any inputs added by chain\\nmemory.\\noutputs (Dict[str, str]) â€“ Dictionary of initial chain outputs.\\nreturn_only_outputs (bool) â€“ Whether to only return the chain outputs. If False,\\ninputs are also added to the final outputs.\\n\\n\\nReturns:\\nA dict of the final chain outputs.\\n\\nReturn type:\\nDict[str, str]\\n\\n\\n\\n\\n\\nprep_prompts(input_list: List[Dict[str, Any]], run_manager: CallbackManagerForChainRun | None = None) â†’ Tuple[List[PromptValue], List[str] | None][source]#\\nPrepare prompts from inputs.\\n\\nParameters:\\n\\ninput_list (List[Dict[str, Any]])\\nrun_manager (CallbackManagerForChainRun | None)'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/langchain/chains/langchain.chains.llm.LLMChain.html', 'title': 'LLMChain â€” ðŸ¦œðŸ”— LangChain  documentation', 'language': 'en'}, page_content='Parameters:\\n\\ninput_list (List[Dict[str, Any]])\\nrun_manager (CallbackManagerForChainRun | None)\\n\\n\\nReturn type:\\nTuple[List[PromptValue], List[str] | None]\\n\\n\\n\\n\\n\\nrun(*args: Any, callbacks: list[BaseCallbackHandler] | BaseCallbackManager | None = None, tags: List[str] | None = None, metadata: Dict[str, Any] | None = None, **kwargs: Any) â†’ Any#\\n\\nDeprecated since version 0.1.0: Use invoke() instead. It will not be removed until langchain==1.0.\\n\\nConvenience method for executing chain.\\nThe main difference between this method and Chain.__call__ is that this\\nmethod expects inputs to be passed directly in as positional arguments or\\nkeyword arguments, whereas Chain.__call__ expects a single input dictionary\\nwith all the inputs\\n\\nParameters:'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/langchain/chains/langchain.chains.llm.LLMChain.html', 'title': 'LLMChain â€” ðŸ¦œðŸ”— LangChain  documentation', 'language': 'en'}, page_content='Parameters:\\n\\n*args (Any) â€“ If the chain expects a single input, it can be passed in as the\\nsole positional argument.\\ncallbacks (list[BaseCallbackHandler] | BaseCallbackManager | None) â€“ Callbacks to use for this chain run. These will be called in\\naddition to callbacks passed to the chain during construction, but only\\nthese runtime callbacks will propagate to calls to other objects.\\ntags (List[str] | None) â€“ List of string tags to pass to all callbacks. These will be passed in\\naddition to tags passed to the chain during construction, but only\\nthese runtime tags will propagate to calls to other objects.\\n**kwargs (Any) â€“ If the chain expects multiple inputs, they can be passed in\\ndirectly as keyword arguments.\\nmetadata (Dict[str, Any] | None)\\n**kwargs\\n\\n\\nReturns:\\nThe chain output.\\n\\nReturn type:\\nAny\\n\\n\\nExample\\n# Suppose we have a single-input chain that takes a \\'question\\' string:\\nchain.run(\"What\\'s the temperature in Boise, Idaho?\")\\n# -> \"The temperature in Boise is...\"'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/langchain/chains/langchain.chains.llm.LLMChain.html', 'title': 'LLMChain â€” ðŸ¦œðŸ”— LangChain  documentation', 'language': 'en'}, page_content='Return type:\\nAny\\n\\n\\nExample\\n# Suppose we have a single-input chain that takes a \\'question\\' string:\\nchain.run(\"What\\'s the temperature in Boise, Idaho?\")\\n# -> \"The temperature in Boise is...\"\\n\\n# Suppose we have a multi-input chain that takes a \\'question\\' string\\n# and \\'context\\' string:\\nquestion = \"What\\'s the temperature in Boise, Idaho?\"\\ncontext = \"Weather report for Boise, Idaho on 07/03/23...\"\\nchain.run(question=question, context=context)\\n# -> \"The temperature in Boise is...\"\\n\\n\\n\\n\\n\\nsave(file_path: Path | str) â†’ None#\\nSave the chain.\\n\\nExpects Chain._chain_type property to be implemented and for memory to benull.\\n\\n\\n\\nParameters:\\nfile_path (Path | str) â€“ Path to file to save the chain to.\\n\\nReturn type:\\nNone\\n\\n\\nExample\\nchain.save(file_path=\"path/chain.yaml\")\\n\\n\\n\\n\\n\\nstream(input: Input, config: RunnableConfig | None = None, **kwargs: Any | None) â†’ Iterator[Output]#\\nDefault implementation of stream, which calls invoke.\\nSubclasses should override this method if they support streaming output.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/langchain/chains/langchain.chains.llm.LLMChain.html', 'title': 'LLMChain â€” ðŸ¦œðŸ”— LangChain  documentation', 'language': 'en'}, page_content='Parameters:\\n\\ninput (Input) â€“ The input to the Runnable.\\nconfig (RunnableConfig | None) â€“ The config to use for the Runnable. Defaults to None.\\nkwargs (Any | None) â€“ Additional keyword arguments to pass to the Runnable.\\n\\n\\nYields:\\nThe output of the Runnable.\\n\\nReturn type:\\nIterator[Output]\\n\\n\\n\\n\\n\\nwith_alisteners(*, on_start: AsyncListener | None = None, on_end: AsyncListener | None = None, on_error: AsyncListener | None = None) â†’ Runnable[Input, Output]#\\nBind async lifecycle listeners to a Runnable, returning a new Runnable.\\non_start: Asynchronously called before the Runnable starts running.\\non_end: Asynchronously called after the Runnable finishes running.\\non_error: Asynchronously called if the Runnable throws an error.\\nThe Run object contains information about the run, including its id,\\ntype, input, output, error, start_time, end_time, and any tags or metadata\\nadded to the run.\\n\\nParameters:'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/langchain/chains/langchain.chains.llm.LLMChain.html', 'title': 'LLMChain â€” ðŸ¦œðŸ”— LangChain  documentation', 'language': 'en'}, page_content='Parameters:\\n\\non_start (Optional[AsyncListener]) â€“ Asynchronously called before the Runnable starts running.\\nDefaults to None.\\non_end (Optional[AsyncListener]) â€“ Asynchronously called after the Runnable finishes running.\\nDefaults to None.\\non_error (Optional[AsyncListener]) â€“ Asynchronously called if the Runnable throws an error.\\nDefaults to None.\\n\\n\\nReturns:\\nA new Runnable with the listeners bound.\\n\\nReturn type:\\nRunnable[Input, Output]\\n\\n\\nExample:\\nfrom langchain_core.runnables import RunnableLambda\\nimport time\\n\\nasync def test_runnable(time_to_sleep : int):\\n    print(f\"Runnable[{time_to_sleep}s]: starts at {format_t(time.time())}\")\\n    await asyncio.sleep(time_to_sleep)\\n    print(f\"Runnable[{time_to_sleep}s]: ends at {format_t(time.time())}\")\\n\\nasync def fn_start(run_obj : Runnable):\\n    print(f\"on start callback starts at {format_t(time.time())}\\n    await asyncio.sleep(3)\\n    print(f\"on start callback ends at {format_t(time.time())}\")'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/langchain/chains/langchain.chains.llm.LLMChain.html', 'title': 'LLMChain â€” ðŸ¦œðŸ”— LangChain  documentation', 'language': 'en'}, page_content='async def fn_start(run_obj : Runnable):\\n    print(f\"on start callback starts at {format_t(time.time())}\\n    await asyncio.sleep(3)\\n    print(f\"on start callback ends at {format_t(time.time())}\")\\n\\nasync def fn_end(run_obj : Runnable):\\n    print(f\"on end callback starts at {format_t(time.time())}\\n    await asyncio.sleep(2)\\n    print(f\"on end callback ends at {format_t(time.time())}\")\\n\\nrunnable = RunnableLambda(test_runnable).with_alisteners(\\n    on_start=fn_start,\\n    on_end=fn_end\\n)\\nasync def concurrent_runs():\\n    await asyncio.gather(runnable.ainvoke(2), runnable.ainvoke(3))'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/langchain/chains/langchain.chains.llm.LLMChain.html', 'title': 'LLMChain â€” ðŸ¦œðŸ”— LangChain  documentation', 'language': 'en'}, page_content='runnable = RunnableLambda(test_runnable).with_alisteners(\\n    on_start=fn_start,\\n    on_end=fn_end\\n)\\nasync def concurrent_runs():\\n    await asyncio.gather(runnable.ainvoke(2), runnable.ainvoke(3))\\n\\nasyncio.run(concurrent_runs())\\nResult:\\non start callback starts at 2024-05-16T14:20:29.637053+00:00\\non start callback starts at 2024-05-16T14:20:29.637150+00:00\\non start callback ends at 2024-05-16T14:20:32.638305+00:00\\non start callback ends at 2024-05-16T14:20:32.638383+00:00\\nRunnable[3s]: starts at 2024-05-16T14:20:32.638849+00:00\\nRunnable[5s]: starts at 2024-05-16T14:20:32.638999+00:00\\nRunnable[3s]: ends at 2024-05-16T14:20:35.640016+00:00\\non end callback starts at 2024-05-16T14:20:35.640534+00:00\\nRunnable[5s]: ends at 2024-05-16T14:20:37.640169+00:00\\non end callback starts at 2024-05-16T14:20:37.640574+00:00\\non end callback ends at 2024-05-16T14:20:37.640654+00:00\\non end callback ends at 2024-05-16T14:20:39.641751+00:00'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/langchain/chains/langchain.chains.llm.LLMChain.html', 'title': 'LLMChain â€” ðŸ¦œðŸ”— LangChain  documentation', 'language': 'en'}, page_content=\"with_config(config: RunnableConfig | None = None, **kwargs: Any) â†’ Runnable[Input, Output]#\\nBind config to a Runnable, returning a new Runnable.\\n\\nParameters:\\n\\nconfig (RunnableConfig | None) â€“ The config to bind to the Runnable.\\nkwargs (Any) â€“ Additional keyword arguments to pass to the Runnable.\\n\\n\\nReturns:\\nA new Runnable with the config bound.\\n\\nReturn type:\\nRunnable[Input, Output]\\n\\n\\n\\n\\n\\nwith_fallbacks(fallbacks: Sequence[Runnable[Input, Output]], *, exceptions_to_handle: tuple[type[BaseException], ...] = (<class 'Exception'>,), exception_key: Optional[str] = None) â†’ RunnableWithFallbacksT[Input, Output]#\\nAdd fallbacks to a Runnable, returning a new Runnable.\\nThe new Runnable will try the original Runnable, and then each fallback\\nin order, upon failures.\\n\\nParameters:\"),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/langchain/chains/langchain.chains.llm.LLMChain.html', 'title': 'LLMChain â€” ðŸ¦œðŸ”— LangChain  documentation', 'language': 'en'}, page_content='Parameters:\\n\\nfallbacks (Sequence[Runnable[Input, Output]]) â€“ A sequence of runnables to try if the original Runnable fails.\\nexceptions_to_handle (tuple[type[BaseException], ...]) â€“ A tuple of exception types to handle.\\nDefaults to (Exception,).\\nexception_key (Optional[str]) â€“ If string is specified then handled exceptions will be passed\\nto fallbacks as part of the input under the specified key. If None,\\nexceptions will not be passed to fallbacks. If used, the base Runnable\\nand its fallbacks must accept a dictionary as input. Defaults to None.\\n\\n\\nReturns:\\nA new Runnable that will try the original Runnable, and then each\\nfallback in order, upon failures.\\n\\nReturn type:\\nRunnableWithFallbacksT[Input, Output]\\n\\n\\nExample\\nfrom typing import Iterator\\n\\nfrom langchain_core.runnables import RunnableGenerator\\n\\n\\ndef _generate_immediate_error(input: Iterator) -> Iterator[str]:\\n    raise ValueError()\\n    yield \"\"\\n\\n\\ndef _generate(input: Iterator) -> Iterator[str]:\\n    yield from \"foo bar\"'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/langchain/chains/langchain.chains.llm.LLMChain.html', 'title': 'LLMChain â€” ðŸ¦œðŸ”— LangChain  documentation', 'language': 'en'}, page_content='def _generate_immediate_error(input: Iterator) -> Iterator[str]:\\n    raise ValueError()\\n    yield \"\"\\n\\n\\ndef _generate(input: Iterator) -> Iterator[str]:\\n    yield from \"foo bar\"\\n\\n\\nrunnable = RunnableGenerator(_generate_immediate_error).with_fallbacks(\\n    [RunnableGenerator(_generate)]\\n    )\\nprint(\\'\\'.join(runnable.stream({}))) #foo bar\\n\\n\\n\\nParameters:\\n\\nfallbacks (Sequence[Runnable[Input, Output]]) â€“ A sequence of runnables to try if the original Runnable fails.\\nexceptions_to_handle (tuple[type[BaseException], ...]) â€“ A tuple of exception types to handle.\\nexception_key (Optional[str]) â€“ If string is specified then handled exceptions will be passed\\nto fallbacks as part of the input under the specified key. If None,\\nexceptions will not be passed to fallbacks. If used, the base Runnable\\nand its fallbacks must accept a dictionary as input.\\n\\n\\nReturns:\\nA new Runnable that will try the original Runnable, and then each\\nfallback in order, upon failures.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/langchain/chains/langchain.chains.llm.LLMChain.html', 'title': 'LLMChain â€” ðŸ¦œðŸ”— LangChain  documentation', 'language': 'en'}, page_content='Returns:\\nA new Runnable that will try the original Runnable, and then each\\nfallback in order, upon failures.\\n\\nReturn type:\\nRunnableWithFallbacksT[Input, Output]\\n\\n\\n\\n\\n\\nwith_listeners(*, on_start: Callable[[Run], None] | Callable[[Run, RunnableConfig], None] | None = None, on_end: Callable[[Run], None] | Callable[[Run, RunnableConfig], None] | None = None, on_error: Callable[[Run], None] | Callable[[Run, RunnableConfig], None] | None = None) â†’ Runnable[Input, Output]#\\nBind lifecycle listeners to a Runnable, returning a new Runnable.\\non_start: Called before the Runnable starts running, with the Run object.\\non_end: Called after the Runnable finishes running, with the Run object.\\non_error: Called if the Runnable throws an error, with the Run object.\\nThe Run object contains information about the run, including its id,\\ntype, input, output, error, start_time, end_time, and any tags or metadata\\nadded to the run.\\n\\nParameters:'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/langchain/chains/langchain.chains.llm.LLMChain.html', 'title': 'LLMChain â€” ðŸ¦œðŸ”— LangChain  documentation', 'language': 'en'}, page_content='Parameters:\\n\\non_start (Optional[Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]]) â€“ Called before the Runnable starts running. Defaults to None.\\non_end (Optional[Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]]) â€“ Called after the Runnable finishes running. Defaults to None.\\non_error (Optional[Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]]) â€“ Called if the Runnable throws an error. Defaults to None.\\n\\n\\nReturns:\\nA new Runnable with the listeners bound.\\n\\nReturn type:\\nRunnable[Input, Output]\\n\\n\\nExample:\\nfrom langchain_core.runnables import RunnableLambda\\nfrom langchain_core.tracers.schemas import Run\\n\\nimport time\\n\\ndef test_runnable(time_to_sleep : int):\\n    time.sleep(time_to_sleep)\\n\\ndef fn_start(run_obj: Run):\\n    print(\"start_time:\", run_obj.start_time)\\n\\ndef fn_end(run_obj: Run):\\n    print(\"end_time:\", run_obj.end_time)'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/langchain/chains/langchain.chains.llm.LLMChain.html', 'title': 'LLMChain â€” ðŸ¦œðŸ”— LangChain  documentation', 'language': 'en'}, page_content='def fn_start(run_obj: Run):\\n    print(\"start_time:\", run_obj.start_time)\\n\\ndef fn_end(run_obj: Run):\\n    print(\"end_time:\", run_obj.end_time)\\n\\nchain = RunnableLambda(test_runnable).with_listeners(\\n    on_start=fn_start,\\n    on_end=fn_end\\n)\\nchain.invoke(2)\\n\\n\\n\\n\\n\\nwith_retry(*, retry_if_exception_type: tuple[type[BaseException], ...] = (<class \\'Exception\\'>,), wait_exponential_jitter: bool = True, stop_after_attempt: int = 3) â†’ Runnable[Input, Output]#\\nCreate a new Runnable that retries the original Runnable on exceptions.\\n\\nParameters:\\n\\nretry_if_exception_type (tuple[type[BaseException], ...]) â€“ A tuple of exception types to retry on.\\nDefaults to (Exception,).\\nwait_exponential_jitter (bool) â€“ Whether to add jitter to the wait\\ntime between retries. Defaults to True.\\nstop_after_attempt (int) â€“ The maximum number of attempts to make before\\ngiving up. Defaults to 3.\\n\\n\\nReturns:\\nA new Runnable that retries the original Runnable on exceptions.\\n\\nReturn type:\\nRunnable[Input, Output]'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/langchain/chains/langchain.chains.llm.LLMChain.html', 'title': 'LLMChain â€” ðŸ¦œðŸ”— LangChain  documentation', 'language': 'en'}, page_content='Returns:\\nA new Runnable that retries the original Runnable on exceptions.\\n\\nReturn type:\\nRunnable[Input, Output]\\n\\n\\nExample:\\nfrom langchain_core.runnables import RunnableLambda\\n\\ncount = 0\\n\\n\\ndef _lambda(x: int) -> None:\\n    global count\\n    count = count + 1\\n    if x == 1:\\n        raise ValueError(\"x is 1\")\\n    else:\\n         pass\\n\\n\\nrunnable = RunnableLambda(_lambda)\\ntry:\\n    runnable.with_retry(\\n        stop_after_attempt=2,\\n        retry_if_exception_type=(ValueError,),\\n    ).invoke(1)\\nexcept ValueError:\\n    pass\\n\\nassert (count == 2)\\n\\n\\n\\nParameters:\\n\\nretry_if_exception_type (tuple[type[BaseException], ...]) â€“ A tuple of exception types to retry on\\nwait_exponential_jitter (bool) â€“ Whether to add jitter to the wait time\\nbetween retries\\nstop_after_attempt (int) â€“ The maximum number of attempts to make before giving up\\n\\n\\nReturns:\\nA new Runnable that retries the original Runnable on exceptions.\\n\\nReturn type:\\nRunnable[Input, Output]'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/langchain/chains/langchain.chains.llm.LLMChain.html', 'title': 'LLMChain â€” ðŸ¦œðŸ”— LangChain  documentation', 'language': 'en'}, page_content='Returns:\\nA new Runnable that retries the original Runnable on exceptions.\\n\\nReturn type:\\nRunnable[Input, Output]\\n\\n\\n\\n\\n\\nwith_types(*, input_type: type[Input] | None = None, output_type: type[Output] | None = None) â†’ Runnable[Input, Output]#\\nBind input and output types to a Runnable, returning a new Runnable.\\n\\nParameters:\\n\\ninput_type (type[Input] | None) â€“ The input type to bind to the Runnable. Defaults to None.\\noutput_type (type[Output] | None) â€“ The output type to bind to the Runnable. Defaults to None.\\n\\n\\nReturns:\\nA new Runnable with the types bound.\\n\\nReturn type:\\nRunnable[Input, Output]\\n\\n\\n\\n\\nExamples using LLMChain'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/langchain/chains/langchain.chains.llm.LLMChain.html', 'title': 'LLMChain â€” ðŸ¦œðŸ”— LangChain  documentation', 'language': 'en'}, page_content='Returns:\\nA new Runnable with the types bound.\\n\\nReturn type:\\nRunnable[Input, Output]\\n\\n\\n\\n\\nExamples using LLMChain\\n\\n# Basic example (short documents)\\n# Example\\n# Legacy\\nAim\\nAlibaba Cloud PAI EAS\\nAnyscale\\nAphrodite Engine\\nArgilla\\nAzure ML\\nBanana\\nBaseten\\nBittensor\\nC Transformers\\nCTranslate2\\nCerebriumAI\\nChatGLM\\nClarifai\\nCloudflare Workers AI\\nComet\\nContext\\nDall-E Image Generator\\nDeepInfra\\nEden AI\\nFlyte\\nForefrontAI\\nGigaChat\\nGoogle Cloud Vertex AI Reranker\\nGooseAI\\nGradient\\nHuggingface Endpoints\\nIPEX-LLM\\nJavelin AI Gateway\\nJavelin AI Gateway Tutorial\\nLlama2Chat\\nMLflow AI Gateway\\nMLflow Deployments for LLMs\\nMemorize\\nMinimax\\nModal\\nMosaicML\\nMotÃ¶rhead\\nNLP Cloud\\nNebula (Symbl.ai)\\nOctoAI\\nOpaquePrompts\\nOpenLLM\\nOpenLM\\nPetals\\nPredibase\\nPrediction Guard\\nRay Serve\\nRePhraseQuery\\nRebuff\\nReddit Search \\nReplicate\\nRunhouse\\nSageMaker Tracking\\nSolar\\nStochasticAI\\nSummarize Text\\nTextGen\\nWeights & Biases\\nWriter\\nXorbits Inference (Xinference)\\nYandexGPT\\nYellowbrick\\nYuan2.0\\nZapier Natural Language Actions\\nvLLM'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/langchain/chains/langchain.chains.llm.LLMChain.html', 'title': 'LLMChain â€” ðŸ¦œðŸ”— LangChain  documentation', 'language': 'en'}, page_content='On this page\\n  \\n\\n\\nLLMChain\\ncallback_manager\\ncallbacks\\nllm\\nllm_kwargs\\nmemory\\nmetadata\\noutput_parser\\nprompt\\nreturn_final_only\\ntags\\nverbose\\n__call__()\\naapply()\\naapply_and_parse()\\nabatch()\\nabatch_as_completed()\\nacall()\\nainvoke()\\napply()\\napply_and_parse()\\napredict_and_parse()\\naprep_inputs()\\naprep_outputs()\\naprep_prompts()\\narun()\\nastream()\\nastream_events()\\nbatch()\\nbatch_as_completed()\\nbind()\\nconfigurable_alternatives()\\nconfigurable_fields()\\ncreate_outputs()\\nfrom_string()\\ninvoke()\\npredict_and_parse()\\nprep_inputs()\\nprep_outputs()\\nprep_prompts()\\nrun()\\nsave()\\nstream()\\nwith_alisteners()\\nwith_config()\\nwith_fallbacks()\\nwith_listeners()\\nwith_retry()\\nwith_types()\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    \\n      Â© Copyright 2023, LangChain Inc.')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "document=text_splitter.split_documents(documents)\n",
    "document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "embeddings=OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x1a0245a9ac0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "vectorstore=FAISS.from_documents(documents, embeddings)\n",
    "vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"This application will translate text from English to another language\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n\\n\\n\\n\\n\\n\\nLLMChain â€” ðŸ¦œðŸ”— LangChain  documentation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to main content\\n\\n\\nBack to top\\n\\n\\n\\n\\nCtrl+K\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Reference\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCtrl+K\\n\\n\\n\\n\\n\\n\\n\\nDocs\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nGitHub\\n\\n\\n\\nX / Twitter\\n\\n\\n\\n\\n\\n\\n\\n\\nCtrl+K\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Reference\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDocs\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nGitHub\\n\\n\\n\\nX / Twitter\\n\\n\\n\\n\\n\\n\\n\\nSection Navigation\\nBase packages\\n\\nCore\\nLangchain\\nagents\\ncallbacks\\nchains\\nChain\\nBaseCombineDocumentsChain\\nAsyncCombineDocsProtocol\\nCombineDocsProtocol\\nConstitutionalPrinciple\\nBaseConversationalRetrievalChain\\nChatVectorDBChain\\nInputType\\nElasticsearchDatabaseChain\\nFlareChain\\nQuestionGeneratorChain\\nFinishedOutputParser\\nHypotheticalDocumentEmbedder\\nOpenAIModerationChain\\nCrawler\\nElementInViewPort\\nFactWithEvidence\\nQuestionAnswer\\nSimpleRequestChain\\nAnswerWithSources\\nBasePromptSelector\\nConditionalPromptSelector\\nLoadingCallable\\nRetrievalQAWithSourcesChain\\nVectorDBQAWithSourcesChain\\nStructuredQueryOutputParser\\nISO8601Date\\nISO8601DateTime\\nAttributeInfo\\nLoadingCallable\\nMultiRouteChain\\nRoute\\nRouterChain\\nEmbeddingRouterChain\\nRouterOutputParser\\nMultiRetrievalQAChain\\nSequentialChain\\nSimpleSequentialChain\\nSQLInput\\nSQLInputWithTables\\nLoadingCallable\\nTransformChain\\nacollapse_docs\\ncollapse_docs\\nsplit_list_of_docs\\ncreate_stuff_documents_chain\\ngenerate_example\\ncreate_history_aware_retriever\\ncreate_citation_fuzzy_match_runnable\\nopenapi_spec_to_openai_fn\\nget_llm_kwargs\\nis_chat_model\\nis_llm\\nconstruct_examples\\nfix_filter_directive\\nget_query_constructor_prompt\\nload_query_constructor_runnable\\nget_parser\\nv_args\\ncreate_retrieval_chain\\ncreate_sql_query_chain\\nget_openai_output_parser\\nload_summarize_chain\\nAPIChain\\nAnalyzeDocumentChain\\nMapReduceDocumentsChain\\nMapRerankDocumentsChain\\nReduceDocumentsChain\\nRefineDocumentsChain\\nStuffDocumentsChain\\nConstitutionalChain\\nConversationChain\\nConversationalRetrievalChain\\nLLMChain\\nLLMCheckerChain\\nLLMMathChain\\nLLMSummarizationCheckerChain\\nMapReduceChain\\nNatBotChain\\nQAGenerationChain\\nBaseQAWithSourcesChain\\nQAWithSourcesChain\\nBaseRetrievalQA\\nRetrievalQA\\nVectorDBQA\\nLLMRouterChain\\nMultiPromptChain\\nload_chain\\nload_chain_from_config\\ncreate_openai_fn_chain\\ncreate_structured_output_chain\\ncreate_citation_fuzzy_match_chain\\ncreate_extraction_chain\\ncreate_extraction_chain_pydantic\\nget_openapi_chain\\ncreate_qa_with_sources_chain\\ncreate_qa_with_structure_chain\\ncreate_tagging_chain\\ncreate_tagging_chain_pydantic\\ncreate_extraction_chain_pydantic\\nload_qa_with_sources_chain\\nload_query_constructor_chain\\nload_qa_chain\\ncreate_openai_fn_runnable\\ncreate_structured_output_runnable\\n\\n\\nchat_models\\nembeddings\\nevaluation\\nglobals\\nhub\\nindexes\\nmemory\\nmodel_laboratory\\noutput_parsers\\nretrievers\\nrunnables\\nsmith\\nstorage\\n\\n\\nText Splitters\\nCommunity\\nExperimental\\n\\nIntegrations\\n\\nAI21\\nAnthropic\\nAstraDB\\nAWS\\nAzure Dynamic Sessions\\nCerebras\\nChroma\\nCohere\\nDeepseek\\nElasticsearch\\nExa\\nFireworks\\nGoogle Community\\nGoogle GenAI\\nGoogle VertexAI\\nGroq\\nHuggingface\\nIBM\\nMilvus\\nMistralAI\\nNeo4J\\nNomic\\nNvidia Ai Endpoints\\nOllama\\nOpenAI\\nPinecone\\nPostgres\\nPrompty\\nQdrant\\nRedis\\nSema4\\nSnowflake\\nSqlserver\\nStandard Tests\\nTogether\\nUnstructured\\nUpstage\\nVoyageAI\\nWeaviate\\nXAI\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLangChain Python API Reference\\nlangchain: 0.3.18\\nchains\\nLLMChain\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLLMChain#\\n\\n\\nclass langchain.chains.llm.LLMChain[source]#\\nBases: Chain\\n\\nDeprecated since version 0.1.17: Use , `prompt | llm`() instead. It will not be removed until langchain==1.0.\\n\\nChain to run queries against LLMs.\\nThis class is deprecated. See below for an example implementation using\\nLangChain runnables:\\n\\nfrom langchain_core.output_parsers import StrOutputParser\\nfrom langchain_core.prompts import PromptTemplate\\nfrom langchain_openai import OpenAI\\n\\nprompt_template = \"Tell me a {adjective} joke\"\\nprompt = PromptTemplate(\\n    input_variables=[\"adjective\"], template=prompt_template\\n)\\nllm = OpenAI()\\nchain = prompt | llm | StrOutputParser()\\n\\nchain.invoke(\"your adjective here\")\\n\\n\\n\\nExample\\nfrom langchain.chains import LLMChain\\nfrom langchain_community.llms import OpenAI\\nfrom langchain_core.prompts import PromptTemplate\\nprompt_template = \"Tell me a {adjective} joke\"\\nprompt = PromptTemplate(\\n    input_variables=[\"adjective\"], template=prompt_template\\n)\\nllm = LLMChain(llm=OpenAI(), prompt=prompt)\\n\\n\\n\\nNote\\nLLMChain implements the standard Runnable Interface. ðŸƒ\\nThe Runnable Interface has additional methods that are available on runnables, such as with_types, with_retry, assign, bind, get_graph, and more.\\n\\n\\n\\nparam callback_manager: BaseCallbackManager | None = None#\\n[DEPRECATED] Use callbacks instead.\\n\\n\\n\\nparam callbacks: Callbacks = None#\\nOptional list of callback handlers (or callback manager). Defaults to None.\\nCallback handlers are called throughout the lifecycle of a call to a chain,\\nstarting with on_chain_start, ending with on_chain_end or on_chain_error.\\nEach custom chain can optionally call additional callback methods, see Callback docs\\nfor full details.\\n\\n\\n\\nparam llm: Runnable[LanguageModelInput, str] | Runnable[LanguageModelInput, BaseMessage] [Required]#\\nLanguage model to call.\\n\\n\\n\\nparam llm_kwargs: dict [Optional]#\\n\\n\\n\\nparam memory: BaseMemory | None = None#\\nOptional memory object. Defaults to None.\\nMemory is a class that gets called at the start\\nand at the end of every chain. At the start, memory loads variables and passes\\nthem along in the chain. At the end, it saves any returned variables.\\nThere are many different types of memory - please see memory docs\\nfor the full catalog.\\n\\n\\n\\nparam metadata: Dict[str, Any] | None = None#\\nOptional metadata associated with the chain. Defaults to None.\\nThis metadata will be associated with each call to this chain,\\nand passed as arguments to the handlers defined in callbacks.\\nYou can use these to eg identify a specific instance of a chain with its use case.\\n\\n\\n\\nparam output_parser: BaseLLMOutputParser [Optional]#\\nOutput parser to use.\\nDefaults to one that takes the most likely string but does not change it\\notherwise.\\n\\n\\n\\nparam prompt: BasePromptTemplate [Required]#\\nPrompt object to use.\\n\\n\\n\\nparam return_final_only: bool = True#\\nWhether to return only the final parsed result. Defaults to True.\\nIf false, will return a bunch of extra information about the generation.\\n\\n\\n\\nparam tags: List[str] | None = None#\\nOptional list of tags associated with the chain. Defaults to None.\\nThese tags will be associated with each call to this chain,\\nand passed as arguments to the handlers defined in callbacks.\\nYou can use these to eg identify a specific instance of a chain with its use case.\\n\\n\\n\\nparam verbose: bool [Optional]#\\nWhether or not run in verbose mode. In verbose mode, some intermediate logs\\nwill be printed to the console. Defaults to the global verbose value,\\naccessible via langchain.globals.get_verbose().\\n\\n\\n\\n__call__(inputs: Dict[str, Any] | Any, return_only_outputs: bool = False, callbacks: list[BaseCallbackHandler] | BaseCallbackManager | None = None, *, tags: List[str] | None = None, metadata: Dict[str, Any] | None = None, run_name: str | None = None, include_run_info: bool = False) â†’ Dict[str, Any]#\\n\\nDeprecated since version 0.1.0: Use invoke() instead. It will not be removed until langchain==1.0.\\n\\nExecute the chain.\\n\\nParameters:\\n\\ninputs (Dict[str, Any] | Any) â€“ Dictionary of inputs, or single input if chain expects\\nonly one param. Should contain all inputs specified in\\nChain.input_keys except for inputs that will be set by the chainâ€™s\\nmemory.\\nreturn_only_outputs (bool) â€“ Whether to return only outputs in the\\nresponse. If True, only new keys generated by this chain will be\\nreturned. If False, both input keys and new keys generated by this\\nchain will be returned. Defaults to False.\\ncallbacks (list[BaseCallbackHandler] | BaseCallbackManager | None) â€“ Callbacks to use for this chain run. These will be called in\\naddition to callbacks passed to the chain during construction, but only\\nthese runtime callbacks will propagate to calls to other objects.\\ntags (List[str] | None) â€“ List of string tags to pass to all callbacks. These will be passed in\\naddition to tags passed to the chain during construction, but only\\nthese runtime tags will propagate to calls to other objects.\\nmetadata (Dict[str, Any] | None) â€“ Optional metadata associated with the chain. Defaults to None\\ninclude_run_info (bool) â€“ Whether to include run info in the response. Defaults\\nto False.\\nrun_name (str | None)\\n\\n\\nReturns:\\n\\nA dict of named outputs. Should contain all outputs specified inChain.output_keys.\\n\\n\\n\\n\\nReturn type:\\nDict[str, Any]\\n\\n\\n\\n\\n\\nasync aapply(input_list: List[Dict[str, Any]], callbacks: list[BaseCallbackHandler] | BaseCallbackManager | None = None) â†’ List[Dict[str, str]][source]#\\nUtilize the LLM generate method for speed gains.\\n\\nParameters:\\n\\ninput_list (List[Dict[str, Any]])\\ncallbacks (list[BaseCallbackHandler] | BaseCallbackManager | None)\\n\\n\\nReturn type:\\nList[Dict[str, str]]\\n\\n\\n\\n\\n\\nasync aapply_and_parse(input_list: List[Dict[str, Any]], callbacks: list[BaseCallbackHandler] | BaseCallbackManager | None = None) â†’ Sequence[str | List[str] | Dict[str, str]][source]#\\nCall apply and then parse the results.\\n\\nParameters:\\n\\ninput_list (List[Dict[str, Any]])\\ncallbacks (list[BaseCallbackHandler] | BaseCallbackManager | None)\\n\\n\\nReturn type:\\nSequence[str | List[str] | Dict[str, str]]\\n\\n\\n\\n\\n\\nasync abatch(inputs: list[Input], config: RunnableConfig | list[RunnableConfig] | None = None, *, return_exceptions: bool = False, **kwargs: Any | None) â†’ list[Output]#\\nDefault implementation runs ainvoke in parallel using asyncio.gather.\\nThe default implementation of batch works well for IO bound runnables.\\nSubclasses should override this method if they can batch more efficiently;\\ne.g., if the underlying Runnable uses an API which supports a batch mode.\\n\\nParameters:\\n\\ninputs (list[Input]) â€“ A list of inputs to the Runnable.\\nconfig (RunnableConfig | list[RunnableConfig] | None) â€“ A config to use when invoking the Runnable.\\nThe config supports standard keys like â€˜tagsâ€™, â€˜metadataâ€™ for tracing\\npurposes, â€˜max_concurrencyâ€™ for controlling how much work to do\\nin parallel, and other keys. Please refer to the RunnableConfig\\nfor more details. Defaults to None.\\nreturn_exceptions (bool) â€“ Whether to return exceptions instead of raising them.\\nDefaults to False.\\nkwargs (Any | None) â€“ Additional keyword arguments to pass to the Runnable.\\n\\n\\nReturns:\\nA list of outputs from the Runnable.\\n\\nReturn type:\\nlist[Output]\\n\\n\\n\\n\\n\\nasync abatch_as_completed(inputs: Sequence[Input], config: RunnableConfig | Sequence[RunnableConfig] | None = None, *, return_exceptions: bool = False, **kwargs: Any | None) â†’ AsyncIterator[tuple[int, Output | Exception]]#\\nRun ainvoke in parallel on a list of inputs,\\nyielding results as they complete.\\n\\nParameters:\\n\\ninputs (Sequence[Input]) â€“ A list of inputs to the Runnable.\\nconfig (RunnableConfig | Sequence[RunnableConfig] | None) â€“ A config to use when invoking the Runnable.\\nThe config supports standard keys like â€˜tagsâ€™, â€˜metadataâ€™ for tracing\\npurposes, â€˜max_concurrencyâ€™ for controlling how much work to do\\nin parallel, and other keys. Please refer to the RunnableConfig\\nfor more details. Defaults to None. Defaults to None.\\nreturn_exceptions (bool) â€“ Whether to return exceptions instead of raising them.\\nDefaults to False.\\nkwargs (Any | None) â€“ Additional keyword arguments to pass to the Runnable.\\n\\n\\nYields:\\nA tuple of the index of the input and the output from the Runnable.\\n\\nReturn type:\\nAsyncIterator[tuple[int, Output | Exception]]\\n\\n\\n\\n\\n\\nasync acall(inputs: Dict[str, Any] | Any, return_only_outputs: bool = False, callbacks: list[BaseCallbackHandler] | BaseCallbackManager | None = None, *, tags: List[str] | None = None, metadata: Dict[str, Any] | None = None, run_name: str | None = None, include_run_info: bool = False) â†’ Dict[str, Any]#\\n\\nDeprecated since version 0.1.0: Use ainvoke() instead. It will not be removed until langchain==1.0.\\n\\nAsynchronously execute the chain.\\n\\nParameters:\\n\\ninputs (Dict[str, Any] | Any) â€“ Dictionary of inputs, or single input if chain expects\\nonly one param. Should contain all inputs specified in\\nChain.input_keys except for inputs that will be set by the chainâ€™s\\nmemory.\\nreturn_only_outputs (bool) â€“ Whether to return only outputs in the\\nresponse. If True, only new keys generated by this chain will be\\nreturned. If False, both input keys and new keys generated by this\\nchain will be returned. Defaults to False.\\ncallbacks (list[BaseCallbackHandler] | BaseCallbackManager | None) â€“ Callbacks to use for this chain run. These will be called in\\naddition to callbacks passed to the chain during construction, but only\\nthese runtime callbacks will propagate to calls to other objects.\\ntags (List[str] | None) â€“ List of string tags to pass to all callbacks. These will be passed in\\naddition to tags passed to the chain during construction, but only\\nthese runtime tags will propagate to calls to other objects.\\nmetadata (Dict[str, Any] | None) â€“ Optional metadata associated with the chain. Defaults to None\\ninclude_run_info (bool) â€“ Whether to include run info in the response. Defaults\\nto False.\\nrun_name (str | None)\\n\\n\\nReturns:\\n\\nA dict of named outputs. Should contain all outputs specified inChain.output_keys.\\n\\n\\n\\n\\nReturn type:\\nDict[str, Any]\\n\\n\\n\\n\\n\\nasync ainvoke(input: Dict[str, Any], config: RunnableConfig | None = None, **kwargs: Any) â†’ Dict[str, Any]#\\nDefault implementation of ainvoke, calls invoke from a thread.\\nThe default implementation allows usage of async code even if\\nthe Runnable did not implement a native async version of invoke.\\nSubclasses should override this method if they can run asynchronously.\\n\\nParameters:\\n\\ninput (Dict[str, Any])\\nconfig (RunnableConfig | None)\\nkwargs (Any)\\n\\n\\nReturn type:\\nDict[str, Any]\\n\\n\\n\\n\\n\\napply(input_list: List[Dict[str, Any]], callbacks: list[BaseCallbackHandler] | BaseCallbackManager | None = None) â†’ List[Dict[str, str]][source]#\\nUtilize the LLM generate method for speed gains.\\n\\nParameters:\\n\\ninput_list (List[Dict[str, Any]])\\ncallbacks (list[BaseCallbackHandler] | BaseCallbackManager | None)\\n\\n\\nReturn type:\\nList[Dict[str, str]]\\n\\n\\n\\n\\n\\napply_and_parse(input_list: List[Dict[str, Any]], callbacks: list[BaseCallbackHandler] | BaseCallbackManager | None = None) â†’ Sequence[str | List[str] | Dict[str, str]][source]#\\nCall apply and then parse the results.\\n\\nParameters:\\n\\ninput_list (List[Dict[str, Any]])\\ncallbacks (list[BaseCallbackHandler] | BaseCallbackManager | None)\\n\\n\\nReturn type:\\nSequence[str | List[str] | Dict[str, str]]\\n\\n\\n\\n\\n\\nasync apredict_and_parse(callbacks: list[BaseCallbackHandler] | BaseCallbackManager | None = None, **kwargs: Any) â†’ str | List[str] | Dict[str, str][source]#\\nCall apredict and then parse the results.\\n\\nParameters:\\n\\ncallbacks (list[BaseCallbackHandler] | BaseCallbackManager | None)\\nkwargs (Any)\\n\\n\\nReturn type:\\nstr | List[str] | Dict[str, str]\\n\\n\\n\\n\\n\\nasync aprep_inputs(inputs: Dict[str, Any] | Any) â†’ Dict[str, str]#\\nPrepare chain inputs, including adding inputs from memory.\\n\\nParameters:\\ninputs (Dict[str, Any] | Any) â€“ Dictionary of raw inputs, or single input if chain expects\\nonly one param. Should contain all inputs specified in\\nChain.input_keys except for inputs that will be set by the chainâ€™s\\nmemory.\\n\\nReturns:\\nA dictionary of all inputs, including those added by the chainâ€™s memory.\\n\\nReturn type:\\nDict[str, str]\\n\\n\\n\\n\\n\\nasync aprep_outputs(inputs: Dict[str, str], outputs: Dict[str, str], return_only_outputs: bool = False) â†’ Dict[str, str]#\\nValidate and prepare chain outputs, and save info about this run to memory.\\n\\nParameters:\\n\\ninputs (Dict[str, str]) â€“ Dictionary of chain inputs, including any inputs added by chain\\nmemory.\\noutputs (Dict[str, str]) â€“ Dictionary of initial chain outputs.\\nreturn_only_outputs (bool) â€“ Whether to only return the chain outputs. If False,\\ninputs are also added to the final outputs.\\n\\n\\nReturns:\\nA dict of the final chain outputs.\\n\\nReturn type:\\nDict[str, str]\\n\\n\\n\\n\\n\\nasync aprep_prompts(input_list: List[Dict[str, Any]], run_manager: AsyncCallbackManagerForChainRun | None = None) â†’ Tuple[List[PromptValue], List[str] | None][source]#\\nPrepare prompts from inputs.\\n\\nParameters:\\n\\ninput_list (List[Dict[str, Any]])\\nrun_manager (AsyncCallbackManagerForChainRun | None)\\n\\n\\nReturn type:\\nTuple[List[PromptValue], List[str] | None]\\n\\n\\n\\n\\n\\nasync arun(*args: Any, callbacks: list[BaseCallbackHandler] | BaseCallbackManager | None = None, tags: List[str] | None = None, metadata: Dict[str, Any] | None = None, **kwargs: Any) â†’ Any#\\n\\nDeprecated since version 0.1.0: Use ainvoke() instead. It will not be removed until langchain==1.0.\\n\\nConvenience method for executing chain.\\nThe main difference between this method and Chain.__call__ is that this\\nmethod expects inputs to be passed directly in as positional arguments or\\nkeyword arguments, whereas Chain.__call__ expects a single input dictionary\\nwith all the inputs\\n\\nParameters:\\n\\n*args (Any) â€“ If the chain expects a single input, it can be passed in as the\\nsole positional argument.\\ncallbacks (list[BaseCallbackHandler] | BaseCallbackManager | None) â€“ Callbacks to use for this chain run. These will be called in\\naddition to callbacks passed to the chain during construction, but only\\nthese runtime callbacks will propagate to calls to other objects.\\ntags (List[str] | None) â€“ List of string tags to pass to all callbacks. These will be passed in\\naddition to tags passed to the chain during construction, but only\\nthese runtime tags will propagate to calls to other objects.\\n**kwargs (Any) â€“ If the chain expects multiple inputs, they can be passed in\\ndirectly as keyword arguments.\\nmetadata (Dict[str, Any] | None)\\n**kwargs\\n\\n\\nReturns:\\nThe chain output.\\n\\nReturn type:\\nAny\\n\\n\\nExample\\n# Suppose we have a single-input chain that takes a \\'question\\' string:\\nawait chain.arun(\"What\\'s the temperature in Boise, Idaho?\")\\n# -> \"The temperature in Boise is...\"\\n\\n# Suppose we have a multi-input chain that takes a \\'question\\' string\\n# and \\'context\\' string:\\nquestion = \"What\\'s the temperature in Boise, Idaho?\"\\ncontext = \"Weather report for Boise, Idaho on 07/03/23...\"\\nawait chain.arun(question=question, context=context)\\n# -> \"The temperature in Boise is...\"\\n\\n\\n\\n\\n\\nasync astream(input: Input, config: RunnableConfig | None = None, **kwargs: Any | None) â†’ AsyncIterator[Output]#\\nDefault implementation of astream, which calls ainvoke.\\nSubclasses should override this method if they support streaming output.\\n\\nParameters:\\n\\ninput (Input) â€“ The input to the Runnable.\\nconfig (RunnableConfig | None) â€“ The config to use for the Runnable. Defaults to None.\\nkwargs (Any | None) â€“ Additional keyword arguments to pass to the Runnable.\\n\\n\\nYields:\\nThe output of the Runnable.\\n\\nReturn type:\\nAsyncIterator[Output]\\n\\n\\n\\n\\n\\nasync astream_events(input: Any, config: RunnableConfig | None = None, *, version: Literal[\\'v1\\', \\'v2\\'], include_names: Sequence[str] | None = None, include_types: Sequence[str] | None = None, include_tags: Sequence[str] | None = None, exclude_names: Sequence[str] | None = None, exclude_types: Sequence[str] | None = None, exclude_tags: Sequence[str] | None = None, **kwargs: Any) â†’ AsyncIterator[StandardStreamEvent | CustomStreamEvent]#\\nGenerate a stream of events.\\nUse to create an iterator over StreamEvents that provide real-time information\\nabout the progress of the Runnable, including StreamEvents from intermediate\\nresults.\\nA StreamEvent is a dictionary with the following schema:\\n\\n\\nevent: str - Event names are of theformat: on_[runnable_type]_(start|stream|end).\\n\\n\\n\\nname: str - The name of the Runnable that generated the event.\\n\\nrun_id: str - randomly generated ID associated with the given execution ofthe Runnable that emitted the event.\\nA child Runnable that gets invoked as part of the execution of a\\nparent Runnable is assigned its own unique ID.\\n\\n\\n\\n\\nparent_ids: List[str] - The IDs of the parent runnables thatgenerated the event. The root Runnable will have an empty list.\\nThe order of the parent IDs is from the root to the immediate parent.\\nOnly available for v2 version of the API. The v1 version of the API\\nwill return an empty list.\\n\\n\\n\\n\\ntags: Optional[List[str]] - The tags of the Runnable that generatedthe event.\\n\\n\\n\\n\\nmetadata: Optional[Dict[str, Any]] - The metadata of the Runnablethat generated the event.\\n\\n\\n\\ndata: Dict[str, Any]\\n\\nBelow is a table that illustrates some events that might be emitted by various\\nchains. Metadata fields have been omitted from the table for brevity.\\nChain definitions have been included after the table.\\nATTENTION This reference table is for the V2 version of the schema.\\n\\n\\nevent\\nname\\nchunk\\ninput\\noutput\\n\\n\\n\\non_chat_model_start\\n[model name]\\n\\n{â€œmessagesâ€: [[SystemMessage, HumanMessage]]}\\n\\n\\non_chat_model_stream\\n[model name]\\nAIMessageChunk(content=â€helloâ€)\\n\\n\\n\\non_chat_model_end\\n[model name]\\n\\n{â€œmessagesâ€: [[SystemMessage, HumanMessage]]}\\nAIMessageChunk(content=â€hello worldâ€)\\n\\non_llm_start\\n[model name]\\n\\n{â€˜inputâ€™: â€˜helloâ€™}\\n\\n\\non_llm_stream\\n[model name]\\nâ€˜Helloâ€™\\n\\n\\n\\non_llm_end\\n[model name]\\n\\nâ€˜Hello human!â€™\\n\\n\\non_chain_start\\nformat_docs\\n\\n\\n\\n\\non_chain_stream\\nformat_docs\\nâ€œhello world!, goodbye world!â€\\n\\n\\n\\non_chain_end\\nformat_docs\\n\\n[Document(â€¦)]\\nâ€œhello world!, goodbye world!â€\\n\\non_tool_start\\nsome_tool\\n\\n{â€œxâ€: 1, â€œyâ€: â€œ2â€}\\n\\n\\non_tool_end\\nsome_tool\\n\\n\\n{â€œxâ€: 1, â€œyâ€: â€œ2â€}\\n\\non_retriever_start\\n[retriever name]\\n\\n{â€œqueryâ€: â€œhelloâ€}\\n\\n\\non_retriever_end\\n[retriever name]\\n\\n{â€œqueryâ€: â€œhelloâ€}\\n[Document(â€¦), ..]\\n\\non_prompt_start\\n[template_name]\\n\\n{â€œquestionâ€: â€œhelloâ€}\\n\\n\\non_prompt_end\\n[template_name]\\n\\n{â€œquestionâ€: â€œhelloâ€}\\nChatPromptValue(messages: [SystemMessage, â€¦])\\n\\n\\n\\n\\nIn addition to the standard events, users can also dispatch custom events (see example below).\\nCustom events will be only be surfaced with in the v2 version of the API!\\nA custom event has following format:\\n\\n\\nAttribute\\nType\\nDescription\\n\\n\\n\\nname\\nstr\\nA user defined name for the event.\\n\\ndata\\nAny\\nThe data associated with the event. This can be anything, though we suggest making it JSON serializable.\\n\\n\\n\\n\\nHere are declarations associated with the standard events shown above:\\nformat_docs:\\ndef format_docs(docs: List[Document]) -> str:\\n    \\'\\'\\'Format the docs.\\'\\'\\'\\n    return \", \".join([doc.page_content for doc in docs])\\n\\nformat_docs = RunnableLambda(format_docs)\\n\\n\\nsome_tool:\\n@tool\\ndef some_tool(x: int, y: str) -> dict:\\n    \\'\\'\\'Some_tool.\\'\\'\\'\\n    return {\"x\": x, \"y\": y}\\n\\n\\nprompt:\\ntemplate = ChatPromptTemplate.from_messages(\\n    [(\"system\", \"You are Cat Agent 007\"), (\"human\", \"{question}\")]\\n).with_config({\"run_name\": \"my_template\", \"tags\": [\"my_template\"]})\\n\\n\\nExample:\\nfrom langchain_core.runnables import RunnableLambda\\n\\nasync def reverse(s: str) -> str:\\n    return s[::-1]\\n\\nchain = RunnableLambda(func=reverse)\\n\\nevents = [\\n    event async for event in chain.astream_events(\"hello\", version=\"v2\")\\n]\\n\\n# will produce the following events (run_id, and parent_ids\\n# has been omitted for brevity):\\n[\\n    {\\n        \"data\": {\"input\": \"hello\"},\\n        \"event\": \"on_chain_start\",\\n        \"metadata\": {},\\n        \"name\": \"reverse\",\\n        \"tags\": [],\\n    },\\n    {\\n        \"data\": {\"chunk\": \"olleh\"},\\n        \"event\": \"on_chain_stream\",\\n        \"metadata\": {},\\n        \"name\": \"reverse\",\\n        \"tags\": [],\\n    },\\n    {\\n        \"data\": {\"output\": \"olleh\"},\\n        \"event\": \"on_chain_end\",\\n        \"metadata\": {},\\n        \"name\": \"reverse\",\\n        \"tags\": [],\\n    },\\n]\\n\\n\\nExample: Dispatch Custom Event\\nfrom langchain_core.callbacks.manager import (\\n    adispatch_custom_event,\\n)\\nfrom langchain_core.runnables import RunnableLambda, RunnableConfig\\nimport asyncio\\n\\n\\nasync def slow_thing(some_input: str, config: RunnableConfig) -> str:\\n    \"\"\"Do something that takes a long time.\"\"\"\\n    await asyncio.sleep(1) # Placeholder for some slow operation\\n    await adispatch_custom_event(\\n        \"progress_event\",\\n        {\"message\": \"Finished step 1 of 3\"},\\n        config=config # Must be included for python < 3.10\\n    )\\n    await asyncio.sleep(1) # Placeholder for some slow operation\\n    await adispatch_custom_event(\\n        \"progress_event\",\\n        {\"message\": \"Finished step 2 of 3\"},\\n        config=config # Must be included for python < 3.10\\n    )\\n    await asyncio.sleep(1) # Placeholder for some slow operation\\n    return \"Done\"\\n\\nslow_thing = RunnableLambda(slow_thing)\\n\\nasync for event in slow_thing.astream_events(\"some_input\", version=\"v2\"):\\n    print(event)\\n\\n\\n\\nParameters:\\n\\ninput (Any) â€“ The input to the Runnable.\\nconfig (RunnableConfig | None) â€“ The config to use for the Runnable.\\nversion (Literal[\\'v1\\', \\'v2\\']) â€“ The version of the schema to use either v2 or v1.\\nUsers should use v2.\\nv1 is for backwards compatibility and will be deprecated\\nin 0.4.0.\\nNo default will be assigned until the API is stabilized.\\ncustom events will only be surfaced in v2.\\ninclude_names (Sequence[str] | None) â€“ Only include events from runnables with matching names.\\ninclude_types (Sequence[str] | None) â€“ Only include events from runnables with matching types.\\ninclude_tags (Sequence[str] | None) â€“ Only include events from runnables with matching tags.\\nexclude_names (Sequence[str] | None) â€“ Exclude events from runnables with matching names.\\nexclude_types (Sequence[str] | None) â€“ Exclude events from runnables with matching types.\\nexclude_tags (Sequence[str] | None) â€“ Exclude events from runnables with matching tags.\\nkwargs (Any) â€“ Additional keyword arguments to pass to the Runnable.\\nThese will be passed to astream_log as this implementation\\nof astream_events is built on top of astream_log.\\n\\n\\nYields:\\nAn async stream of StreamEvents.\\n\\nRaises:\\nNotImplementedError â€“ If the version is not v1 or v2.\\n\\nReturn type:\\nAsyncIterator[StandardStreamEvent | CustomStreamEvent]\\n\\n\\n\\n\\n\\nbatch(inputs: list[Input], config: RunnableConfig | list[RunnableConfig] | None = None, *, return_exceptions: bool = False, **kwargs: Any | None) â†’ list[Output]#\\nDefault implementation runs invoke in parallel using a thread pool executor.\\nThe default implementation of batch works well for IO bound runnables.\\nSubclasses should override this method if they can batch more efficiently;\\ne.g., if the underlying Runnable uses an API which supports a batch mode.\\n\\nParameters:\\n\\ninputs (list[Input])\\nconfig (RunnableConfig | list[RunnableConfig] | None)\\nreturn_exceptions (bool)\\nkwargs (Any | None)\\n\\n\\nReturn type:\\nlist[Output]\\n\\n\\n\\n\\n\\nbatch_as_completed(inputs: Sequence[Input], config: RunnableConfig | Sequence[RunnableConfig] | None = None, *, return_exceptions: bool = False, **kwargs: Any | None) â†’ Iterator[tuple[int, Output | Exception]]#\\nRun invoke in parallel on a list of inputs,\\nyielding results as they complete.\\n\\nParameters:\\n\\ninputs (Sequence[Input])\\nconfig (RunnableConfig | Sequence[RunnableConfig] | None)\\nreturn_exceptions (bool)\\nkwargs (Any | None)\\n\\n\\nReturn type:\\nIterator[tuple[int, Output | Exception]]\\n\\n\\n\\n\\n\\nbind(**kwargs: Any) â†’ Runnable[Input, Output]#\\nBind arguments to a Runnable, returning a new Runnable.\\nUseful when a Runnable in a chain requires an argument that is not\\nin the output of the previous Runnable or included in the user input.\\n\\nParameters:\\nkwargs (Any) â€“ The arguments to bind to the Runnable.\\n\\nReturns:\\nA new Runnable with the arguments bound.\\n\\nReturn type:\\nRunnable[Input, Output]\\n\\n\\nExample:\\nfrom langchain_community.chat_models import ChatOllama\\nfrom langchain_core.output_parsers import StrOutputParser\\n\\nllm = ChatOllama(model=\\'llama2\\')\\n\\n# Without bind.\\nchain = (\\n    llm\\n    | StrOutputParser()\\n)\\n\\nchain.invoke(\"Repeat quoted words exactly: \\'One two three four five.\\'\")\\n# Output is \\'One two three four five.\\'\\n\\n# With bind.\\nchain = (\\n    llm.bind(stop=[\"three\"])\\n    | StrOutputParser()\\n)\\n\\nchain.invoke(\"Repeat quoted words exactly: \\'One two three four five.\\'\")\\n# Output is \\'One two\\'\\n\\n\\n\\n\\n\\nconfigurable_alternatives(which: ConfigurableField, *, default_key: str = \\'default\\', prefix_keys: bool = False, **kwargs: Runnable[Input, Output] | Callable[[], Runnable[Input, Output]]) â†’ RunnableSerializable#\\nConfigure alternatives for Runnables that can be set at runtime.\\n\\nParameters:\\n\\nwhich (ConfigurableField) â€“ The ConfigurableField instance that will be used to select the\\nalternative.\\ndefault_key (str) â€“ The default key to use if no alternative is selected.\\nDefaults to â€œdefaultâ€.\\nprefix_keys (bool) â€“ Whether to prefix the keys with the ConfigurableField id.\\nDefaults to False.\\n**kwargs (Runnable[Input, Output] | Callable[[], Runnable[Input, Output]]) â€“ A dictionary of keys to Runnable instances or callables that\\nreturn Runnable instances.\\n\\n\\nReturns:\\nA new Runnable with the alternatives configured.\\n\\nReturn type:\\nRunnableSerializable\\n\\n\\nfrom langchain_anthropic import ChatAnthropic\\nfrom langchain_core.runnables.utils import ConfigurableField\\nfrom langchain_openai import ChatOpenAI\\n\\nmodel = ChatAnthropic(\\n    model_name=\"claude-3-sonnet-20240229\"\\n).configurable_alternatives(\\n    ConfigurableField(id=\"llm\"),\\n    default_key=\"anthropic\",\\n    openai=ChatOpenAI()\\n)\\n\\n# uses the default model ChatAnthropic\\nprint(model.invoke(\"which organization created you?\").content)\\n\\n# uses ChatOpenAI\\nprint(\\n    model.with_config(\\n        configurable={\"llm\": \"openai\"}\\n    ).invoke(\"which organization created you?\").content\\n)\\n\\n\\n\\n\\n\\nconfigurable_fields(**kwargs: ConfigurableField | ConfigurableFieldSingleOption | ConfigurableFieldMultiOption) â†’ RunnableSerializable#\\nConfigure particular Runnable fields at runtime.\\n\\nParameters:\\n**kwargs (ConfigurableField | ConfigurableFieldSingleOption | ConfigurableFieldMultiOption) â€“ A dictionary of ConfigurableField instances to configure.\\n\\nReturns:\\nA new Runnable with the fields configured.\\n\\nReturn type:\\nRunnableSerializable\\n\\n\\nfrom langchain_core.runnables import ConfigurableField\\nfrom langchain_openai import ChatOpenAI\\n\\nmodel = ChatOpenAI(max_tokens=20).configurable_fields(\\n    max_tokens=ConfigurableField(\\n        id=\"output_token_number\",\\n        name=\"Max tokens in the output\",\\n        description=\"The maximum number of tokens in the output\",\\n    )\\n)\\n\\n# max_tokens = 20\\nprint(\\n    \"max_tokens_20: \",\\n    model.invoke(\"tell me something about chess\").content\\n)\\n\\n# max_tokens = 200\\nprint(\"max_tokens_200: \", model.with_config(\\n    configurable={\"output_token_number\": 200}\\n    ).invoke(\"tell me something about chess\").content\\n)\\n\\n\\n\\n\\n\\ncreate_outputs(llm_result: LLMResult) â†’ List[Dict[str, Any]][source]#\\nCreate outputs from response.\\n\\nParameters:\\nllm_result (LLMResult)\\n\\nReturn type:\\nList[Dict[str, Any]]\\n\\n\\n\\n\\n\\nclassmethod from_string(llm: BaseLanguageModel, template: str) â†’ LLMChain[source]#\\nCreate LLMChain from LLM and template.\\n\\nParameters:\\n\\nllm (BaseLanguageModel)\\ntemplate (str)\\n\\n\\nReturn type:\\nLLMChain\\n\\n\\n\\n\\n\\ninvoke(input: Dict[str, Any], config: RunnableConfig | None = None, **kwargs: Any) â†’ Dict[str, Any]#\\nTransform a single input into an output. Override to implement.\\n\\nParameters:\\n\\ninput (Dict[str, Any]) â€“ The input to the Runnable.\\nconfig (RunnableConfig | None) â€“ A config to use when invoking the Runnable.\\nThe config supports standard keys like â€˜tagsâ€™, â€˜metadataâ€™ for tracing\\npurposes, â€˜max_concurrencyâ€™ for controlling how much work to do\\nin parallel, and other keys. Please refer to the RunnableConfig\\nfor more details.\\nkwargs (Any)\\n\\n\\nReturns:\\nThe output of the Runnable.\\n\\nReturn type:\\nDict[str, Any]\\n\\n\\n\\n\\n\\npredict_and_parse(callbacks: list[BaseCallbackHandler] | BaseCallbackManager | None = None, **kwargs: Any) â†’ str | List[str] | Dict[str, Any][source]#\\nCall predict and then parse the results.\\n\\nParameters:\\n\\ncallbacks (list[BaseCallbackHandler] | BaseCallbackManager | None)\\nkwargs (Any)\\n\\n\\nReturn type:\\nstr | List[str] | Dict[str, Any]\\n\\n\\n\\n\\n\\nprep_inputs(inputs: Dict[str, Any] | Any) â†’ Dict[str, str]#\\nPrepare chain inputs, including adding inputs from memory.\\n\\nParameters:\\ninputs (Dict[str, Any] | Any) â€“ Dictionary of raw inputs, or single input if chain expects\\nonly one param. Should contain all inputs specified in\\nChain.input_keys except for inputs that will be set by the chainâ€™s\\nmemory.\\n\\nReturns:\\nA dictionary of all inputs, including those added by the chainâ€™s memory.\\n\\nReturn type:\\nDict[str, str]\\n\\n\\n\\n\\n\\nprep_outputs(inputs: Dict[str, str], outputs: Dict[str, str], return_only_outputs: bool = False) â†’ Dict[str, str]#\\nValidate and prepare chain outputs, and save info about this run to memory.\\n\\nParameters:\\n\\ninputs (Dict[str, str]) â€“ Dictionary of chain inputs, including any inputs added by chain\\nmemory.\\noutputs (Dict[str, str]) â€“ Dictionary of initial chain outputs.\\nreturn_only_outputs (bool) â€“ Whether to only return the chain outputs. If False,\\ninputs are also added to the final outputs.\\n\\n\\nReturns:\\nA dict of the final chain outputs.\\n\\nReturn type:\\nDict[str, str]\\n\\n\\n\\n\\n\\nprep_prompts(input_list: List[Dict[str, Any]], run_manager: CallbackManagerForChainRun | None = None) â†’ Tuple[List[PromptValue], List[str] | None][source]#\\nPrepare prompts from inputs.\\n\\nParameters:\\n\\ninput_list (List[Dict[str, Any]])\\nrun_manager (CallbackManagerForChainRun | None)\\n\\n\\nReturn type:\\nTuple[List[PromptValue], List[str] | None]\\n\\n\\n\\n\\n\\nrun(*args: Any, callbacks: list[BaseCallbackHandler] | BaseCallbackManager | None = None, tags: List[str] | None = None, metadata: Dict[str, Any] | None = None, **kwargs: Any) â†’ Any#\\n\\nDeprecated since version 0.1.0: Use invoke() instead. It will not be removed until langchain==1.0.\\n\\nConvenience method for executing chain.\\nThe main difference between this method and Chain.__call__ is that this\\nmethod expects inputs to be passed directly in as positional arguments or\\nkeyword arguments, whereas Chain.__call__ expects a single input dictionary\\nwith all the inputs\\n\\nParameters:\\n\\n*args (Any) â€“ If the chain expects a single input, it can be passed in as the\\nsole positional argument.\\ncallbacks (list[BaseCallbackHandler] | BaseCallbackManager | None) â€“ Callbacks to use for this chain run. These will be called in\\naddition to callbacks passed to the chain during construction, but only\\nthese runtime callbacks will propagate to calls to other objects.\\ntags (List[str] | None) â€“ List of string tags to pass to all callbacks. These will be passed in\\naddition to tags passed to the chain during construction, but only\\nthese runtime tags will propagate to calls to other objects.\\n**kwargs (Any) â€“ If the chain expects multiple inputs, they can be passed in\\ndirectly as keyword arguments.\\nmetadata (Dict[str, Any] | None)\\n**kwargs\\n\\n\\nReturns:\\nThe chain output.\\n\\nReturn type:\\nAny\\n\\n\\nExample\\n# Suppose we have a single-input chain that takes a \\'question\\' string:\\nchain.run(\"What\\'s the temperature in Boise, Idaho?\")\\n# -> \"The temperature in Boise is...\"\\n\\n# Suppose we have a multi-input chain that takes a \\'question\\' string\\n# and \\'context\\' string:\\nquestion = \"What\\'s the temperature in Boise, Idaho?\"\\ncontext = \"Weather report for Boise, Idaho on 07/03/23...\"\\nchain.run(question=question, context=context)\\n# -> \"The temperature in Boise is...\"\\n\\n\\n\\n\\n\\nsave(file_path: Path | str) â†’ None#\\nSave the chain.\\n\\nExpects Chain._chain_type property to be implemented and for memory to benull.\\n\\n\\n\\nParameters:\\nfile_path (Path | str) â€“ Path to file to save the chain to.\\n\\nReturn type:\\nNone\\n\\n\\nExample\\nchain.save(file_path=\"path/chain.yaml\")\\n\\n\\n\\n\\n\\nstream(input: Input, config: RunnableConfig | None = None, **kwargs: Any | None) â†’ Iterator[Output]#\\nDefault implementation of stream, which calls invoke.\\nSubclasses should override this method if they support streaming output.\\n\\nParameters:\\n\\ninput (Input) â€“ The input to the Runnable.\\nconfig (RunnableConfig | None) â€“ The config to use for the Runnable. Defaults to None.\\nkwargs (Any | None) â€“ Additional keyword arguments to pass to the Runnable.\\n\\n\\nYields:\\nThe output of the Runnable.\\n\\nReturn type:\\nIterator[Output]\\n\\n\\n\\n\\n\\nwith_alisteners(*, on_start: AsyncListener | None = None, on_end: AsyncListener | None = None, on_error: AsyncListener | None = None) â†’ Runnable[Input, Output]#\\nBind async lifecycle listeners to a Runnable, returning a new Runnable.\\non_start: Asynchronously called before the Runnable starts running.\\non_end: Asynchronously called after the Runnable finishes running.\\non_error: Asynchronously called if the Runnable throws an error.\\nThe Run object contains information about the run, including its id,\\ntype, input, output, error, start_time, end_time, and any tags or metadata\\nadded to the run.\\n\\nParameters:\\n\\non_start (Optional[AsyncListener]) â€“ Asynchronously called before the Runnable starts running.\\nDefaults to None.\\non_end (Optional[AsyncListener]) â€“ Asynchronously called after the Runnable finishes running.\\nDefaults to None.\\non_error (Optional[AsyncListener]) â€“ Asynchronously called if the Runnable throws an error.\\nDefaults to None.\\n\\n\\nReturns:\\nA new Runnable with the listeners bound.\\n\\nReturn type:\\nRunnable[Input, Output]\\n\\n\\nExample:\\nfrom langchain_core.runnables import RunnableLambda\\nimport time\\n\\nasync def test_runnable(time_to_sleep : int):\\n    print(f\"Runnable[{time_to_sleep}s]: starts at {format_t(time.time())}\")\\n    await asyncio.sleep(time_to_sleep)\\n    print(f\"Runnable[{time_to_sleep}s]: ends at {format_t(time.time())}\")\\n\\nasync def fn_start(run_obj : Runnable):\\n    print(f\"on start callback starts at {format_t(time.time())}\\n    await asyncio.sleep(3)\\n    print(f\"on start callback ends at {format_t(time.time())}\")\\n\\nasync def fn_end(run_obj : Runnable):\\n    print(f\"on end callback starts at {format_t(time.time())}\\n    await asyncio.sleep(2)\\n    print(f\"on end callback ends at {format_t(time.time())}\")\\n\\nrunnable = RunnableLambda(test_runnable).with_alisteners(\\n    on_start=fn_start,\\n    on_end=fn_end\\n)\\nasync def concurrent_runs():\\n    await asyncio.gather(runnable.ainvoke(2), runnable.ainvoke(3))\\n\\nasyncio.run(concurrent_runs())\\nResult:\\non start callback starts at 2024-05-16T14:20:29.637053+00:00\\non start callback starts at 2024-05-16T14:20:29.637150+00:00\\non start callback ends at 2024-05-16T14:20:32.638305+00:00\\non start callback ends at 2024-05-16T14:20:32.638383+00:00\\nRunnable[3s]: starts at 2024-05-16T14:20:32.638849+00:00\\nRunnable[5s]: starts at 2024-05-16T14:20:32.638999+00:00\\nRunnable[3s]: ends at 2024-05-16T14:20:35.640016+00:00\\non end callback starts at 2024-05-16T14:20:35.640534+00:00\\nRunnable[5s]: ends at 2024-05-16T14:20:37.640169+00:00\\non end callback starts at 2024-05-16T14:20:37.640574+00:00\\non end callback ends at 2024-05-16T14:20:37.640654+00:00\\non end callback ends at 2024-05-16T14:20:39.641751+00:00\\n\\n\\n\\n\\n\\nwith_config(config: RunnableConfig | None = None, **kwargs: Any) â†’ Runnable[Input, Output]#\\nBind config to a Runnable, returning a new Runnable.\\n\\nParameters:\\n\\nconfig (RunnableConfig | None) â€“ The config to bind to the Runnable.\\nkwargs (Any) â€“ Additional keyword arguments to pass to the Runnable.\\n\\n\\nReturns:\\nA new Runnable with the config bound.\\n\\nReturn type:\\nRunnable[Input, Output]\\n\\n\\n\\n\\n\\nwith_fallbacks(fallbacks: Sequence[Runnable[Input, Output]], *, exceptions_to_handle: tuple[type[BaseException], ...] = (<class \\'Exception\\'>,), exception_key: Optional[str] = None) â†’ RunnableWithFallbacksT[Input, Output]#\\nAdd fallbacks to a Runnable, returning a new Runnable.\\nThe new Runnable will try the original Runnable, and then each fallback\\nin order, upon failures.\\n\\nParameters:\\n\\nfallbacks (Sequence[Runnable[Input, Output]]) â€“ A sequence of runnables to try if the original Runnable fails.\\nexceptions_to_handle (tuple[type[BaseException], ...]) â€“ A tuple of exception types to handle.\\nDefaults to (Exception,).\\nexception_key (Optional[str]) â€“ If string is specified then handled exceptions will be passed\\nto fallbacks as part of the input under the specified key. If None,\\nexceptions will not be passed to fallbacks. If used, the base Runnable\\nand its fallbacks must accept a dictionary as input. Defaults to None.\\n\\n\\nReturns:\\nA new Runnable that will try the original Runnable, and then each\\nfallback in order, upon failures.\\n\\nReturn type:\\nRunnableWithFallbacksT[Input, Output]\\n\\n\\nExample\\nfrom typing import Iterator\\n\\nfrom langchain_core.runnables import RunnableGenerator\\n\\n\\ndef _generate_immediate_error(input: Iterator) -> Iterator[str]:\\n    raise ValueError()\\n    yield \"\"\\n\\n\\ndef _generate(input: Iterator) -> Iterator[str]:\\n    yield from \"foo bar\"\\n\\n\\nrunnable = RunnableGenerator(_generate_immediate_error).with_fallbacks(\\n    [RunnableGenerator(_generate)]\\n    )\\nprint(\\'\\'.join(runnable.stream({}))) #foo bar\\n\\n\\n\\nParameters:\\n\\nfallbacks (Sequence[Runnable[Input, Output]]) â€“ A sequence of runnables to try if the original Runnable fails.\\nexceptions_to_handle (tuple[type[BaseException], ...]) â€“ A tuple of exception types to handle.\\nexception_key (Optional[str]) â€“ If string is specified then handled exceptions will be passed\\nto fallbacks as part of the input under the specified key. If None,\\nexceptions will not be passed to fallbacks. If used, the base Runnable\\nand its fallbacks must accept a dictionary as input.\\n\\n\\nReturns:\\nA new Runnable that will try the original Runnable, and then each\\nfallback in order, upon failures.\\n\\nReturn type:\\nRunnableWithFallbacksT[Input, Output]\\n\\n\\n\\n\\n\\nwith_listeners(*, on_start: Callable[[Run], None] | Callable[[Run, RunnableConfig], None] | None = None, on_end: Callable[[Run], None] | Callable[[Run, RunnableConfig], None] | None = None, on_error: Callable[[Run], None] | Callable[[Run, RunnableConfig], None] | None = None) â†’ Runnable[Input, Output]#\\nBind lifecycle listeners to a Runnable, returning a new Runnable.\\non_start: Called before the Runnable starts running, with the Run object.\\non_end: Called after the Runnable finishes running, with the Run object.\\non_error: Called if the Runnable throws an error, with the Run object.\\nThe Run object contains information about the run, including its id,\\ntype, input, output, error, start_time, end_time, and any tags or metadata\\nadded to the run.\\n\\nParameters:\\n\\non_start (Optional[Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]]) â€“ Called before the Runnable starts running. Defaults to None.\\non_end (Optional[Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]]) â€“ Called after the Runnable finishes running. Defaults to None.\\non_error (Optional[Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]]) â€“ Called if the Runnable throws an error. Defaults to None.\\n\\n\\nReturns:\\nA new Runnable with the listeners bound.\\n\\nReturn type:\\nRunnable[Input, Output]\\n\\n\\nExample:\\nfrom langchain_core.runnables import RunnableLambda\\nfrom langchain_core.tracers.schemas import Run\\n\\nimport time\\n\\ndef test_runnable(time_to_sleep : int):\\n    time.sleep(time_to_sleep)\\n\\ndef fn_start(run_obj: Run):\\n    print(\"start_time:\", run_obj.start_time)\\n\\ndef fn_end(run_obj: Run):\\n    print(\"end_time:\", run_obj.end_time)\\n\\nchain = RunnableLambda(test_runnable).with_listeners(\\n    on_start=fn_start,\\n    on_end=fn_end\\n)\\nchain.invoke(2)\\n\\n\\n\\n\\n\\nwith_retry(*, retry_if_exception_type: tuple[type[BaseException], ...] = (<class \\'Exception\\'>,), wait_exponential_jitter: bool = True, stop_after_attempt: int = 3) â†’ Runnable[Input, Output]#\\nCreate a new Runnable that retries the original Runnable on exceptions.\\n\\nParameters:\\n\\nretry_if_exception_type (tuple[type[BaseException], ...]) â€“ A tuple of exception types to retry on.\\nDefaults to (Exception,).\\nwait_exponential_jitter (bool) â€“ Whether to add jitter to the wait\\ntime between retries. Defaults to True.\\nstop_after_attempt (int) â€“ The maximum number of attempts to make before\\ngiving up. Defaults to 3.\\n\\n\\nReturns:\\nA new Runnable that retries the original Runnable on exceptions.\\n\\nReturn type:\\nRunnable[Input, Output]\\n\\n\\nExample:\\nfrom langchain_core.runnables import RunnableLambda\\n\\ncount = 0\\n\\n\\ndef _lambda(x: int) -> None:\\n    global count\\n    count = count + 1\\n    if x == 1:\\n        raise ValueError(\"x is 1\")\\n    else:\\n         pass\\n\\n\\nrunnable = RunnableLambda(_lambda)\\ntry:\\n    runnable.with_retry(\\n        stop_after_attempt=2,\\n        retry_if_exception_type=(ValueError,),\\n    ).invoke(1)\\nexcept ValueError:\\n    pass\\n\\nassert (count == 2)\\n\\n\\n\\nParameters:\\n\\nretry_if_exception_type (tuple[type[BaseException], ...]) â€“ A tuple of exception types to retry on\\nwait_exponential_jitter (bool) â€“ Whether to add jitter to the wait time\\nbetween retries\\nstop_after_attempt (int) â€“ The maximum number of attempts to make before giving up\\n\\n\\nReturns:\\nA new Runnable that retries the original Runnable on exceptions.\\n\\nReturn type:\\nRunnable[Input, Output]\\n\\n\\n\\n\\n\\nwith_types(*, input_type: type[Input] | None = None, output_type: type[Output] | None = None) â†’ Runnable[Input, Output]#\\nBind input and output types to a Runnable, returning a new Runnable.\\n\\nParameters:\\n\\ninput_type (type[Input] | None) â€“ The input type to bind to the Runnable. Defaults to None.\\noutput_type (type[Output] | None) â€“ The output type to bind to the Runnable. Defaults to None.\\n\\n\\nReturns:\\nA new Runnable with the types bound.\\n\\nReturn type:\\nRunnable[Input, Output]\\n\\n\\n\\n\\nExamples using LLMChain\\n\\n# Basic example (short documents)\\n# Example\\n# Legacy\\nAim\\nAlibaba Cloud PAI EAS\\nAnyscale\\nAphrodite Engine\\nArgilla\\nAzure ML\\nBanana\\nBaseten\\nBittensor\\nC Transformers\\nCTranslate2\\nCerebriumAI\\nChatGLM\\nClarifai\\nCloudflare Workers AI\\nComet\\nContext\\nDall-E Image Generator\\nDeepInfra\\nEden AI\\nFlyte\\nForefrontAI\\nGigaChat\\nGoogle Cloud Vertex AI Reranker\\nGooseAI\\nGradient\\nHuggingface Endpoints\\nIPEX-LLM\\nJavelin AI Gateway\\nJavelin AI Gateway Tutorial\\nLlama2Chat\\nMLflow AI Gateway\\nMLflow Deployments for LLMs\\nMemorize\\nMinimax\\nModal\\nMosaicML\\nMotÃ¶rhead\\nNLP Cloud\\nNebula (Symbl.ai)\\nOctoAI\\nOpaquePrompts\\nOpenLLM\\nOpenLM\\nPetals\\nPredibase\\nPrediction Guard\\nRay Serve\\nRePhraseQuery\\nRebuff\\nReddit Search \\nReplicate\\nRunhouse\\nSageMaker Tracking\\nSolar\\nStochasticAI\\nSummarize Text\\nTextGen\\nWeights & Biases\\nWriter\\nXorbits Inference (Xinference)\\nYandexGPT\\nYellowbrick\\nYuan2.0\\nZapier Natural Language Actions\\nvLLM\\n\\n\\n\\n\\n\\n\\n\\n\\n On this page\\n  \\n\\n\\nLLMChain\\ncallback_manager\\ncallbacks\\nllm\\nllm_kwargs\\nmemory\\nmetadata\\noutput_parser\\nprompt\\nreturn_final_only\\ntags\\nverbose\\n__call__()\\naapply()\\naapply_and_parse()\\nabatch()\\nabatch_as_completed()\\nacall()\\nainvoke()\\napply()\\napply_and_parse()\\napredict_and_parse()\\naprep_inputs()\\naprep_outputs()\\naprep_prompts()\\narun()\\nastream()\\nastream_events()\\nbatch()\\nbatch_as_completed()\\nbind()\\nconfigurable_alternatives()\\nconfigurable_fields()\\ncreate_outputs()\\nfrom_string()\\ninvoke()\\npredict_and_parse()\\nprep_inputs()\\nprep_outputs()\\nprep_prompts()\\nrun()\\nsave()\\nstream()\\nwith_alisteners()\\nwith_config()\\nwith_fallbacks()\\nwith_listeners()\\nwith_retry()\\nwith_types()\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    \\n      Â© Copyright 2023, LangChain Inc.\\n      \\n\\n\\n\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result=vectorstore.similarity_search(query)\n",
    "result[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt=ChatPromptTemplate.from_template(\n",
    "\"\"\"\n",
    "Answer the following question based only on the provided context: \n",
    "<context>\n",
    "{context}\n",
    "<context>\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableLambda(format_docs)\n",
       "}), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "| ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\nAnswer the following question based only on the provided context: \\n<context>\\n{context}\\n<context>\\n'), additional_kwargs={})])\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x000001A01412F170>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000001A01414CE60>, root_client=<openai.OpenAI object at 0x000001A011726630>, root_async_client=<openai.AsyncOpenAI object at 0x000001A0140DD580>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "| StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "document_chain=create_stuff_documents_chain(llm, prompt)\n",
    "document_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "experimenting with chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# document_chain.invoke({\n",
    "#     \"input\": \"Note that ChatModels receive message objects as input\",\n",
    "#     \"context\":[Document(page_content=\"Note that ChatModels receive message objects as input amd return message objects as output\")]\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#interface for querying vectorstore\n",
    "retriever=vectorstore.as_retriever()\n",
    "\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "retriever_chain=create_retrieval_chain(retriever, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=retriever_chain.invoke({\"input\":\"what are inputs for chatmodels\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the primary purpose of the `LLMChain` class in LangChain?'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided context, ChatModels handle input as message objects, which include not just text but also convey conversational roles and other essential data. They can be invoked using either a string or a structured message format. Additionally, they support various invocation modes, including async and streaming. The context also mentions the capability to stream individual tokens from chat models, providing a mechanism to handle outputs in real-time.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "document_chain.invoke({\n",
    "    \"input\":\"Note that ChatModels receive message objects as input\",\n",
    "    \"context\":[Document(page_content=\"Note that ChatModels receive message objects as input and generate message objects as output. In addition to text content, message objects convey conversational roles and hold important data, such as tool calls and token usage counts.\\nLangChain also supports chat model inputs via strings or OpenAI format. The following are equivalent:\\nmodel.invoke('Hello')model.invoke([{'role': 'user', 'content': 'Hello'}])model.invoke([HumanMessage('Hello')]) StreamingBecause chat models are Runnables, they expose a standard interface that includes async and streaming modes of invocation. This allows us to stream individual tokens from a chat model:\\nfor token in model.stream(messages):    print(token.content, end='|')|C|iao|!|| You can find more details on streaming chat model outputs in this guide.\\nPrompt Templates\")]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
       "           | VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001A0245A9AC0>, search_kwargs={}), kwargs={}, config={'run_name': 'retrieve_documents'}, config_factories=[])\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "            | ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\nAnswer the following question based only on the provided context: \\n<context>\\n{context}\\n<context>\\n'), additional_kwargs={})])\n",
       "            | ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x000001A01412F170>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000001A01414CE60>, root_client=<openai.OpenAI object at 0x000001A011726630>, root_async_client=<openai.AsyncOpenAI object at 0x000001A0140DD580>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "            | StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])\n",
       "  }), kwargs={}, config={'run_name': 'retrieval_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever=vectorstore.as_retriever()\n",
    "\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "retrieval_chain=create_retrieval_chain(retriever,document_chain)\n",
    "retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=retrieval_chain.invoke({\"input\":\"Note that ChatModels receive message objects as input\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The LLMChain class in LangChain is used to run queries against Language Models (LLMs). It is deprecated and is recommended to use the newer approach of combining prompts and LLMs, represented as `prompt | llm()`. The class requires a prompt template and a language model instance, and it offers various methods for processing inputs and outputs, including synchronous and asynchronous execution options. It supports callbacks for tracking the execution lifecycle and allows for configuration, memory management, and error handling. Additionally, users can bind inputs and set alternative configurations at runtime.'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['answer']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
